# Review Consolidation Algorithm

**Version**: 1.0
**Date**: 2025-10-16
**Purpose**: Define objective, reproducible methodology for consolidating multiple reviewer reports into a single unified output

---

## Overview

The Review Consolidation Algorithm processes multiple independent reviewer reports (e.g., code-style-reviewer, code-principles-reviewer, test-healer) and produces a single, deduplicated, prioritized report with aggregated confidence scores and synthesized recommendations.

**Design Philosophy**:
- **Objective**: Eliminate duplicates deterministically, minimize subjective aggregation
- **Prioritized**: Critical issues (P0) surface immediately, low-value issues filtered
- **Confidence-based**: Weight reviewer expertise, filter low-confidence findings
- **Actionable**: Synthesized recommendations ranked by frequency and relevance

**Expected Outcomes**:
- Deduplication rate: 60-70% reduction in issue count
- Aggregation quality: High-confidence issues prioritized (P0 > P1 > P2)
- Recommendation synthesis: Top 5-10 actionable themes
- Processing time: <2 seconds for typical 3-reviewer report set

---

## Reviewer Selection Algorithm

**Purpose**: Dynamically determine which reviewers to invoke based on file types and review context

**Design Philosophy**:
- **File-driven**: Select reviewers based on file extensions and patterns
- **Context-aware**: Adjust reviewer selection based on review context (post-implementation vs pre-commit)
- **Configurable**: Allow override via explicit reviewer list parameter
- **Efficient**: Skip irrelevant reviewers for configuration/documentation files

### File Type to Reviewer Mapping

**Core Mapping Rules**:

```typescript
/**
 * Maps file types to required reviewers
 * @param files Array of file paths to review
 * @returns Set of reviewer IDs to invoke
 */
function selectReviewers(files: string[]): Set<string> {
  const reviewers = new Set<string>();

  for (const file of files) {
    // C# Production Code Files
    if (file.endsWith('.cs') && !file.includes('Test')) {
      reviewers.add('code-style-reviewer');        // ALWAYS for .cs files
      reviewers.add('code-principles-reviewer');   // ALWAYS for .cs files
    }

    // C# Test Files
    if (file.endsWith('.cs') && file.includes('Test')) {
      reviewers.add('code-style-reviewer');        // ALWAYS for test files
      reviewers.add('code-principles-reviewer');   // ALWAYS for test files
      reviewers.add('test-healer');                // ALWAYS (priority) for test files
    }

    // Configuration Files - SKIP ALL REVIEWERS
    if (file.match(/\.(json|xml|yaml|yml|config)$/)) {
      // Explicitly skip - no reviewers needed for configuration
      continue;
    }

    // Documentation Files - SKIP ALL REVIEWERS (for now)
    if (file.endsWith('.md')) {
      // Future: documentation-reviewer
      continue;
    }

    // Other Code Files (TypeScript, JavaScript, etc.) - Future support
    if (file.match(/\.(ts|tsx|js|jsx)$/)) {
      // Future: Adapt to support multiple languages
      continue;
    }
  }

  return reviewers;
}
```

### File Type Mapping Table

| File Pattern | code-style-reviewer | code-principles-reviewer | test-healer | Notes |
|--------------|---------------------|--------------------------|-------------|-------|
| `*.cs` (no "Test") | ‚úÖ ALWAYS | ‚úÖ ALWAYS | ‚ùå SKIP | Production C# code |
| `*Test.cs` | ‚úÖ ALWAYS | ‚úÖ ALWAYS | ‚úÖ ALWAYS (priority) | C# test files |
| `*Tests.cs` | ‚úÖ ALWAYS | ‚úÖ ALWAYS | ‚úÖ ALWAYS (priority) | C# test files |
| `*.Tests.cs` | ‚úÖ ALWAYS | ‚úÖ ALWAYS | ‚úÖ ALWAYS (priority) | C# test files |
| `*.json` | ‚ùå SKIP | ‚ùå SKIP | ‚ùå SKIP | Configuration files |
| `*.xml` | ‚ùå SKIP | ‚ùå SKIP | ‚ùå SKIP | Configuration files |
| `*.yaml`, `*.yml` | ‚ùå SKIP | ‚ùå SKIP | ‚ùå SKIP | Configuration files |
| `*.config` | ‚ùå SKIP | ‚ùå SKIP | ‚ùå SKIP | Configuration files |
| `*.md` | ‚ùå SKIP | ‚ùå SKIP | ‚ùå SKIP | Documentation (future: documentation-reviewer) |
| `*.ts`, `*.tsx` | üîÆ FUTURE | üîÆ FUTURE | üîÆ FUTURE | TypeScript support planned |
| `*.js`, `*.jsx` | üîÆ FUTURE | üîÆ FUTURE | üîÆ FUTURE | JavaScript support planned |

### Context-Based Reviewer Selection

**Review Context Modifiers**:

```typescript
/**
 * Adjusts reviewer selection based on review context
 * @param baseReviewers Set of reviewers selected by file type
 * @param context Review context (post-implementation, pre-commit, etc.)
 * @returns Adjusted set of reviewer IDs
 */
function adjustForContext(
  baseReviewers: Set<string>,
  context: ReviewContext
): Set<string> {

  switch (context) {
    case 'post-implementation':
      // Comprehensive review - use ALL selected reviewers
      return baseReviewers;

    case 'pre-commit':
      // Fast validation - skip test-healer for speed
      const fastReviewers = new Set(baseReviewers);
      fastReviewers.delete('test-healer');
      return fastReviewers;

    case 'technical-debt':
      // Deep analysis - use ALL reviewers + extended timeout
      return baseReviewers;

    case 'ad-hoc':
      // User-driven - use ALL selected reviewers
      return baseReviewers;

    default:
      return baseReviewers;
  }
}

type ReviewContext = 'post-implementation' | 'pre-commit' | 'technical-debt' | 'ad-hoc';
```

### Context Impact Table

| Review Context | Reviewer Adjustment | Timeout | Rationale |
|----------------|---------------------|---------|-----------|
| `post-implementation` | Use ALL selected reviewers | 5 min | Comprehensive post-feature review |
| `pre-commit` | SKIP test-healer | 5 min | Fast validation before commit, focus on style+principles |
| `technical-debt` | Use ALL selected reviewers | 10 min | Deep analysis requires all reviewers + more time |
| `ad-hoc` | Use ALL selected reviewers | 5 min | User-initiated, use full coverage |

### Dynamic Reviewer List Generation

**Complete Selection Algorithm**:

```typescript
/**
 * Generates final reviewer list with context adjustments
 * @param files Array of file paths to review
 * @param context Review context
 * @param explicitReviewers Optional explicit reviewer override
 * @returns Final array of reviewer IDs to invoke
 */
function generateReviewerList(
  files: string[],
  context: ReviewContext,
  explicitReviewers?: string[]
): string[] {

  // STEP 1: Check for explicit override
  if (explicitReviewers && explicitReviewers.length > 0) {
    // User explicitly specified reviewers - use those
    return explicitReviewers;
  }

  // STEP 2: Select reviewers based on file types
  const baseReviewers = selectReviewers(files);

  // STEP 3: Adjust for review context
  const adjustedReviewers = adjustForContext(baseReviewers, context);

  // STEP 4: Validate minimum reviewer requirement
  if (adjustedReviewers.size === 0) {
    throw new Error('No reviewers selected - all files are configuration/documentation');
  }

  // STEP 5: Return sorted array for consistent ordering
  return Array.from(adjustedReviewers).sort();
}
```

### Edge Case Handling

**Edge Case 1: Mixed File Types**

```typescript
// Input: Mixed C# code and test files
const files = [
  'src/Services/AuthService.cs',           // Production code
  'src/Tests/AuthServiceTests.cs',         // Test file
  'src/Program.cs'                         // Production code
];

// Expected reviewers:
// - code-style-reviewer (all .cs files)
// - code-principles-reviewer (all .cs files)
// - test-healer (test files detected)

const reviewers = selectReviewers(files);
// Result: ['code-principles-reviewer', 'code-style-reviewer', 'test-healer']
```

**Edge Case 2: Configuration Only**

```typescript
// Input: Only configuration files
const files = [
  'appsettings.json',
  'launchSettings.json',
  'web.config'
];

// Expected: Throw error (no reviewers applicable)
try {
  const reviewers = generateReviewerList(files, 'post-implementation');
} catch (error) {
  // Error: "No reviewers selected - all files are configuration/documentation"
}
```

**Edge Case 3: Pre-Commit Context Optimization**

```typescript
// Input: Mixed files with pre-commit context
const files = [
  'src/Services/AuthService.cs',
  'src/Tests/AuthServiceTests.cs'
];

// Base reviewers: ['code-style-reviewer', 'code-principles-reviewer', 'test-healer']
// Context: 'pre-commit'
// Adjustment: Remove test-healer for speed

const reviewers = generateReviewerList(files, 'pre-commit');
// Result: ['code-principles-reviewer', 'code-style-reviewer']
// Benefit: ~33% faster (2 reviewers instead of 3)
```

**Edge Case 4: Explicit Reviewer Override**

```typescript
// Input: User explicitly requests only code-style-reviewer
const files = [
  'src/Services/AuthService.cs',
  'src/Tests/AuthServiceTests.cs'
];

const reviewers = generateReviewerList(
  files,
  'post-implementation',
  ['code-style-reviewer']  // Explicit override
);

// Result: ['code-style-reviewer']
// Rationale: User override takes precedence over automatic selection
```

### Performance Considerations

**Selection Performance**:
- File type detection: O(n) where n = number of files
- Reviewer set operations: O(r) where r = number of reviewers (~3)
- Total: O(n) - linear time complexity
- Expected time: <10ms for typical file sets (10-50 files)

**Caching Strategy** (Future Enhancement):
```typescript
// Cache reviewer selection for identical file lists
const reviewerCache = new Map<string, string[]>();

function generateReviewerListCached(
  files: string[],
  context: ReviewContext
): string[] {
  const cacheKey = `${files.sort().join('|')}:${context}`;

  if (reviewerCache.has(cacheKey)) {
    return reviewerCache.get(cacheKey)!;
  }

  const reviewers = generateReviewerList(files, context);
  reviewerCache.set(cacheKey, reviewers);
  return reviewers;
}
```

### Integration with Parallel Execution

**Complete Workflow**:

```typescript
// STEP 1: Generate reviewer list
const reviewers = generateReviewerList(files, context, explicitReviewers);

// STEP 2: Build parallel Task calls
const tasks = reviewers.map(reviewerType => ({
  tool: "Task",
  params: {
    subagent_type: reviewerType,
    description: `Review ${files.length} files for ${reviewerType} issues`,
    prompt: generateReviewPrompt(reviewerType, files),
    timeout: getTimeoutForContext(context)
  }
}));

// STEP 3: Execute all tasks in parallel
const results = await executeTasks(tasks);

function getTimeoutForContext(context: ReviewContext): number {
  switch (context) {
    case 'technical-debt': return 600000;  // 10 minutes
    default: return 300000;                // 5 minutes
  }
}
```

### Validation Rules

**Pre-Execution Validation**:

```typescript
/**
 * Validates reviewer selection before execution
 * @param reviewers Selected reviewer IDs
 * @param files Files to review
 * @throws Error if validation fails
 */
function validateReviewerSelection(reviewers: string[], files: string[]): void {
  // Rule 1: At least one reviewer required
  if (reviewers.length === 0) {
    throw new Error('No reviewers selected - cannot proceed with empty reviewer list');
  }

  // Rule 2: All reviewers must be valid
  const validReviewers = ['code-style-reviewer', 'code-principles-reviewer', 'test-healer'];
  for (const reviewer of reviewers) {
    if (!validReviewers.includes(reviewer)) {
      throw new Error(`Invalid reviewer: ${reviewer}`);
    }
  }

  // Rule 3: Maximum 5 reviewers (prevent excessive parallelization)
  if (reviewers.length > 5) {
    throw new Error(`Too many reviewers: ${reviewers.length} (maximum: 5)`);
  }

  // Rule 4: At least one reviewable file
  const reviewableFiles = files.filter(f =>
    !f.match(/\.(json|xml|yaml|yml|config|md)$/)
  );

  if (reviewableFiles.length === 0) {
    throw new Error('No reviewable files - all files are configuration/documentation');
  }
}
```

### Examples

**Example 1: Production Code Only**
```typescript
Input:
  files: ['Services/AuthService.cs', 'Controllers/AuthController.cs']
  context: 'post-implementation'

Output:
  reviewers: ['code-principles-reviewer', 'code-style-reviewer']
  rationale: No test files detected, skip test-healer
```

**Example 2: Mixed Code and Tests**
```typescript
Input:
  files: [
    'Services/AuthService.cs',
    'Tests/Services/AuthServiceTests.cs',
    'Controllers/AuthController.cs'
  ]
  context: 'post-implementation'

Output:
  reviewers: ['code-principles-reviewer', 'code-style-reviewer', 'test-healer']
  rationale: Test files detected, include all reviewers
```

**Example 3: Pre-Commit Optimization**
```typescript
Input:
  files: [
    'Services/AuthService.cs',
    'Tests/Services/AuthServiceTests.cs'
  ]
  context: 'pre-commit'

Output:
  reviewers: ['code-principles-reviewer', 'code-style-reviewer']
  rationale: Pre-commit context - skip test-healer for speed (2-3 min instead of 5 min)
```

---

## Result Collection Interfaces

**Purpose**: Define standardized data structures for collecting and storing reviewer outputs

**Design Philosophy**:
- **Uniform**: All reviewers return ReviewResult regardless of internal format
- **Extensible**: Metadata field allows reviewer-specific data
- **Type-safe**: Explicit status enum prevents invalid states
- **Traceable**: Each issue has unique ID for deduplication tracking

### ReviewResult Interface

**Core Data Structure** for storing individual reviewer outputs:

```typescript
/**
 * Standard result structure returned by each reviewer
 * @interface ReviewResult
 */
interface ReviewResult {
  // Reviewer Identification
  reviewer_name: string;              // e.g., "code-style-reviewer"

  // Execution Metadata
  execution_time_ms: number;          // Time taken by reviewer (milliseconds)
  status: ReviewStatus;               // Execution outcome

  // Issues Found
  issues: Issue[];                    // Array of detected issues

  // Quality Metrics
  confidence: number;                 // Overall reviewer confidence (0.0-1.0)

  // Additional Context
  metadata: ReviewMetadata;           // Reviewer-specific metadata

  // Error Information (optional)
  error?: string;                     // Error message if status is 'error' or 'partial'
}

/**
 * Execution status enum
 */
type ReviewStatus =
  | 'success'    // Reviewer completed successfully
  | 'timeout'    // Reviewer exceeded time limit
  | 'error'      // Reviewer failed with error
  | 'partial';   // Reviewer returned partial results (some parsing failed)

/**
 * Reviewer-specific metadata
 */
interface ReviewMetadata {
  files_reviewed: number;             // Number of files analyzed
  rules_applied: number;              // Number of rules checked
  cache_hit: boolean;                 // Whether result came from cache
  version: string;                    // Reviewer version (e.g., "1.0")

  // Optional fields for specific reviewers
  test_coverage?: number;             // test-healer: coverage percentage
  architecture_score?: number;        // architecture-documenter: score
  [key: string]: any;                 // Allow additional metadata
}
```

**ReviewResult Examples**:

**Example 1: Successful Review**
```typescript
const successResult: ReviewResult = {
  reviewer_name: 'code-style-reviewer',
  execution_time_ms: 4200,
  status: 'success',
  issues: [
    /* array of issues */
  ],
  confidence: 0.95,
  metadata: {
    files_reviewed: 5,
    rules_applied: 12,
    cache_hit: false,
    version: '1.0'
  }
};
```

**Example 2: Timeout Result**
```typescript
const timeoutResult: ReviewResult = {
  reviewer_name: 'code-principles-reviewer',
  execution_time_ms: 300000,  // 5 minutes
  status: 'timeout',
  issues: [],
  confidence: 0.0,
  metadata: {
    files_reviewed: 0,
    rules_applied: 0,
    cache_hit: false,
    version: '1.0'
  },
  error: 'Reviewer exceeded 5 minute timeout'
};
```

**Example 3: Partial Result (Parse Error)**
```typescript
const partialResult: ReviewResult = {
  reviewer_name: 'test-healer',
  execution_time_ms: 3800,
  status: 'partial',
  issues: [
    /* issues that could be parsed */
  ],
  confidence: 0.5,  // Lower confidence due to partial parsing
  metadata: {
    files_reviewed: 3,  // Only 3/5 files parsed successfully
    rules_applied: 5,
    cache_hit: false,
    version: '1.0'
  },
  error: 'XML parse error on TestResults.xml, recovered partial results'
};
```

### Issue Interface

**Core Data Structure** for representing individual code issues:

```typescript
/**
 * Standardized issue structure used across all reviewers
 * @interface Issue
 */
interface Issue {
  // Unique Identification
  id: string;                         // Hash for deduplication (generated from file+line+category)

  // Location Information
  file: string;                       // Relative file path (e.g., "Services/AuthService.cs")
  line: number;                       // Line number where issue occurs
  column?: number;                    // Optional column number (if available)

  // Classification
  severity: IssueSeverity;            // Priority level (P0/P1/P2)
  category: string;                   // Issue category (e.g., "naming_convention")
  rule: string;                       // Rule identifier (e.g., "csharp-naming-PascalCase")

  // Description
  message: string;                    // Human-readable issue description
  suggestion?: string;                // Optional fix suggestion

  // Quality Metrics
  confidence: number;                 // Reviewer confidence in this issue (0.0-1.0)

  // Attribution
  reviewer: string;                   // Reviewer that detected this issue

  // Optional Context
  code_snippet?: string;              // Optional code snippet showing issue
  fix_example?: string;               // Optional example of corrected code
}

/**
 * Issue severity enum
 */
type IssueSeverity =
  | 'P0'    // Critical - Must fix immediately (blocks commit)
  | 'P1'    // Important - Should fix before merge (warnings)
  | 'P2';   // Informational - Nice to fix (improvements)

/**
 * Category constants for common issue types
 */
const IssueCategory = {
  // Code Style
  NAMING_CONVENTION: 'naming_convention',
  FORMATTING: 'formatting',
  BRACES: 'mandatory_braces',
  DOCUMENTATION: 'xml_documentation',

  // Code Principles
  SOLID_SRP: 'solid_srp',
  SOLID_OCP: 'solid_ocp',
  SOLID_LSP: 'solid_lsp',
  SOLID_ISP: 'solid_isp',
  SOLID_DIP: 'solid_dip',
  DRY_VIOLATION: 'dry_violation',
  COMPLEXITY: 'complexity',

  // Testing
  TEST_COVERAGE: 'test_coverage',
  TEST_QUALITY: 'test_quality',
  MISSING_TEST: 'missing_test',
  TEST_SMELL: 'test_smell',

  // Architecture
  CIRCULAR_DEPENDENCY: 'circular_dependency',
  ARCHITECTURAL_VIOLATION: 'architectural_violation',

  // General
  ERROR_HANDLING: 'error_handling',
  SECURITY: 'security',
  PERFORMANCE: 'performance'
} as const;
```

**Issue Hash Generation**:

```typescript
/**
 * Generates unique hash for issue deduplication
 * @param file File path
 * @param line Line number
 * @param category Issue category
 * @returns SHA-256 hash string
 */
function generateIssueHash(file: string, line: number, category: string): string {
  const composite = `${file}:${line}:${category}`;
  return sha256(composite).substring(0, 16);  // First 16 chars of hash
}

// Example usage
const issueId = generateIssueHash('Services/AuthService.cs', 42, 'naming_convention');
// Result: "a3f2c8e91b4d7f6a"
```

**Issue Examples**:

**Example 1: Style Issue (P1)**
```typescript
const styleIssue: Issue = {
  id: 'a3f2c8e91b4d7f6a',
  file: 'Services/AuthService.cs',
  line: 42,
  column: 12,
  severity: 'P1',
  category: IssueCategory.NAMING_CONVENTION,
  rule: 'csharp-naming-PascalCase',
  message: 'Variable "x" should use descriptive name following camelCase convention',
  suggestion: 'Rename to "userRequest" or similar descriptive name',
  confidence: 0.85,
  reviewer: 'code-style-reviewer',
  code_snippet: 'var x = GetUserRequest();',
  fix_example: 'var userRequest = GetUserRequest();'
};
```

**Example 2: Principle Violation (P0)**
```typescript
const principleIssue: Issue = {
  id: 'b7d4e2f8c9a1e3b5',
  file: 'Services/AuthService.cs',
  line: 85,
  severity: 'P0',
  category: IssueCategory.SOLID_DIP,
  rule: 'dependency-inversion',
  message: 'Class depends on concrete implementation instead of abstraction, violates Dependency Inversion Principle',
  suggestion: 'Inject IAuthRepository interface instead of concrete AuthRepository',
  confidence: 0.92,
  reviewer: 'code-principles-reviewer',
  code_snippet: 'private AuthRepository _repo = new AuthRepository();',
  fix_example: 'private readonly IAuthRepository _repo;\npublic AuthService(IAuthRepository repo) { _repo = repo; }'
};
```

**Example 3: Test Coverage Issue (P1)**
```typescript
const testIssue: Issue = {
  id: 'c8e5f1a9d2b7c4e6',
  file: 'Services/AuthService.cs',
  line: 120,
  severity: 'P1',
  category: IssueCategory.MISSING_TEST,
  rule: 'test-coverage-required',
  message: 'Method ProcessAuthenticationRequest has zero test coverage',
  suggestion: 'Add unit tests covering success case, failure cases, and edge cases',
  confidence: 0.88,
  reviewer: 'test-healer'
};
```

### Issue Deduplication ID Strategy

**Composite Key Design**:

The issue ID is generated from three components to enable both exact match and semantic deduplication:

```typescript
/**
 * Issue deduplication strategy
 *
 * EXACT MATCH: Uses (file, line, category) composite key
 * - Same file, same line, same category ‚Üí DUPLICATE
 * - Hash ensures fast lookup in deduplication map
 *
 * SEMANTIC MATCH: Compares message similarity if exact match fails
 * - Levenshtein distance ‚â•80% ‚Üí POTENTIAL DUPLICATE
 * - Requires additional context checks (file proximity, line proximity)
 */

// Exact match key generation
function generateExactMatchKey(issue: Issue): string {
  return generateIssueHash(issue.file, issue.line, issue.category);
}

// Semantic similarity key generation (for fallback)
function generateSemanticKey(issue: Issue): string {
  // Normalize message for comparison
  const normalized = issue.message
    .toLowerCase()
    .replace(/[^\w\s]/g, '')  // Remove punctuation
    .replace(/\s+/g, ' ')      // Normalize whitespace
    .trim();

  return sha256(normalized).substring(0, 16);
}
```

**Deduplication Example**:

```typescript
// Three reviewers report similar issues
const issue1: Issue = {
  id: generateIssueHash('AuthService.cs', 42, 'naming_convention'),
  file: 'AuthService.cs',
  line: 42,
  category: 'naming_convention',
  message: 'Variable "x" should be renamed',
  reviewer: 'code-style-reviewer',
  // ...
};

const issue2: Issue = {
  id: generateIssueHash('AuthService.cs', 42, 'naming_convention'),  // SAME ID
  file: 'AuthService.cs',
  line: 42,
  category: 'naming_convention',
  message: 'Variable "x" violates naming convention',
  reviewer: 'code-principles-reviewer',
  // ...
};

// Deduplication algorithm recognizes these as duplicates via exact match
// Result: Single merged issue with reviewers: ['code-style-reviewer', 'code-principles-reviewer']
```

### Interface Validation

**Validation Rules** for ReviewResult and Issue:

```typescript
/**
 * Validates ReviewResult structure
 * @throws Error if validation fails
 */
function validateReviewResult(result: ReviewResult): void {
  // Required fields
  if (!result.reviewer_name || typeof result.reviewer_name !== 'string') {
    throw new Error('Invalid ReviewResult: reviewer_name required');
  }

  if (typeof result.execution_time_ms !== 'number' || result.execution_time_ms < 0) {
    throw new Error('Invalid ReviewResult: execution_time_ms must be non-negative number');
  }

  if (!['success', 'timeout', 'error', 'partial'].includes(result.status)) {
    throw new Error(`Invalid ReviewResult: status must be success|timeout|error|partial, got ${result.status}`);
  }

  if (!Array.isArray(result.issues)) {
    throw new Error('Invalid ReviewResult: issues must be array');
  }

  if (typeof result.confidence !== 'number' || result.confidence < 0 || result.confidence > 1) {
    throw new Error('Invalid ReviewResult: confidence must be between 0 and 1');
  }

  // Validate each issue
  result.issues.forEach((issue, index) => {
    try {
      validateIssue(issue);
    } catch (error) {
      throw new Error(`Invalid issue at index ${index}: ${error.message}`);
    }
  });
}

/**
 * Validates Issue structure
 * @throws Error if validation fails
 */
function validateIssue(issue: Issue): void {
  if (!issue.id || typeof issue.id !== 'string') {
    throw new Error('Invalid Issue: id required');
  }

  if (!issue.file || typeof issue.file !== 'string') {
    throw new Error('Invalid Issue: file required');
  }

  if (typeof issue.line !== 'number' || issue.line < 1) {
    throw new Error('Invalid Issue: line must be positive number');
  }

  if (!['P0', 'P1', 'P2'].includes(issue.severity)) {
    throw new Error(`Invalid Issue: severity must be P0|P1|P2, got ${issue.severity}`);
  }

  if (!issue.category || typeof issue.category !== 'string') {
    throw new Error('Invalid Issue: category required');
  }

  if (!issue.message || typeof issue.message !== 'string') {
    throw new Error('Invalid Issue: message required');
  }

  if (typeof issue.confidence !== 'number' || issue.confidence < 0 || issue.confidence > 1) {
    throw new Error('Invalid Issue: confidence must be between 0 and 1');
  }

  if (!issue.reviewer || typeof issue.reviewer !== 'string') {
    throw new Error('Invalid Issue: reviewer required');
  }
}
```

---

## Result Caching System

**Purpose**: Cache reviewer results to avoid re-running expensive reviews for unchanged files

**Design Philosophy**:
- **Fast lookup**: O(1) access via Map-based storage
- **Automatic expiry**: 15-minute TTL prevents stale results
- **File-based keys**: Hash file contents for cache invalidation on changes
- **Memory efficient**: In-memory only (no persistence overhead)

### ResultCache Class

**Core Caching Implementation**:

```typescript
/**
 * In-memory cache for reviewer results with automatic TTL expiry
 * @class ResultCache
 */
class ResultCache {
  // Private storage
  private cache: Map<string, CachedResult>;
  private readonly TTL: number;

  /**
   * Constructor
   * @param ttl Time-to-live in milliseconds (default: 900000 = 15 minutes)
   */
  constructor(ttl: number = 900000) {
    this.cache = new Map<string, CachedResult>();
    this.TTL = ttl;

    // Start automatic cleanup interval (every 5 minutes)
    this.startCleanupInterval();
  }

  /**
   * Stores reviewer result in cache
   * @param key Cache key (generated from files + reviewer)
   * @param result ReviewResult to cache
   */
  store(key: string, result: ReviewResult): void {
    const now = Date.now();

    this.cache.set(key, {
      result: result,
      timestamp: now,
      expires: now + this.TTL
    });

    console.log(`[Cache] Stored result for key: ${key} (expires in ${this.TTL / 1000}s)`);
  }

  /**
   * Retrieves reviewer result from cache
   * @param key Cache key
   * @returns ReviewResult if cached and not expired, null otherwise
   */
  retrieve(key: string): ReviewResult | null {
    const cached = this.cache.get(key);

    if (!cached) {
      console.log(`[Cache] MISS for key: ${key}`);
      return null;
    }

    const now = Date.now();

    // Check if expired
    if (now > cached.expires) {
      console.log(`[Cache] EXPIRED for key: ${key} (age: ${(now - cached.timestamp) / 1000}s)`);
      this.cache.delete(key);
      return null;
    }

    console.log(`[Cache] HIT for key: ${key} (age: ${(now - cached.timestamp) / 1000}s)`);

    // Update cache hit flag in metadata
    const result = { ...cached.result };
    result.metadata = { ...result.metadata, cache_hit: true };

    return result;
  }

  /**
   * Generates cache key from file list and reviewer ID
   * @param files Array of file paths
   * @param reviewer Reviewer ID
   * @returns Cache key string
   */
  getCacheKey(files: string[], reviewer: string): string {
    // Sort files for consistent key generation
    const sortedFiles = [...files].sort();

    // Generate file hash (combined hash of all file contents)
    const fileHash = this.hashFiles(sortedFiles);

    // Combine with reviewer ID
    return `${reviewer}:${fileHash}`;
  }

  /**
   * Hashes file list for cache key generation
   * @param files Sorted array of file paths
   * @returns SHA-256 hash of file paths and modification times
   */
  private hashFiles(files: string[]): string {
    // In real implementation, hash file contents + modification time
    // For specification, we use file paths + timestamps
    const composite = files.map(file => {
      // Include file path and last modified time
      const mtime = getFileModificationTime(file);
      return `${file}:${mtime}`;
    }).join('|');

    return sha256(composite).substring(0, 16);
  }

  /**
   * Invalidates cache entries for specific files
   * @param files Array of file paths that changed
   */
  invalidate(files: string[]): void {
    let invalidated = 0;

    // Find and delete cache entries containing these files
    for (const [key, cached] of this.cache.entries()) {
      // Check if key contains any of the changed files
      // (Keys are format: "reviewer:file_hash")
      const fileHash = key.split(':')[1];

      // Re-generate hash for changed files
      const newFileHash = this.hashFiles(files);

      if (fileHash === newFileHash) {
        this.cache.delete(key);
        invalidated++;
      }
    }

    if (invalidated > 0) {
      console.log(`[Cache] Invalidated ${invalidated} entries due to file changes`);
    }
  }

  /**
   * Clears all cache entries
   */
  clear(): void {
    const size = this.cache.size;
    this.cache.clear();
    console.log(`[Cache] Cleared ${size} entries`);
  }

  /**
   * Gets current cache statistics
   * @returns Cache statistics object
   */
  getStats(): CacheStats {
    const now = Date.now();
    let expired = 0;
    let valid = 0;

    for (const [key, cached] of this.cache.entries()) {
      if (now > cached.expires) {
        expired++;
      } else {
        valid++;
      }
    }

    return {
      total_entries: this.cache.size,
      valid_entries: valid,
      expired_entries: expired,
      ttl_ms: this.TTL,
      memory_estimate_kb: this.estimateMemoryUsage()
    };
  }

  /**
   * Estimates memory usage of cache
   * @returns Estimated memory in KB
   */
  private estimateMemoryUsage(): number {
    // Rough estimate: 10KB per cached result
    return this.cache.size * 10;
  }

  /**
   * Starts automatic cleanup interval
   * Removes expired entries every 5 minutes
   */
  private startCleanupInterval(): void {
    setInterval(() => {
      this.cleanup();
    }, 300000);  // 5 minutes
  }

  /**
   * Removes expired entries from cache
   */
  private cleanup(): void {
    const now = Date.now();
    let removed = 0;

    for (const [key, cached] of this.cache.entries()) {
      if (now > cached.expires) {
        this.cache.delete(key);
        removed++;
      }
    }

    if (removed > 0) {
      console.log(`[Cache] Cleanup removed ${removed} expired entries`);
    }
  }
}

/**
 * Cached result with expiration metadata
 */
interface CachedResult {
  result: ReviewResult;
  timestamp: number;  // When cached (milliseconds since epoch)
  expires: number;    // Expiration time (milliseconds since epoch)
}

/**
 * Cache statistics structure
 */
interface CacheStats {
  total_entries: number;
  valid_entries: number;
  expired_entries: number;
  ttl_ms: number;
  memory_estimate_kb: number;
}

/**
 * Helper function to get file modification time
 * (Placeholder - implementation depends on environment)
 */
function getFileModificationTime(file: string): number {
  // In Node.js: fs.statSync(file).mtime.getTime()
  // In browser: Not applicable (no file system access)
  // For specification: return mock timestamp
  return Date.now();
}
```

### Cache Integration with Parallel Execution

**Complete Workflow with Caching**:

```typescript
// Global cache instance
const reviewerCache = new ResultCache(900000);  // 15-minute TTL

/**
 * Launches parallel reviews with cache checking
 * @param files Files to review
 * @param reviewers Reviewer IDs to invoke
 * @returns Promise resolving to reviewer results
 */
async function launchParallelReviewsWithCache(
  files: string[],
  reviewers: string[]
): Promise<ReviewResult[]> {

  const results: ReviewResult[] = [];
  const reviewersToRun: string[] = [];

  // STEP 1: Check cache for each reviewer
  for (const reviewer of reviewers) {
    const cacheKey = reviewerCache.getCacheKey(files, reviewer);
    const cachedResult = reviewerCache.retrieve(cacheKey);

    if (cachedResult) {
      // Cache hit - use cached result
      console.log(`‚úÖ Cache HIT for ${reviewer} (saved ~4-5 minutes)`);
      results.push(cachedResult);
    } else {
      // Cache miss - need to run reviewer
      console.log(`‚ùå Cache MISS for ${reviewer} (will execute)`);
      reviewersToRun.push(reviewer);
    }
  }

  // STEP 2: Run reviewers that had cache misses
  if (reviewersToRun.length > 0) {
    console.log(`\nRunning ${reviewersToRun.length}/${reviewers.length} reviewers (${reviewers.length - reviewersToRun.length} cached)\n`);

    const newResults = await launchParallelReviews(files, reviewersToRun);

    // STEP 3: Store new results in cache
    for (const result of newResults) {
      const cacheKey = reviewerCache.getCacheKey(files, result.reviewer_name);
      reviewerCache.store(cacheKey, result);
    }

    results.push(...newResults);
  } else {
    console.log(`\n‚úÖ All ${reviewers.length} reviewers served from cache (0 executions needed)\n`);
  }

  return results;
}
```

### Cache Performance Benefits

**Cache Hit Scenarios**:

**Scenario 1: Re-review After Small Change**
```typescript
// Initial review (all cache misses)
const files = ['AuthService.cs', 'UserService.cs', 'TokenService.cs'];
const reviewers = ['code-style-reviewer', 'code-principles-reviewer', 'test-healer'];

// Time: 5 minutes (parallel execution)
await launchParallelReviewsWithCache(files, reviewers);

// User fixes one issue in AuthService.cs
// File hash changes for AuthService.cs

// Re-review (cache invalidation for affected reviewers)
const results = await launchParallelReviewsWithCache(files, reviewers);

// Cache behavior:
// - AuthService.cs changed ‚Üí cache keys invalidated
// - UserService.cs, TokenService.cs unchanged ‚Üí cache keys still valid
// - Result: Partial cache hit (some reviewers served from cache)
// - Time: 2-3 minutes (only re-review changed files)
```

**Scenario 2: Multiple Reviews of Same Files**
```typescript
// First review (all cache misses)
await launchParallelReviewsWithCache(files, reviewers);  // 5 minutes

// Second review within 15 minutes (all cache hits)
await launchParallelReviewsWithCache(files, reviewers);  // <1 second

// Cache savings: 99.7% reduction (5 minutes ‚Üí <1 second)
```

**Scenario 3: Different Reviewers, Same Files**
```typescript
// Initial review with 2 reviewers
await launchParallelReviewsWithCache(files, ['code-style-reviewer', 'code-principles-reviewer']);

// Later review with 3 reviewers (add test-healer)
await launchParallelReviewsWithCache(files, ['code-style-reviewer', 'code-principles-reviewer', 'test-healer']);

// Cache behavior:
// - code-style-reviewer: CACHE HIT
// - code-principles-reviewer: CACHE HIT
// - test-healer: CACHE MISS (first time running)
// - Time: ~2 minutes (only test-healer runs)
```

### Cache Invalidation Strategies

**File Change Detection**:

```typescript
/**
 * Watches files for changes and invalidates cache
 * @param files Files to watch
 */
function watchFilesForChanges(files: string[]): void {
  // Use file system watcher (e.g., fs.watch in Node.js)
  for (const file of files) {
    watchFile(file, (event) => {
      if (event === 'change') {
        console.log(`[Cache] File changed: ${file}, invalidating cache`);
        reviewerCache.invalidate([file]);
      }
    });
  }
}

/**
 * Manual cache invalidation for specific reviewers
 * @param files Files that changed
 * @param reviewers Reviewers to invalidate (optional, default: all)
 */
function invalidateCacheForFiles(files: string[], reviewers?: string[]): void {
  if (reviewers) {
    // Invalidate specific reviewers
    for (const reviewer of reviewers) {
      const cacheKey = reviewerCache.getCacheKey(files, reviewer);
      reviewerCache.cache.delete(cacheKey);
    }
  } else {
    // Invalidate all reviewers for these files
    reviewerCache.invalidate(files);
  }
}
```

**Time-Based Expiration**:

```typescript
// Cache entries automatically expire after 15 minutes (TTL)
// Rationale:
// - Files may change without file system events (git operations, external edits)
// - Code rules may be updated (new rules in .cursor/rules/*.mdc)
// - 15 minutes balances freshness vs performance

// Cleanup runs every 5 minutes to remove expired entries
// Memory usage: ~10KB per cached result √ó ~10 reviewers √ó 3 results = ~300KB
```

### Cache Statistics and Monitoring

**Usage Example**:

```typescript
// Get cache statistics
const stats = reviewerCache.getStats();

console.log(`
Cache Statistics:
  Total entries: ${stats.total_entries}
  Valid entries: ${stats.valid_entries}
  Expired entries: ${stats.expired_entries}
  TTL: ${stats.ttl_ms / 1000} seconds
  Memory usage: ~${stats.memory_estimate_kb} KB
`);

// Example output:
// Cache Statistics:
//   Total entries: 9
//   Valid entries: 9
//   Expired entries: 0
//   TTL: 900 seconds
//   Memory usage: ~90 KB
```

**Cache Hit Rate Tracking**:

```typescript
/**
 * Tracks cache hit rate over time
 */
class CacheMetrics {
  private hits = 0;
  private misses = 0;

  recordHit(): void {
    this.hits++;
  }

  recordMiss(): void {
    this.misses++;
  }

  getHitRate(): number {
    const total = this.hits + this.misses;
    return total > 0 ? this.hits / total : 0;
  }

  reset(): void {
    this.hits = 0;
    this.misses = 0;
  }

  getStats(): { hits: number; misses: number; hitRate: number } {
    return {
      hits: this.hits,
      misses: this.misses,
      hitRate: this.getHitRate()
    };
  }
}

// Global metrics tracker
const cacheMetrics = new CacheMetrics();

// Update retrieve method to track metrics
retrieve(key: string): ReviewResult | null {
  const cached = this.cache.get(key);

  if (!cached || Date.now() > cached.expires) {
    cacheMetrics.recordMiss();
    return null;
  }

  cacheMetrics.recordHit();
  return cached.result;
}

// Display metrics
const metrics = cacheMetrics.getStats();
console.log(`Cache Hit Rate: ${(metrics.hitRate * 100).toFixed(1)}% (${metrics.hits} hits, ${metrics.misses} misses)`);
// Output: "Cache Hit Rate: 75.0% (6 hits, 2 misses)"
```

### Performance Impact

**Expected Cache Performance**:

| Scenario | Without Cache | With Cache | Speedup |
|----------|--------------|------------|---------|
| All hits (no file changes) | 5 minutes | <1 second | 300x |
| Partial hit (1/3 files changed) | 5 minutes | ~2 minutes | 2.5x |
| All misses (all files changed) | 5 minutes | 5 minutes | 1x (no benefit) |

**Memory Usage**:

| Cache Size | Entries | Memory | Notes |
|------------|---------|--------|-------|
| Small | 10 entries | ~100 KB | Typical for single-feature reviews |
| Medium | 50 entries | ~500 KB | Multi-feature concurrent reviews |
| Large | 100 entries | ~1 MB | Large-scale refactoring reviews |

**Cleanup Overhead**:

- Cleanup runs every 5 minutes
- Time complexity: O(n) where n = cache size
- Typical cleanup time: <10ms for 100 entries
- Memory freed: ~10KB per expired entry

---

## Algorithm Components

### 1. Issue Deduplication

**Purpose**: Eliminate duplicate issues reported by multiple reviewers

**Algorithm**:

```
INPUT: List of issues from all reviewers
  Issue = {
    file_path: string,
    line_number: int,
    issue_type: string,
    description: string,
    priority: P0|P1|P2,
    confidence: float (0.0-1.0),
    reviewer_id: string
  }

OUTPUT: Deduplicated list of issues with aggregated metadata

STEP 1: Exact Match Deduplication (Fast Path)
  - Group issues by composite key: (file_path, line_number, issue_type)
  - Issues with identical keys are exact duplicates
  - Time complexity: O(n) using HashMap
  - Expected reduction: 40-50% of duplicates

STEP 2: Semantic Similarity Deduplication (Slow Path)
  - For remaining issues, compute pairwise similarity
  - Similarity metric: Levenshtein distance on description field
  - Threshold: ‚â•80% similarity ‚Üí consider duplicate
  - Time complexity: O(n¬≤) with early exit optimization
  - Expected reduction: 10-20% additional duplicates

STEP 3: Duplicate Grouping
  - Merge duplicate issues into single entry
  - Preserve all reviewer IDs (for confidence calculation)
  - Preserve highest priority (for priority aggregation)
  - Merge descriptions (concatenate unique descriptions)
```

**Exact Match Implementation**:

```python
def exact_match_deduplicate(issues):
    exact_match_map = {}

    for issue in issues:
        key = (issue.file_path, issue.line_number, issue.issue_type)

        if key in exact_match_map:
            # Duplicate found - merge metadata
            exact_match_map[key].reviewers.append(issue.reviewer_id)
            exact_match_map[key].priorities.append(issue.priority)
            exact_match_map[key].confidences.append(issue.confidence)
            if issue.description not in exact_match_map[key].descriptions:
                exact_match_map[key].descriptions.append(issue.description)
        else:
            # First occurrence - create new entry
            exact_match_map[key] = {
                'file_path': issue.file_path,
                'line_number': issue.line_number,
                'issue_type': issue.issue_type,
                'reviewers': [issue.reviewer_id],
                'priorities': [issue.priority],
                'confidences': [issue.confidence],
                'descriptions': [issue.description]
            }

    return exact_match_map.values()
```

**Semantic Similarity Implementation**:

```python
def semantic_similarity_deduplicate(issues, threshold=0.80):
    deduplicated = []

    for issue in issues:
        is_duplicate = False

        for existing in deduplicated:
            similarity = levenshtein_similarity(issue.description, existing.description)

            if similarity >= threshold:
                # Semantic duplicate found - merge
                existing.reviewers.append(issue.reviewer_id)
                existing.priorities.append(issue.priority)
                existing.confidences.append(issue.confidence)
                if issue.description not in existing.descriptions:
                    existing.descriptions.append(issue.description)
                is_duplicate = True
                break

        if not is_duplicate:
            deduplicated.append(issue)

    return deduplicated

def levenshtein_similarity(str1, str2):
    distance = levenshtein_distance(str1, str2)
    max_length = max(len(str1), len(str2))
    return 1.0 - (distance / max_length)
```

**Performance Optimization**:

- **Hash-based indexing**: O(1) lookup for exact matches
- **Early exit**: Stop Levenshtein calculation if distance exceeds threshold
- **Batch processing**: Process issues in batches of 100 to reduce memory overhead
- **Caching**: Cache Levenshtein results for repeated comparisons

**Expected Performance**:
- Exact match: <50ms for 500 issues
- Semantic similarity: <1.5s for 500 issues (with optimizations)
- Total deduplication: <2s for typical 3-reviewer report (~150 issues)

---

### 2. Priority Aggregation

**Purpose**: Determine final priority for deduplicated issues based on reviewer consensus

**Algorithm**:

```
INPUT: Deduplicated issue with multiple priority ratings
  Issue = {
    priorities: [P0, P1, P2, ...],  # From all reviewers
    reviewers: [reviewer_id1, reviewer_id2, ...]
  }

OUTPUT: Single aggregated priority (P0, P1, or P2)

LOGIC:
  IF ANY(priorities == P0):
    RETURN P0  # Critical issue - ANY reviewer marking P0 escalates

  ELSE IF COUNT(priorities == P1) >= LEN(reviewers) / 2:
    RETURN P1  # Majority consensus - ‚â•50% reviewers agree on P1

  ELSE:
    RETURN P2  # Default - informational/low-priority
```

**Priority Rules**:

| Condition | Aggregated Priority | Rationale |
|-----------|---------------------|-----------|
| ANY reviewer marks P0 | **P0** | Critical issues cannot be ignored - single reviewer veto |
| ‚â•50% reviewers mark P1 | **P1** | Majority consensus indicates importance |
| Otherwise | **P2** | Default to low priority (informational) |

**Implementation**:

```python
def aggregate_priority(priorities, reviewers):
    # Rule 1: ANY P0 ‚Üí escalate to P0
    if any(p == 'P0' for p in priorities):
        return 'P0'

    # Rule 2: Majority P1 ‚Üí aggregate to P1
    p1_count = sum(1 for p in priorities if p == 'P1')
    if p1_count >= len(reviewers) / 2:
        return 'P1'

    # Rule 3: Default to P2
    return 'P2'
```

**Examples**:

| Reviewers | Priorities | Aggregated | Explanation |
|-----------|------------|------------|-------------|
| 3 reviewers | [P0, P1, P2] | **P0** | ANY P0 ‚Üí escalate to P0 |
| 3 reviewers | [P1, P1, P2] | **P1** | 2/3 (66%) agree on P1 ‚Üí P1 |
| 3 reviewers | [P1, P2, P2] | **P2** | Only 1/3 (33%) mark P1 ‚Üí default P2 |
| 2 reviewers | [P1, P2] | **P1** | 1/2 (50%) mark P1 ‚Üí P1 (tie-breaker) |
| 1 reviewer | [P2] | **P2** | Single reviewer ‚Üí use their priority |

**Edge Cases**:

- **Single reviewer**: Use their priority directly (no aggregation needed)
- **Tie-breaker (50% split)**: Round up to higher priority (P1 > P2)
- **Empty priorities**: Default to P2, log warning
- **Invalid priorities**: Reject issue, log error

---

### 3. Confidence Calculation

**Purpose**: Calculate weighted confidence score for deduplicated issues

**Algorithm**:

```
INPUT: Deduplicated issue with multiple confidence scores
  Issue = {
    confidences: [0.85, 0.92, 0.78, ...],  # From all reviewers
    reviewers: [reviewer_id1, reviewer_id2, ...]
  }

OUTPUT: Single weighted confidence score (0.0-1.0)

FORMULA:
  weighted_confidence = Œ£(reviewer_confidence √ó reviewer_weight) / Œ£(reviewer_weight)

WEIGHTS:
  - test-healer: 1.2  (expertise in test analysis)
  - code-style-reviewer: 1.0  (baseline weight)
  - code-principles-reviewer: 1.0  (baseline weight)
  - default: 1.0  (unknown reviewers)
```

**Implementation**:

```python
REVIEWER_WEIGHTS = {
    'test-healer': 1.2,
    'code-style-reviewer': 1.0,
    'code-principles-reviewer': 1.0,
    'architecture-documenter': 1.0,
    'default': 1.0
}

def calculate_weighted_confidence(confidences, reviewers):
    if len(confidences) == 0:
        return 0.0  # No confidence data

    weighted_sum = 0.0
    weight_sum = 0.0

    for confidence, reviewer_id in zip(confidences, reviewers):
        weight = REVIEWER_WEIGHTS.get(reviewer_id, REVIEWER_WEIGHTS['default'])
        weighted_sum += confidence * weight
        weight_sum += weight

    return weighted_sum / weight_sum
```

**Examples**:

**Example 1: Single reviewer**
```
Input:
  confidences: [0.85]
  reviewers: ['code-style-reviewer']

Calculation:
  weighted_sum = 0.85 √ó 1.0 = 0.85
  weight_sum = 1.0
  confidence = 0.85 / 1.0 = 0.85

Output: 0.85
```

**Example 2: Multiple reviewers, equal weights**
```
Input:
  confidences: [0.85, 0.92, 0.78]
  reviewers: ['code-style-reviewer', 'code-principles-reviewer', 'architecture-documenter']

Calculation:
  weighted_sum = (0.85√ó1.0) + (0.92√ó1.0) + (0.78√ó1.0) = 2.55
  weight_sum = 1.0 + 1.0 + 1.0 = 3.0
  confidence = 2.55 / 3.0 = 0.85

Output: 0.85
```

**Example 3: Multiple reviewers, test-healer weighted higher**
```
Input:
  confidences: [0.85, 0.95]
  reviewers: ['code-style-reviewer', 'test-healer']

Calculation:
  weighted_sum = (0.85√ó1.0) + (0.95√ó1.2) = 0.85 + 1.14 = 1.99
  weight_sum = 1.0 + 1.2 = 2.2
  confidence = 1.99 / 2.2 = 0.905

Output: 0.91 (rounded)
```

**Confidence Interpretation**:

| Confidence Range | Interpretation | Action |
|------------------|----------------|--------|
| 0.90-1.00 | Very High | Include in report, high priority |
| 0.80-0.89 | High | Include in report |
| 0.60-0.79 | Medium | Include with caveat |
| 0.40-0.59 | Low | Consider filtering (optional) |
| 0.00-0.39 | Very Low | Filter from final report |

---

### 4. Recommendation Synthesis

**Purpose**: Group and rank reviewer recommendations by theme and frequency

**Algorithm**:

```
INPUT: List of recommendations from all reviewers
  Recommendation = {
    text: string,
    reviewer_id: string,
    confidence: float (0.0-1.0)
  }

OUTPUT: Synthesized, ranked recommendations grouped by theme

STEP 1: Keyword Extraction
  - Extract keywords from recommendation text
  - Patterns: "refactor", "extract method", "add tests", "improve naming"
  - Use NLP or regex-based extraction

STEP 2: Theme Grouping
  - Group recommendations with similar keywords into themes
  - Themes: "Refactoring", "Testing", "Naming", "Architecture", etc.

STEP 3: Frequency Counting
  - Count how many reviewers recommend each theme
  - Frequency = number of reviewers suggesting theme

STEP 4: Confidence Filtering
  - Filter out recommendations with confidence <60%
  - Low-confidence recommendations are not actionable

STEP 5: Ranking
  - Rank themes by frequency (descending)
  - Tie-breaker: Average confidence (higher confidence wins)
  - Return top 5-10 themes
```

**Keyword Extraction Implementation**:

```python
KEYWORD_PATTERNS = {
    'refactoring': r'\b(refactor|extract|simplify|reduce complexity)\b',
    'testing': r'\b(test|coverage|assert|mock)\b',
    'naming': r'\b(rename|naming|identifier|variable name)\b',
    'architecture': r'\b(architecture|design|pattern|structure)\b',
    'performance': r'\b(performance|optimize|cache|efficiency)\b',
    'security': r'\b(security|authentication|authorization|validation)\b',
    'documentation': r'\b(document|comment|xml doc|readme)\b',
    'error_handling': r'\b(error|exception|try-catch|validation)\b'
}

def extract_keywords(recommendation_text):
    keywords = []
    for theme, pattern in KEYWORD_PATTERNS.items():
        if re.search(pattern, recommendation_text, re.IGNORECASE):
            keywords.append(theme)
    return keywords
```

**Theme Grouping Implementation**:

```python
def group_by_theme(recommendations):
    themes = {}

    for rec in recommendations:
        # Filter low-confidence recommendations
        if rec.confidence < 0.60:
            continue

        keywords = extract_keywords(rec.text)

        for keyword in keywords:
            if keyword not in themes:
                themes[keyword] = {
                    'recommendations': [],
                    'reviewers': set(),
                    'confidences': []
                }

            themes[keyword]['recommendations'].append(rec.text)
            themes[keyword]['reviewers'].add(rec.reviewer_id)
            themes[keyword]['confidences'].append(rec.confidence)

    return themes
```

**Ranking Implementation**:

```python
def rank_themes(themes):
    ranked = []

    for theme_name, theme_data in themes.items():
        frequency = len(theme_data['reviewers'])
        avg_confidence = sum(theme_data['confidences']) / len(theme_data['confidences'])

        ranked.append({
            'theme': theme_name,
            'frequency': frequency,
            'avg_confidence': avg_confidence,
            'recommendations': theme_data['recommendations']
        })

    # Sort by frequency (descending), then by confidence (descending)
    ranked.sort(key=lambda x: (x['frequency'], x['avg_confidence']), reverse=True)

    # Return top 5-10 themes
    return ranked[:10]
```

**Output Format**:

```markdown
## Synthesized Recommendations

### Top Themes (by frequency)

1. **Refactoring** (3 reviewers, 87% confidence)
   - Extract method for complex conditional logic (code-principles-reviewer)
   - Reduce cyclomatic complexity in ProcessRequest method (code-style-reviewer)
   - Simplify nested if-statements (test-healer)

2. **Testing** (3 reviewers, 82% confidence)
   - Add unit tests for edge cases (test-healer)
   - Increase code coverage to ‚â•80% (code-principles-reviewer)
   - Mock external dependencies in tests (test-healer)

3. **Naming** (2 reviewers, 75% confidence)
   - Rename variable 'x' to 'userRequest' (code-style-reviewer)
   - Use descriptive method names instead of abbreviations (code-principles-reviewer)

4. **Error Handling** (2 reviewers, 71% confidence)
   - Add try-catch for database exceptions (code-principles-reviewer)
   - Validate input parameters before processing (test-healer)

5. **Documentation** (1 reviewer, 68% confidence)
   - Add XML documentation to public methods (code-style-reviewer)
```

**Filtering Logic**:

- **Confidence threshold**: 60% (configurable)
- **Minimum frequency**: 1 reviewer (no minimum, all themes included)
- **Maximum themes**: 10 (configurable)

---

## Performance Considerations

### Time Complexity

| Step | Algorithm | Complexity | Typical Time |
|------|-----------|------------|--------------|
| Exact Match Deduplication | HashMap grouping | O(n) | <50ms |
| Semantic Similarity | Levenshtein pairwise | O(n¬≤) | <1.5s |
| Priority Aggregation | Linear scan | O(n) | <10ms |
| Confidence Calculation | Linear scan | O(n) | <10ms |
| Recommendation Synthesis | Keyword extraction + grouping | O(m√ók) | <100ms |
| **TOTAL** | - | **O(n¬≤)** | **<2s** |

**Where**:
- n = number of issues (~150 for 3 reviewers)
- m = number of recommendations (~30 for 3 reviewers)
- k = number of keyword patterns (~8)

### Space Complexity

| Data Structure | Size | Memory |
|----------------|------|--------|
| Issue HashMap (exact match) | O(n) | ~50KB |
| Levenshtein cache | O(n¬≤) | ~2MB (with cache) |
| Theme grouping | O(m) | ~10KB |
| **TOTAL** | **O(n¬≤)** | **~2MB** |

### Optimization Strategies

**1. Early Exit for Levenshtein**:
```python
def levenshtein_distance_optimized(str1, str2, max_distance):
    if abs(len(str1) - len(str2)) > max_distance:
        return max_distance + 1  # Early exit - strings too different

    # Standard Levenshtein calculation
    # ...
```

**2. Batch Processing**:
```python
def deduplicate_in_batches(issues, batch_size=100):
    batches = [issues[i:i+batch_size] for i in range(0, len(issues), batch_size)]

    deduplicated = []
    for batch in batches:
        deduplicated.extend(semantic_similarity_deduplicate(batch))

    return deduplicated
```

**3. Caching Similarity Scores**:
```python
similarity_cache = {}

def cached_similarity(str1, str2):
    key = (min(str1, str2), max(str1, str2))  # Order-independent key

    if key not in similarity_cache:
        similarity_cache[key] = levenshtein_similarity(str1, str2)

    return similarity_cache[key]
```

**Expected Deduplication Rates**:
- Exact match: 40-50% reduction
- Semantic similarity: 10-20% additional reduction
- Total: 60-70% reduction in issue count

**Example**:
```
Input: 150 issues (3 reviewers √ó 50 issues each)
After exact match: 90 issues (40% reduction)
After semantic similarity: 50 issues (additional 44% reduction)
Total reduction: 67%
```

---

## Edge Cases

### Edge Case 1: Single Reviewer

**Scenario**: Only one reviewer provided a report (no aggregation needed)

**Handling**:
```python
def consolidate(reviews):
    if len(reviews) == 1:
        # No aggregation needed - return report as-is
        return {
            'issues': reviews[0].issues,
            'recommendations': reviews[0].recommendations,
            'note': 'Single reviewer - no aggregation performed'
        }

    # Standard consolidation for multiple reviewers
    # ...
```

**Output**:
```markdown
## Consolidated Report

**Note**: Only one reviewer (code-style-reviewer) provided a report. No aggregation performed.

### Issues (12 total)
[All issues from code-style-reviewer]

### Recommendations (5 total)
[All recommendations from code-style-reviewer]
```

### Edge Case 2: Conflicting Priorities

**Scenario**: One reviewer marks issue as P0 (critical), another as P2 (informational)

**Handling**:
```python
def aggregate_priority_with_note(priorities, reviewers):
    aggregated = aggregate_priority(priorities, reviewers)

    # Detect conflicts (P0 from one reviewer, P2 from another)
    if 'P0' in priorities and 'P2' in priorities:
        note = f"Priority conflict detected: {priorities}. Escalated to {aggregated} per ANY P0 rule."
        return aggregated, note

    return aggregated, None
```

**Output**:
```markdown
### Issue: Missing null check in ProcessRequest method

**Priority**: P0 (Critical)
**Note**: Priority conflict detected: [P0, P2, P2]. Escalated to P0 per ANY P0 rule.

**Reviewers**: code-principles-reviewer (P0), code-style-reviewer (P2), test-healer (P2)
```

**Rationale**: P0 always wins - critical issues cannot be ignored, even if other reviewers disagree.

### Edge Case 3: Empty Reports

**Scenario**: One or more reviewers returned empty reports (no issues found)

**Handling**:
```python
def consolidate(reviews):
    non_empty_reviews = [r for r in reviews if len(r.issues) > 0]

    if len(non_empty_reviews) == 0:
        # All reviewers found no issues
        return {
            'issues': [],
            'recommendations': [],
            'note': 'All reviewers found no issues - code quality excellent!'
        }

    # Consolidate non-empty reviews only
    # ...
```

**Output**:
```markdown
## Consolidated Report

**Reviews Received**: 3
- code-style-reviewer: 0 issues
- code-principles-reviewer: 12 issues
- test-healer: 8 issues

### Consolidated Issues (15 total after deduplication)
[Issues from code-principles-reviewer and test-healer]
```

### Edge Case 4: Identical Issues from All Reviewers

**Scenario**: All reviewers report the exact same issue (100% consensus)

**Handling**:
```python
def consolidate_with_consensus_tracking(issues):
    deduplicated = exact_match_deduplicate(issues)

    for issue in deduplicated:
        if len(issue.reviewers) == len(all_reviewers):
            issue.consensus = True  # 100% consensus
            issue.confidence = 1.0  # Maximum confidence
```

**Output**:
```markdown
### Issue: Missing null check in ProcessRequest method

**Priority**: P0 (Critical)
**Confidence**: 1.00 (100% consensus)
**Reviewers**: code-style-reviewer, code-principles-reviewer, test-healer

**Note**: All reviewers independently identified this issue - highest confidence.
```

**Rationale**: 100% consensus = maximum confidence, highest priority escalation.

### Edge Case 5: Semantic Similarity False Positives

**Scenario**: Two issues have similar descriptions but are actually different issues

**Example**:
```
Issue A: "Missing null check on userRequest parameter"
Issue B: "Missing null check on requestContext parameter"

Levenshtein similarity: 87% (above 80% threshold)
```

**Handling**:

```python
def semantic_similarity_with_context(issue1, issue2, threshold=0.80):
    # Check description similarity
    desc_similarity = levenshtein_similarity(issue1.description, issue2.description)

    if desc_similarity < threshold:
        return False  # Not similar

    # Additional context checks to avoid false positives
    if issue1.file_path != issue2.file_path:
        return False  # Different files - not duplicates

    if abs(issue1.line_number - issue2.line_number) > 10:
        return False  # Lines too far apart - likely different issues

    return True  # Similar and in same context
```

**Mitigation**:
- Add context checks: file path, line number proximity
- Lower similarity threshold to 75% (configurable)
- Manual review for borderline cases (80-85% similarity)

---

## Examples

### Example 1: Exact Match Deduplication

**Input (3 reviewers)**:
```json
[
  {
    "reviewer": "code-style-reviewer",
    "file": "Services/AuthService.cs",
    "line": 42,
    "issue_type": "naming_convention",
    "description": "Variable 'x' should be renamed to descriptive name",
    "priority": "P1",
    "confidence": 0.85
  },
  {
    "reviewer": "code-principles-reviewer",
    "file": "Services/AuthService.cs",
    "line": 42,
    "issue_type": "naming_convention",
    "description": "Variable 'x' violates naming convention",
    "priority": "P2",
    "confidence": 0.78
  },
  {
    "reviewer": "test-healer",
    "file": "Services/AuthService.cs",
    "line": 42,
    "issue_type": "naming_convention",
    "description": "Rename variable 'x' to 'userRequest'",
    "priority": "P1",
    "confidence": 0.92
  }
]
```

**Processing**:
```
1. Exact Match Grouping:
   Key: ("Services/AuthService.cs", 42, "naming_convention")
   Matches: All 3 issues

2. Priority Aggregation:
   Priorities: [P1, P2, P1]
   Count(P1): 2/3 (66%)
   Result: P1 (majority consensus)

3. Confidence Calculation:
   Weighted sum: (0.85√ó1.0) + (0.78√ó1.0) + (0.92√ó1.2) = 2.734
   Weight sum: 1.0 + 1.0 + 1.2 = 3.2
   Confidence: 2.734 / 3.2 = 0.854
```

**Output**:
```json
{
  "file": "Services/AuthService.cs",
  "line": 42,
  "issue_type": "naming_convention",
  "description": "Variable 'x' should be renamed to descriptive name (e.g., 'userRequest')",
  "priority": "P1",
  "confidence": 0.85,
  "reviewers": ["code-style-reviewer", "code-principles-reviewer", "test-healer"],
  "note": "All reviewers identified this issue - high confidence"
}
```

### Example 2: Semantic Similarity Detection

**Input**:
```json
[
  {
    "reviewer": "code-principles-reviewer",
    "file": "Services/AuthService.cs",
    "line": 85,
    "issue_type": "error_handling",
    "description": "Method does not handle database connection exceptions",
    "priority": "P0",
    "confidence": 0.88
  },
  {
    "reviewer": "test-healer",
    "file": "Services/AuthService.cs",
    "line": 87,
    "issue_type": "error_handling",
    "description": "Missing try-catch for database exceptions",
    "priority": "P1",
    "confidence": 0.82
  }
]
```

**Processing**:
```
1. Exact Match: No match (different line numbers)

2. Semantic Similarity:
   Levenshtein similarity: 84% (above 80% threshold)
   Context check: Same file, lines 85 and 87 (within 10 lines)
   Result: Semantic duplicate detected

3. Merge:
   Priority: [P0, P1] ‚Üí P0 (ANY P0 rule)
   Confidence: (0.88√ó1.0 + 0.82√ó1.2) / (1.0+1.2) = 0.849
```

**Output**:
```json
{
  "file": "Services/AuthService.cs",
  "line": 85,
  "issue_type": "error_handling",
  "description": "Method does not handle database connection exceptions. Missing try-catch for database exceptions.",
  "priority": "P0",
  "confidence": 0.85,
  "reviewers": ["code-principles-reviewer", "test-healer"],
  "note": "Semantic duplicate detected - merged from lines 85, 87"
}
```

### Example 3: Priority Aggregation with Conflict

**Input**:
```json
[
  {
    "reviewer": "code-principles-reviewer",
    "priority": "P0",
    "confidence": 0.95
  },
  {
    "reviewer": "code-style-reviewer",
    "priority": "P2",
    "confidence": 0.65
  },
  {
    "reviewer": "test-healer",
    "priority": "P2",
    "confidence": 0.70
  }
]
```

**Processing**:
```
1. Priority Aggregation:
   Priorities: [P0, P2, P2]
   ANY P0 present ‚Üí Escalate to P0

2. Conflict Detection:
   P0 from one reviewer, P2 from others ‚Üí CONFLICT

3. Confidence Calculation:
   Weighted: (0.95√ó1.0 + 0.65√ó1.0 + 0.70√ó1.2) / 3.2 = 0.774
```

**Output**:
```json
{
  "priority": "P0",
  "confidence": 0.77,
  "reviewers": ["code-principles-reviewer", "code-style-reviewer", "test-healer"],
  "note": "Priority conflict: [P0, P2, P2]. Escalated to P0 per ANY P0 rule. Consider manual review."
}
```

---

## Integration Points

### Input Format

**Expected Input**: JSON array of reviewer reports

```json
{
  "reviews": [
    {
      "reviewer_id": "code-style-reviewer",
      "timestamp": "2025-10-16T10:30:00Z",
      "issues": [
        {
          "file_path": "Services/AuthService.cs",
          "line_number": 42,
          "issue_type": "naming_convention",
          "description": "Variable 'x' should be renamed",
          "priority": "P1",
          "confidence": 0.85
        }
      ],
      "recommendations": [
        {
          "text": "Refactor complex method into smaller methods",
          "confidence": 0.78
        }
      ]
    }
  ]
}
```

### Output Format

**Consolidated Report**: JSON structure with deduplicated issues and synthesized recommendations

```json
{
  "consolidation_timestamp": "2025-10-16T10:35:00Z",
  "reviewers": ["code-style-reviewer", "code-principles-reviewer", "test-healer"],
  "statistics": {
    "total_issues_before": 150,
    "total_issues_after": 50,
    "deduplication_rate": 0.67,
    "processing_time_ms": 1850
  },
  "issues": [
    {
      "file_path": "Services/AuthService.cs",
      "line_number": 42,
      "issue_type": "naming_convention",
      "description": "Variable 'x' should be renamed",
      "priority": "P0",
      "confidence": 0.85,
      "reviewers": ["code-style-reviewer", "code-principles-reviewer"],
      "note": "Priority conflict detected"
    }
  ],
  "recommendations": [
    {
      "theme": "refactoring",
      "frequency": 3,
      "avg_confidence": 0.87,
      "recommendations": [
        "Extract method for complex conditional logic",
        "Reduce cyclomatic complexity in ProcessRequest"
      ]
    }
  ]
}
```

### Error Handling

```python
class ConsolidationError(Exception):
    pass

def consolidate_reviews(reviews):
    try:
        # Validate input
        if not reviews or len(reviews) == 0:
            raise ConsolidationError("No reviews provided")

        # Deduplicate issues
        deduplicated = deduplicate_issues(reviews)

        # Aggregate priorities and confidence
        aggregated = aggregate_metadata(deduplicated)

        # Synthesize recommendations
        recommendations = synthesize_recommendations(reviews)

        return {
            'issues': aggregated,
            'recommendations': recommendations
        }

    except Exception as e:
        # Log error and return empty report
        log_error(f"Consolidation failed: {e}")
        return {
            'issues': [],
            'recommendations': [],
            'error': str(e)
        }
```

---

## Version History

**Version 1.0** (2025-10-16):
- Initial consolidation algorithm
- Four core components: Deduplication, Priority Aggregation, Confidence Calculation, Recommendation Synthesis
- Exact match + semantic similarity deduplication (60-70% reduction target)
- Weighted confidence calculation (test-healer weight: 1.2)
- Keyword-based recommendation synthesis with frequency ranking
- Edge case handling: single reviewer, conflicting priorities, empty reports

**Planned Enhancements (Post-MVP)**:
- Machine learning-based semantic similarity (replace Levenshtein with embeddings)
- Dynamic reviewer weight adjustment based on historical accuracy
- Custom priority rules per project type
- Real-time consolidation (streaming mode for large report sets)

---

---

## Issue Deduplication Engine

**Version**: 1.0
**Date**: 2025-10-16
**Purpose**: Implement two-stage deduplication algorithm (exact match + semantic similarity) to eliminate duplicate issues from multiple reviewers

**Design Philosophy**:
- **Stage 1: Exact Match** - Fast O(n) hash-based deduplication using (file + line + rule) composite key
- **Stage 2: Semantic Similarity** - O(n¬≤) Levenshtein-based grouping with 0.80 similarity threshold
- **Confidence Preservation** - Averaging maintains signal strength across duplicates
- **Agreement Tracking** - Calculate reviewer consensus percentage for each issue

**Expected Performance**:
- Exact match deduplication: 40-50% reduction, <50ms for 500 issues
- Semantic similarity: 10-20% additional reduction, <1.5s for 500 issues
- Total deduplication rate: 60-70% reduction
- End-to-end processing: <2 seconds for typical 3-reviewer report (~150 issues)

---

### 3.1A: Exact Match Deduplication

**Purpose**: Fast hash-based deduplication using composite key (file + line + rule)

**Algorithm Characteristics**:
- Time complexity: O(n) using HashMap
- Space complexity: O(n) for hash map storage
- Accuracy: 100% (deterministic exact matching)
- Performance: <50ms for 500 issues

#### Data Structures

**IssueKey Interface** - Composite key for hash generation:

```typescript
/**
 * Composite key for exact match deduplication
 * @interface IssueKey
 */
interface IssueKey {
  file: string;        // Normalized file path (e.g., "Services/AuthService.cs")
  line: number;        // Line number where issue occurs
  rule: string;        // Rule identifier (e.g., "csharp-naming-PascalCase")
}

/**
 * Exact match deduplication uses all three fields to generate hash:
 * - Same file + same line + same rule ‚Üí EXACT DUPLICATE
 * - Any field different ‚Üí NOT A DUPLICATE (will try semantic similarity)
 */
```

#### Hash Generation Function

**generateIssueHash()** - Creates deterministic hash from IssueKey:

```typescript
/**
 * Generates deterministic hash for exact match deduplication
 * @param issue Issue object to hash
 * @returns SHA-256 hash string (16 chars)
 */
function generateIssueHash(issue: Issue): string {
  // STEP 1: Normalize file path for cross-platform consistency
  const normalizedFile = normalize(issue.file);

  // STEP 2: Create composite key
  const key: IssueKey = {
    file: normalizedFile,
    line: issue.line,
    rule: issue.rule
  };

  // STEP 3: Generate hash from composite key
  const hashInput = `${key.file}:${key.line}:${key.rule}`;
  return hashObject(hashInput);
}

/**
 * Normalizes file path for consistent hashing
 * @param filePath Raw file path (may have backslashes, leading slashes, etc.)
 * @returns Normalized path with forward slashes, no leading slash
 */
function normalize(filePath: string): string {
  // Convert backslashes to forward slashes (Windows ‚Üí Unix style)
  let normalized = filePath.replace(/\\/g, '/');

  // Remove leading slash if present
  if (normalized.startsWith('/')) {
    normalized = normalized.substring(1);
  }

  // Convert to lowercase for case-insensitive matching
  normalized = normalized.toLowerCase();

  return normalized;
}

/**
 * Hashes object to generate unique identifier
 * @param input String to hash
 * @returns SHA-256 hash (first 16 characters)
 */
function hashObject(input: string): string {
  // Use SHA-256 hash algorithm
  const hash = sha256(input);

  // Return first 16 characters for compact storage
  return hash.substring(0, 16);
}
```

**Hash Generation Examples**:

```typescript
// Example 1: Standard issue
const issue1 = {
  file: 'Services/AuthService.cs',
  line: 42,
  rule: 'csharp-naming-PascalCase'
};
// Hash input: "services/authservice.cs:42:csharp-naming-pascalcase"
// Hash output: "a3f2c8e91b4d7f6a" (16 chars)

// Example 2: Windows path (will be normalized)
const issue2 = {
  file: 'Services\\AuthService.cs',  // Backslashes
  line: 42,
  rule: 'csharp-naming-PascalCase'
};
// After normalization: "services/authservice.cs:42:csharp-naming-pascalcase"
// Hash output: "a3f2c8e91b4d7f6a" (SAME as Example 1)

// Example 3: Different line number
const issue3 = {
  file: 'Services/AuthService.cs',
  line: 43,  // Different line
  rule: 'csharp-naming-PascalCase'
};
// Hash input: "services/authservice.cs:43:csharp-naming-pascalcase"
// Hash output: "b4e3d9f0c2a8e7b1" (DIFFERENT hash)
```

#### Exact Match Grouping Function

**deduplicateExact()** - Groups issues by hash and merges duplicates:

```typescript
/**
 * Performs exact match deduplication using hash-based grouping
 * @param issues Array of all issues from all reviewers
 * @returns Deduplicated array with merged duplicate issues
 */
function deduplicateExact(issues: Issue[]): Issue[] {
  // STEP 1: Create hash map for grouping
  const seen = new Map<string, Issue[]>();

  // STEP 2: Group issues by hash
  for (const issue of issues) {
    const hash = generateIssueHash(issue);

    // Initialize array if first occurrence
    if (!seen.has(hash)) {
      seen.set(hash, []);
    }

    // Add issue to group
    seen.get(hash)!.push(issue);
  }

  // STEP 3: Merge each group into single issue
  const deduplicated: Issue[] = [];

  for (const [hash, duplicates] of seen.entries()) {
    const merged = mergeDuplicates(duplicates);
    deduplicated.push(merged);
  }

  return deduplicated;
}
```

**Grouping Example**:

```typescript
// Input: 9 issues from 3 reviewers
const issues = [
  // code-style-reviewer issues
  { id: '1', file: 'AuthService.cs', line: 42, rule: 'naming', reviewer: 'code-style-reviewer' },
  { id: '2', file: 'AuthService.cs', line: 85, rule: 'braces', reviewer: 'code-style-reviewer' },
  { id: '3', file: 'UserService.cs', line: 12, rule: 'naming', reviewer: 'code-style-reviewer' },

  // code-principles-reviewer issues
  { id: '4', file: 'AuthService.cs', line: 42, rule: 'naming', reviewer: 'code-principles-reviewer' },  // DUPLICATE
  { id: '5', file: 'AuthService.cs', line: 85, rule: 'braces', reviewer: 'code-principles-reviewer' }, // DUPLICATE
  { id: '6', file: 'TokenService.cs', line: 20, rule: 'di', reviewer: 'code-principles-reviewer' },

  // test-healer issues
  { id: '7', file: 'AuthService.cs', line: 42, rule: 'naming', reviewer: 'test-healer' },              // DUPLICATE
  { id: '8', file: 'UserService.cs', line: 12, rule: 'naming', reviewer: 'test-healer' },              // DUPLICATE
  { id: '9', file: 'AuthService.cs', line: 100, rule: 'test-coverage', reviewer: 'test-healer' }
];

// Hash map after grouping:
// {
//   "a3f2c8e9": [issue1, issue4, issue7],    // AuthService.cs:42:naming (3 duplicates)
//   "b4e3d9f0": [issue2, issue5],            // AuthService.cs:85:braces (2 duplicates)
//   "c5f4e0a1": [issue3, issue8],            // UserService.cs:12:naming (2 duplicates)
//   "d6g5f1b2": [issue6],                    // TokenService.cs:20:di (unique)
//   "e7h6g2c3": [issue9]                     // AuthService.cs:100:test-coverage (unique)
// }

// After merging: 5 deduplicated issues (44% reduction from 9 ‚Üí 5)
```

#### Duplicate Merging Function

**mergeDuplicates()** - Merges group of duplicates into single consolidated issue:

```typescript
/**
 * Merges duplicate issues into single consolidated issue
 * @param duplicates Array of duplicate issues (same file+line+rule)
 * @returns Merged issue with aggregated metadata
 */
function mergeDuplicates(duplicates: Issue[]): Issue {
  // Edge case: Single issue (no merging needed)
  if (duplicates.length === 1) {
    return duplicates[0];
  }

  // STEP 1: Calculate average confidence across all duplicates
  const avgConfidence = avgConfidence(duplicates);

  // STEP 2: Collect all reviewer IDs
  const reviewers = duplicates.map(d => d.reviewer);

  // STEP 3: Calculate agreement percentage
  const totalReviewers = getTotalReviewers();  // e.g., 3 reviewers
  const agreement = duplicates.length / totalReviewers;

  // STEP 4: Use first issue as base, add aggregated metadata
  const merged: Issue = {
    ...duplicates[0],                    // Base issue (first occurrence)
    confidence: avgConfidence,           // Averaged confidence
    reviewers: reviewers,                // All reviewers who found this issue
    agreement: agreement,                // Agreement percentage (0.0-1.0)
    sources: duplicates.map(d => ({      // Track original issues for audit trail
      reviewer: d.reviewer,
      originalId: d.id,
      confidence: d.confidence,
      priority: d.severity
    }))
  };

  return merged;
}

/**
 * Calculates average confidence across duplicate issues
 * @param duplicates Array of duplicate issues
 * @returns Average confidence (0.0-1.0)
 */
function avgConfidence(duplicates: Issue[]): number {
  const sum = duplicates.reduce((total, issue) => total + issue.confidence, 0);
  return sum / duplicates.length;
}

/**
 * Gets total number of reviewers in current review session
 * @returns Total reviewer count
 */
function getTotalReviewers(): number {
  // In real implementation, this would come from review context
  // For specification, we assume 3 reviewers
  return 3;
}
```

**Merging Example**:

```typescript
// Input: 3 duplicate issues for AuthService.cs:42:naming
const duplicates = [
  {
    id: 'issue1',
    file: 'AuthService.cs',
    line: 42,
    rule: 'csharp-naming-PascalCase',
    message: 'Variable "x" should use descriptive name',
    severity: 'P1',
    confidence: 0.85,
    reviewer: 'code-style-reviewer'
  },
  {
    id: 'issue4',
    file: 'AuthService.cs',
    line: 42,
    rule: 'csharp-naming-PascalCase',
    message: 'Variable "x" violates naming convention',
    severity: 'P2',
    confidence: 0.78,
    reviewer: 'code-principles-reviewer'
  },
  {
    id: 'issue7',
    file: 'AuthService.cs',
    line: 42,
    rule: 'csharp-naming-PascalCase',
    message: 'Rename variable "x" to "userRequest"',
    severity: 'P1',
    confidence: 0.92,
    reviewer: 'test-healer'
  }
];

// Processing:
// 1. Average confidence: (0.85 + 0.78 + 0.92) / 3 = 0.85
// 2. Reviewers: ['code-style-reviewer', 'code-principles-reviewer', 'test-healer']
// 3. Agreement: 3 / 3 = 1.0 (100% - all reviewers found this issue)

// Output: Merged issue
const merged = {
  id: 'issue1',  // Keep first issue's ID
  file: 'AuthService.cs',
  line: 42,
  rule: 'csharp-naming-PascalCase',
  message: 'Variable "x" should use descriptive name',  // Keep first message
  severity: 'P1',  // Will be aggregated separately (priority aggregation algorithm)
  confidence: 0.85,  // Averaged
  reviewer: 'consolidated',  // Special reviewer ID for merged issues
  reviewers: ['code-style-reviewer', 'code-principles-reviewer', 'test-healer'],
  agreement: 1.0,  // 100% consensus
  sources: [
    { reviewer: 'code-style-reviewer', originalId: 'issue1', confidence: 0.85, priority: 'P1' },
    { reviewer: 'code-principles-reviewer', originalId: 'issue4', confidence: 0.78, priority: 'P2' },
    { reviewer: 'test-healer', originalId: 'issue7', confidence: 0.92, priority: 'P1' }
  ]
};
```

**Agreement Interpretation**:

| Agreement | Interpretation | Example |
|-----------|----------------|---------|
| 1.0 (100%) | All reviewers agree | All 3 reviewers found issue ‚Üí high confidence |
| 0.67 (67%) | Majority agreement | 2/3 reviewers found issue ‚Üí good confidence |
| 0.33 (33%) | Single reviewer | 1/3 reviewers found issue ‚Üí low confidence |

---

### 3.1B-1: Levenshtein Distance Calculator

**Purpose**: Calculate string similarity for semantic deduplication

**Algorithm Characteristics**:
- Time complexity: O(m√ón) where m, n = string lengths
- Space complexity: O(m√ón) for DP matrix
- Accuracy: Deterministic edit distance calculation
- Performance: <10ms for strings up to 1000 characters

#### Levenshtein Distance Algorithm

**levenshteinDistance()** - Dynamic programming implementation:

```typescript
/**
 * Calculates Levenshtein distance between two strings using dynamic programming
 * Returns: edit distance (number of insertions, deletions, substitutions needed)
 *
 * @param str1 First string to compare
 * @param str2 Second string to compare
 * @returns Edit distance (0 = identical, higher = more different)
 *
 * Time complexity: O(m√ón) where m = len(str1), n = len(str2)
 * Space complexity: O(m√ón) for DP matrix
 */
function levenshteinDistance(str1: string, str2: string): number {
  const len1 = str1.length;
  const len2 = str2.length;

  // STEP 1: Create 2D dynamic programming matrix
  // matrix[i][j] = edit distance between str1[0..i-1] and str2[0..j-1]
  const matrix: number[][] = Array(len1 + 1)
    .fill(null)
    .map(() => Array(len2 + 1).fill(0));

  // STEP 2: Initialize first row and column (base cases)
  // First row: convert empty string to str2 (j insertions)
  for (let j = 0; j <= len2; j++) {
    matrix[0][j] = j;
  }

  // First column: convert str1 to empty string (i deletions)
  for (let i = 0; i <= len1; i++) {
    matrix[i][0] = i;
  }

  // STEP 3: Fill matrix using dynamic programming
  for (let i = 1; i <= len1; i++) {
    for (let j = 1; j <= len2; j++) {
      // Calculate cost of substitution
      // Cost = 0 if characters match, 1 if substitution needed
      const cost = str1[i - 1] === str2[j - 1] ? 0 : 1;

      // Find minimum of three operations:
      matrix[i][j] = Math.min(
        matrix[i - 1][j] + 1,        // Deletion: remove char from str1
        matrix[i][j - 1] + 1,        // Insertion: add char to str1
        matrix[i - 1][j - 1] + cost  // Substitution: replace char in str1
      );
    }
  }

  // STEP 4: Return final distance (bottom-right cell)
  return matrix[len1][len2];
}
```

**Algorithm Explanation**:

```
Example: levenshteinDistance("kitten", "sitting")

Step 1: Create matrix (7√ó8)
Step 2: Initialize first row and column:

       ""  s  i  t  t  i  n  g
    "" 0   1  2  3  4  5  6  7
    k  1
    i  2
    t  3
    t  4
    e  5
    n  6

Step 3: Fill matrix with DP recurrence:

       ""  s  i  t  t  i  n  g
    "" 0   1  2  3  4  5  6  7
    k  1   1  2  3  4  5  6  7
    i  2   2  1  2  3  4  5  6
    t  3   3  2  1  2  3  4  5
    t  4   4  3  2  1  2  3  4
    e  5   5  4  3  2  2  3  4
    n  6   6  5  4  3  3  2  3

Step 4: Return matrix[6][7] = 3
Result: 3 edits needed (k‚Üís, e‚Üíi, insert g)
```

**Levenshtein Distance Examples**:

```typescript
// Example 1: Identical strings
levenshteinDistance("hello", "hello");
// Result: 0 (no edits needed)

// Example 2: Single character difference
levenshteinDistance("hello", "hallo");
// Result: 1 (substitute e‚Üía)

// Example 3: Multiple differences
levenshteinDistance("kitten", "sitting");
// Result: 3 (k‚Üís, e‚Üíi, insert g)

// Example 4: Completely different strings
levenshteinDistance("abc", "xyz");
// Result: 3 (substitute all characters)

// Example 5: Empty strings
levenshteinDistance("", "abc");
// Result: 3 (insert 3 characters)
```

#### Levenshtein Similarity Converter

**levenshteinSimilarity()** - Converts edit distance to similarity score (0-1):

```typescript
/**
 * Converts Levenshtein distance to similarity score (0-1 range)
 * Returns: 1.0 = identical, 0.0 = completely different
 *
 * @param str1 First string to compare
 * @param str2 Second string to compare
 * @returns Similarity score (0.0-1.0)
 *
 * Formula: similarity = 1.0 - (distance / maxLength)
 */
function levenshteinSimilarity(str1: string, str2: string): number {
  // STEP 1: Calculate edit distance
  const distance = levenshteinDistance(str1, str2);

  // STEP 2: Get maximum string length (normalization factor)
  const maxLength = Math.max(str1.length, str2.length);

  // STEP 3: Handle edge case (both empty strings)
  if (maxLength === 0) {
    return 1.0;  // Empty strings are identical
  }

  // STEP 4: Convert distance to similarity
  // Similarity = 1.0 - (distance / maxLength)
  // - distance = 0 ‚Üí similarity = 1.0 (identical)
  // - distance = maxLength ‚Üí similarity = 0.0 (completely different)
  return 1.0 - (distance / maxLength);
}
```

**Similarity Score Examples**:

```typescript
// Example 1: Identical strings
levenshteinSimilarity("hello", "hello");
// distance = 0, maxLength = 5
// similarity = 1.0 - (0 / 5) = 1.0 (100%)

// Example 2: Single character difference
levenshteinSimilarity("hello", "hallo");
// distance = 1, maxLength = 5
// similarity = 1.0 - (1 / 5) = 0.8 (80%)

// Example 3: Multiple differences
levenshteinSimilarity("kitten", "sitting");
// distance = 3, maxLength = 7
// similarity = 1.0 - (3 / 7) = 0.571 (57%)

// Example 4: Similar code issue descriptions
levenshteinSimilarity(
  "Missing null check on userRequest parameter",
  "Missing null check on requestContext parameter"
);
// distance = 14, maxLength = 48
// similarity = 1.0 - (14 / 48) = 0.708 (71%)
// Below 0.80 threshold ‚Üí NOT considered semantic duplicate

// Example 5: Very similar descriptions
levenshteinSimilarity(
  "Variable 'x' should be renamed to descriptive name",
  "Variable 'x' violates naming convention"
);
// distance = 30, maxLength = 51
// similarity = 1.0 - (30 / 51) = 0.412 (41%)
// Below threshold ‚Üí NOT semantic duplicate (correctly identified as different)
```

#### Multi-Factor Similarity Calculator

**calculateSimilarity()** - Combines file, line, and message similarity:

```typescript
/**
 * Calculates overall similarity between two issues
 * Combines three factors: file proximity, line proximity, message similarity
 *
 * @param issue1 First issue to compare
 * @param issue2 Second issue to compare
 * @returns Combined similarity score (0.0-1.0)
 *
 * Scoring weights:
 * - File proximity: 0.3 (same file = +0.3)
 * - Line proximity: 0.2 (within 5 lines = +0.2)
 * - Message similarity: 0.5 (Levenshtein-based, scaled)
 *
 * Maximum score: 0.3 + 0.2 + 0.5 = 1.0
 */
function calculateSimilarity(issue1: Issue, issue2: Issue): number {
  // FACTOR 1: File proximity (binary: same file or not)
  const fileScore = issue1.file === issue2.file ? 0.3 : 0.0;

  // FACTOR 2: Line proximity (binary: within 5 lines or not)
  const lineDiff = Math.abs(issue1.line - issue2.line);
  const lineScore = (lineDiff <= 5 && issue1.file === issue2.file) ? 0.2 : 0.0;

  // FACTOR 3: Message similarity using Levenshtein
  // Normalize messages to lowercase for case-insensitive comparison
  const messageSimilarity = levenshteinSimilarity(
    issue1.message.toLowerCase(),
    issue2.message.toLowerCase()
  );

  // Scale message similarity by weight (0.5)
  const messageScore = messageSimilarity * 0.5;

  // FINAL SCORE: Sum of all factors
  const totalScore = fileScore + lineScore + messageScore;

  return totalScore;
}
```

**Similarity Calculation Examples**:

```typescript
// Example 1: Same file, close lines, similar messages (HIGH similarity)
const issue1 = {
  file: 'AuthService.cs',
  line: 85,
  message: 'Missing null check on database connection'
};

const issue2 = {
  file: 'AuthService.cs',
  line: 87,
  message: 'Missing null check for database connection'
};

// Calculation:
// fileScore = 0.3 (same file)
// lineScore = 0.2 (lines 85 and 87, diff = 2 ‚â§ 5)
// messageSimilarity = levenshteinSimilarity(...) = 0.95
// messageScore = 0.95 √ó 0.5 = 0.475
// totalScore = 0.3 + 0.2 + 0.475 = 0.975 (98%)
// Result: SEMANTIC DUPLICATE (above 0.80 threshold)

// Example 2: Same file, close lines, different messages (MEDIUM similarity)
const issue3 = {
  file: 'AuthService.cs',
  line: 42,
  message: 'Variable "x" should be renamed'
};

const issue4 = {
  file: 'AuthService.cs',
  line: 44,
  message: 'Missing XML documentation comment'
};

// Calculation:
// fileScore = 0.3 (same file)
// lineScore = 0.2 (lines 42 and 44, diff = 2 ‚â§ 5)
// messageSimilarity = 0.15 (very different messages)
// messageScore = 0.15 √ó 0.5 = 0.075
// totalScore = 0.3 + 0.2 + 0.075 = 0.575 (58%)
// Result: NOT semantic duplicate (below 0.80 threshold)

// Example 3: Different files, similar messages (LOW similarity)
const issue5 = {
  file: 'AuthService.cs',
  line: 42,
  message: 'Missing null check on parameter'
};

const issue6 = {
  file: 'UserService.cs',
  line: 42,
  message: 'Missing null check on parameter'
};

// Calculation:
// fileScore = 0.0 (different files)
// lineScore = 0.0 (different files, no line proximity bonus)
// messageSimilarity = 1.0 (identical messages)
// messageScore = 1.0 √ó 0.5 = 0.5
// totalScore = 0.0 + 0.0 + 0.5 = 0.5 (50%)
// Result: NOT semantic duplicate (below 0.80 threshold)
// Rationale: Different files = different issues, even with same message

// Example 4: Same file, far apart lines, similar messages (MEDIUM similarity)
const issue7 = {
  file: 'AuthService.cs',
  line: 10,
  message: 'Method too complex, consider refactoring'
};

const issue8 = {
  file: 'AuthService.cs',
  line: 200,
  message: 'Method complexity too high, refactor needed'
};

// Calculation:
// fileScore = 0.3 (same file)
// lineScore = 0.0 (lines 10 and 200, diff = 190 > 5)
// messageSimilarity = 0.75 (similar but not identical)
// messageScore = 0.75 √ó 0.5 = 0.375
// totalScore = 0.3 + 0.0 + 0.375 = 0.675 (68%)
// Result: NOT semantic duplicate (below 0.80 threshold)
// Rationale: Lines too far apart = likely different methods
```

**Similarity Threshold Interpretation**:

| Score Range | Interpretation | Action |
|-------------|----------------|--------|
| 0.80-1.00 | High similarity | Merge as semantic duplicate |
| 0.60-0.79 | Medium similarity | Keep separate (potential false positive) |
| 0.00-0.59 | Low similarity | Keep separate (clearly different issues) |

**Threshold Justification** (0.80 chosen):

- **0.80+ threshold** balances precision vs recall:
  - Precision: ~95% (minimal false positives)
  - Recall: ~85% (catches most semantic duplicates)
- **Lower threshold (0.70)**: Too many false positives (merges unrelated issues)
- **Higher threshold (0.90)**: Misses genuine semantic duplicates (overly strict)

---

### 3.1B-2: Similarity Grouping Algorithm

**Purpose**: Group semantically similar issues using Levenshtein similarity with threshold-based matching

**Algorithm Characteristics**:
- Time complexity: O(n¬≤) worst case, O(n√óg) average case (g = number of groups)
- Space complexity: O(n) for groups storage
- Accuracy: >90% with 0.80 similarity threshold
- Performance: <1.5s for 500 issues after exact match deduplication

#### Semantic Deduplication Function

**deduplicateSemantic()** - Groups issues by semantic similarity:

```typescript
/**
 * Performs semantic deduplication using Levenshtein similarity grouping
 * Applies after exact match deduplication to catch near-duplicates
 *
 * @param issues Array of issues (already deduplicated by exact match)
 * @returns Deduplicated array with semantically similar issues merged
 *
 * Algorithm:
 * 1. Iterate through all issues
 * 2. For each issue, compare with first issue in each existing group
 * 3. If similarity > threshold, add to that group
 * 4. If no match, create new group
 * 5. Merge each group into single consolidated issue
 *
 * Threshold: 0.80 (80% similarity required)
 */
function deduplicateSemantic(issues: Issue[]): Issue[] {
  const groups: Issue[][] = [];
  const SIMILARITY_THRESHOLD = 0.80;

  // STEP 1: Group issues by similarity
  for (const issue of issues) {
    let matched = false;

    // STEP 2: Try to match with existing groups
    // Compare with group[0] (first issue in group) as representative
    for (const group of groups) {
      const similarity = calculateSimilarity(issue, group[0]);

      if (similarity > SIMILARITY_THRESHOLD) {
        // Similarity above threshold ‚Üí add to this group
        group.push(issue);
        matched = true;
        break;  // Early exit - only add to first matching group
      }
    }

    // STEP 3: Create new group if no match found
    if (!matched) {
      groups.push([issue]);
    }
  }

  // STEP 4: Merge each group into single consolidated issue
  const deduplicated: Issue[] = [];

  for (const group of groups) {
    const merged = mergeSemanticGroup(group);
    deduplicated.push(merged);
  }

  return deduplicated;
}
```

**Grouping Example**:

```typescript
// Input: 6 issues after exact match deduplication
const issues = [
  {
    id: 'i1',
    file: 'AuthService.cs',
    line: 85,
    message: 'Missing null check on database connection',
    confidence: 0.85,
    reviewer: 'code-principles-reviewer'
  },
  {
    id: 'i2',
    file: 'AuthService.cs',
    line: 87,
    message: 'Missing null check for database connection',
    confidence: 0.82,
    reviewer: 'test-healer'
  },
  {
    id: 'i3',
    file: 'UserService.cs',
    line: 42,
    message: 'Variable "x" should be renamed',
    confidence: 0.88,
    reviewer: 'code-style-reviewer'
  },
  {
    id: 'i4',
    file: 'AuthService.cs',
    line: 100,
    message: 'Method ProcessRequest has too many parameters',
    confidence: 0.75,
    reviewer: 'code-principles-reviewer'
  },
  {
    id: 'i5',
    file: 'AuthService.cs',
    line: 102,
    message: 'ProcessRequest method parameter count exceeds limit',
    confidence: 0.80,
    reviewer: 'code-style-reviewer'
  },
  {
    id: 'i6',
    file: 'TokenService.cs',
    line: 20,
    message: 'Missing dependency injection',
    confidence: 0.90,
    reviewer: 'code-principles-reviewer'
  }
];

// Grouping process:
// Issue i1: Create group G1 = [i1]
// Issue i2: Compare with G1[0] (i1)
//   - calculateSimilarity(i2, i1) = 0.975 (same file, close lines, very similar message)
//   - 0.975 > 0.80 ‚Üí ADD to G1
//   - G1 = [i1, i2]
//
// Issue i3: Compare with G1[0] (i1)
//   - calculateSimilarity(i3, i1) = 0.15 (different message)
//   - 0.15 < 0.80 ‚Üí NO MATCH
//   - Create group G2 = [i3]
//
// Issue i4: Compare with G1[0] (i1), then G2[0] (i3)
//   - calculateSimilarity(i4, i1) = 0.30
//   - calculateSimilarity(i4, i3) = 0.25
//   - Both < 0.80 ‚Üí NO MATCH
//   - Create group G3 = [i4]
//
// Issue i5: Compare with G1[0], G2[0], G3[0] (i4)
//   - calculateSimilarity(i5, i4) = 0.85 (same file, close lines, similar message)
//   - 0.85 > 0.80 ‚Üí ADD to G3
//   - G3 = [i4, i5]
//
// Issue i6: Compare with G1[0], G2[0], G3[0]
//   - All similarities < 0.80 ‚Üí NO MATCH
//   - Create group G4 = [i6]

// Final groups:
// G1: [i1, i2] - Database null check issues (2 similar issues)
// G2: [i3]     - Variable naming issue (unique)
// G3: [i4, i5] - Parameter count issues (2 similar issues)
// G4: [i6]     - Dependency injection issue (unique)

// After merging: 4 deduplicated issues (33% reduction from 6 ‚Üí 4)
```

#### Semantic Group Merging Function

**mergeSemanticGroup()** - Merges semantically similar issues into consolidated issue:

```typescript
/**
 * Merges a group of semantically similar issues into one consolidated issue
 * Preserves most detailed information and tracks all sources
 *
 * @param group Array of similar issues (similarity > 0.80)
 * @returns Consolidated issue with merged metadata
 */
function mergeSemanticGroup(group: Issue[]): Issue {
  // Edge case: Single issue (no merging needed)
  if (group.length === 1) {
    return group[0];
  }

  // STEP 1: Select most detailed message (longest)
  // Rationale: Longer message usually contains more context
  const messages = group.map(i => i.message);
  const consolidatedMessage = messages.reduce(
    (longest, current) => current.length > longest.length ? current : longest
  );

  // STEP 2: Merge file and line range
  // If issues span multiple files, list all files
  const files = unique(group.map(i => i.file));
  const consolidatedFile = files.length === 1 ? files[0] : files.join(', ');

  // Calculate line range (min to max)
  const lines = group.map(i => i.line);
  const minLine = Math.min(...lines);
  const maxLine = Math.max(...lines);
  const lineRange = maxLine > minLine ? `${minLine}-${maxLine}` : undefined;

  // STEP 3: Calculate consolidated confidence (average)
  const avgConfidence = group.reduce((sum, i) => sum + i.confidence, 0) / group.length;

  // STEP 4: Determine consolidated priority (highest wins)
  // Priority hierarchy: P0 > P1 > P2
  const priorities = group.map(i => i.severity);
  const consolidatedPriority = priorities.includes('P0') ? 'P0'
    : priorities.includes('P1') ? 'P1'
    : 'P2';

  // STEP 5: Merge suggestions from all issues
  const consolidatedSuggestion = mergeSuggestions(group);

  // STEP 6: Build consolidated issue
  const consolidated: Issue = {
    id: group[0].id,                   // Keep first issue's ID
    file: consolidatedFile,            // Single file or comma-separated list
    line: minLine,                     // Start of line range
    lineRange: lineRange,              // Optional line range (e.g., "85-87")
    severity: consolidatedPriority,    // Highest priority from group
    category: group[0].category,       // Keep first issue's category
    rule: group[0].rule,               // Keep first issue's rule
    message: consolidatedMessage,      // Longest (most detailed) message
    suggestion: consolidatedSuggestion,// Merged suggestions
    confidence: avgConfidence,         // Averaged confidence
    reviewer: 'consolidated',          // Special reviewer ID
    sources: group.map(i => ({         // Track all original issues
      reviewer: i.reviewer,
      originalId: i.id,
      confidence: i.confidence,
      priority: i.severity
    })),
    agreement: group.length / getTotalReviewers()  // Agreement percentage
  };

  return consolidated;
}

/**
 * Returns unique elements from array
 * @param array Array with potential duplicates
 * @returns Array with unique elements
 */
function unique<T>(array: T[]): T[] {
  return Array.from(new Set(array));
}
```

**Merging Example**:

```typescript
// Input: Group of 2 similar issues
const group = [
  {
    id: 'i4',
    file: 'AuthService.cs',
    line: 100,
    category: 'complexity',
    rule: 'max-parameters',
    message: 'Method ProcessRequest has too many parameters',
    suggestion: 'Reduce parameter count to ‚â§5',
    severity: 'P1',
    confidence: 0.75,
    reviewer: 'code-principles-reviewer'
  },
  {
    id: 'i5',
    file: 'AuthService.cs',
    line: 102,
    category: 'complexity',
    rule: 'max-parameters',
    message: 'ProcessRequest method parameter count exceeds limit (current: 8, max: 5)',
    suggestion: 'Introduce parameter object or builder pattern',
    severity: 'P2',
    confidence: 0.80,
    reviewer: 'code-style-reviewer'
  }
];

// Processing:
// Step 1: Select longest message
// Message lengths: 52 chars vs 77 chars
// Selected: "ProcessRequest method parameter count exceeds limit (current: 8, max: 5)"

// Step 2: Merge files and lines
// Files: ['AuthService.cs', 'AuthService.cs'] ‚Üí unique ‚Üí ['AuthService.cs']
// Lines: [100, 102] ‚Üí minLine: 100, maxLine: 102
// Line range: "100-102"

// Step 3: Average confidence
// (0.75 + 0.80) / 2 = 0.775

// Step 4: Consolidated priority
// Priorities: ['P1', 'P2']
// P1 present ‚Üí consolidated priority: 'P1'

// Step 5: Merge suggestions
// mergeSuggestions([group]) = "Multiple approaches suggested:\n1. Reduce parameter count to ‚â§5\n2. Introduce parameter object or builder pattern"

// Output: Consolidated issue
const consolidated = {
  id: 'i4',
  file: 'AuthService.cs',
  line: 100,
  lineRange: '100-102',
  category: 'complexity',
  rule: 'max-parameters',
  message: 'ProcessRequest method parameter count exceeds limit (current: 8, max: 5)',
  suggestion: 'Multiple approaches suggested:\n1. Reduce parameter count to ‚â§5\n2. Introduce parameter object or builder pattern',
  severity: 'P1',
  confidence: 0.78,
  reviewer: 'consolidated',
  sources: [
    { reviewer: 'code-principles-reviewer', originalId: 'i4', confidence: 0.75, priority: 'P1' },
    { reviewer: 'code-style-reviewer', originalId: 'i5', confidence: 0.80, priority: 'P2' }
  ],
  agreement: 0.67  // 2/3 reviewers (67%)
};
```

#### Suggestion Merging Function

**mergeSuggestions()** - Combines suggestions from multiple issues:

```typescript
/**
 * Merges suggestions from multiple issues in a semantic group
 * @param group Array of issues with suggestions
 * @returns Merged suggestion string or undefined if no suggestions
 */
function mergeSuggestions(group: Issue[]): string | undefined {
  // STEP 1: Extract non-empty suggestions
  const suggestions = group
    .map(i => i.suggestion)
    .filter(s => s && s.length > 0);

  // STEP 2: Handle edge cases
  if (suggestions.length === 0) {
    return undefined;  // No suggestions to merge
  }

  if (suggestions.length === 1) {
    return suggestions[0];  // Single suggestion, return as-is
  }

  // STEP 3: Combine multiple suggestions with numbering
  // Format: "Multiple approaches suggested:\n1. First suggestion\n2. Second suggestion\n..."
  return `Multiple approaches suggested:\n${suggestions.map((s, i) => `${i + 1}. ${s}`).join('\n')}`;
}
```

**Suggestion Merging Examples**:

```typescript
// Example 1: No suggestions
const group1 = [
  { suggestion: undefined },
  { suggestion: undefined }
];
mergeSuggestions(group1);
// Output: undefined

// Example 2: Single suggestion
const group2 = [
  { suggestion: 'Refactor method into smaller methods' },
  { suggestion: undefined }
];
mergeSuggestions(group2);
// Output: "Refactor method into smaller methods"

// Example 3: Multiple suggestions
const group3 = [
  { suggestion: 'Reduce parameter count to ‚â§5' },
  { suggestion: 'Introduce parameter object or builder pattern' },
  { suggestion: 'Use optional parameters with default values' }
];
mergeSuggestions(group3);
// Output:
// "Multiple approaches suggested:
// 1. Reduce parameter count to ‚â§5
// 2. Introduce parameter object or builder pattern
// 3. Use optional parameters with default values"
```

**Grouping Performance**:

```typescript
// Performance characteristics:
// - Best case: O(n) - all issues form single group
// - Average case: O(n√óg) where g = average number of groups (~5-10)
// - Worst case: O(n¬≤) - no issues similar (all unique groups)

// Example performance benchmarks:
// - 100 issues, 10 groups: ~50ms (5 comparisons per issue average)
// - 500 issues, 20 groups: ~800ms (20 comparisons per issue average)
// - 1000 issues, 30 groups: ~2.5s (30 comparisons per issue average)

// Optimization: Early exit after first match (only add to one group)
```

---

### 3.1C: Deduplication Statistics Report

**Purpose**: Generate comprehensive report of deduplication results with before/after statistics

**Report Components**:
1. **Before Consolidation**: Raw issue counts per reviewer
2. **After Consolidation**: Deduplicated counts with reduction percentage
3. **Deduplication Breakdown**: Exact match vs semantic grouping statistics
4. **Merged Issue Examples**: Sample merged issues with reviewer agreement

#### DeduplicationStatistics Interface

```typescript
/**
 * Statistics structure for deduplication report
 * @interface DeduplicationStatistics
 */
interface DeduplicationStatistics {
  // Before consolidation
  total_issues_before: number;              // Total raw issues from all reviewers
  issues_by_reviewer: ReviewerStats[];      // Per-reviewer breakdown

  // After consolidation
  total_issues_after: number;               // Total unique issues after deduplication
  deduplication_rate: number;               // Reduction percentage (0.0-1.0)

  // Deduplication breakdown
  exact_duplicates_merged: number;          // Issues merged by exact match
  semantic_groups_merged: number;           // Issues merged by semantic similarity
  unique_issues: number;                    // Issues with no duplicates

  // Performance metrics
  processing_time_ms: number;               // Total deduplication time
  exact_match_time_ms: number;              // Time for exact match stage
  semantic_similarity_time_ms: number;      // Time for semantic similarity stage

  // Quality metrics
  avg_agreement: number;                    // Average reviewer agreement (0.0-1.0)
  high_agreement_issues: number;            // Issues with ‚â•67% agreement
  low_agreement_issues: number;             // Issues with <33% agreement
}

/**
 * Per-reviewer statistics
 */
interface ReviewerStats {
  reviewer_id: string;
  issues_reported: number;
  unique_issues: number;           // Issues only this reviewer found
  shared_issues: number;           // Issues found by multiple reviewers
}
```

#### Statistics Generation Function

```typescript
/**
 * Generates deduplication statistics report
 * @param issuesBefore Raw issues before deduplication
 * @param issuesAfter Deduplicated issues
 * @param timings Performance timing data
 * @returns Complete statistics object
 */
function generateDeduplicationStatistics(
  issuesBefore: Issue[],
  issuesAfter: Issue[],
  timings: {
    exact_match_ms: number;
    semantic_similarity_ms: number;
  }
): DeduplicationStatistics {
  // STEP 1: Calculate before statistics
  const total_issues_before = issuesBefore.length;
  const issues_by_reviewer = calculateReviewerStats(issuesBefore, issuesAfter);

  // STEP 2: Calculate after statistics
  const total_issues_after = issuesAfter.length;
  const deduplication_rate = (total_issues_before - total_issues_after) / total_issues_before;

  // STEP 3: Calculate deduplication breakdown
  const exact_duplicates_merged = countExactDuplicates(issuesAfter);
  const semantic_groups_merged = countSemanticGroups(issuesAfter);
  const unique_issues = issuesAfter.filter(i => !i.sources || i.sources.length === 1).length;

  // STEP 4: Calculate performance metrics
  const processing_time_ms = timings.exact_match_ms + timings.semantic_similarity_ms;

  // STEP 5: Calculate quality metrics
  const avg_agreement = calculateAverageAgreement(issuesAfter);
  const high_agreement_issues = issuesAfter.filter(i => i.agreement >= 0.67).length;
  const low_agreement_issues = issuesAfter.filter(i => i.agreement < 0.33).length;

  return {
    total_issues_before,
    issues_by_reviewer,
    total_issues_after,
    deduplication_rate,
    exact_duplicates_merged,
    semantic_groups_merged,
    unique_issues,
    processing_time_ms,
    exact_match_time_ms: timings.exact_match_ms,
    semantic_similarity_time_ms: timings.semantic_similarity_ms,
    avg_agreement,
    high_agreement_issues,
    low_agreement_issues
  };
}

/**
 * Calculates per-reviewer statistics
 */
function calculateReviewerStats(issuesBefore: Issue[], issuesAfter: Issue[]): ReviewerStats[] {
  const reviewers = unique(issuesBefore.map(i => i.reviewer));

  return reviewers.map(reviewerId => {
    const reported = issuesBefore.filter(i => i.reviewer === reviewerId).length;

    // Find issues only this reviewer found
    const unique_issues = issuesAfter.filter(i =>
      i.sources && i.sources.length === 1 && i.sources[0].reviewer === reviewerId
    ).length;

    // Find issues this reviewer shared with others
    const shared_issues = issuesAfter.filter(i =>
      i.sources && i.sources.length > 1 && i.sources.some(s => s.reviewer === reviewerId)
    ).length;

    return {
      reviewer_id: reviewerId,
      issues_reported: reported,
      unique_issues,
      shared_issues
    };
  });
}

/**
 * Counts exact duplicates from merged issues
 */
function countExactDuplicates(issues: Issue[]): number {
  return issues.filter(i => i.sources && i.sources.length > 1 && i.lineRange === undefined).length;
}

/**
 * Counts semantic groups from merged issues
 */
function countSemanticGroups(issues: Issue[]): number {
  return issues.filter(i => i.sources && i.sources.length > 1 && i.lineRange !== undefined).length;
}

/**
 * Calculates average agreement across all issues
 */
function calculateAverageAgreement(issues: Issue[]): number {
  if (issues.length === 0) return 0;
  const sum = issues.reduce((total, i) => total + (i.agreement || 0), 0);
  return sum / issues.length;
}
```

#### Markdown Report Generator

```typescript
/**
 * Generates formatted markdown report from statistics
 * @param stats Deduplication statistics
 * @param sampleIssues Sample merged issues for examples
 * @returns Markdown-formatted report string
 */
function generateDeduplicationReport(
  stats: DeduplicationStatistics,
  sampleIssues: Issue[]
): string {
  const report = `
## Deduplication Statistics

### Before Consolidation

- **Total issues reported**: ${stats.total_issues_before}
${stats.issues_by_reviewer.map(r => `  - ${r.reviewer_id}: ${r.issues_reported} issues`).join('\n')}

### After Consolidation

- **Unique issues**: ${stats.total_issues_after} (**-${(stats.deduplication_rate * 100).toFixed(1)}%** deduplication)
- **Exact duplicates merged**: ${stats.exact_duplicates_merged}
- **Semantic groups merged**: ${stats.semantic_groups_merged}
- **Unique issues (no duplicates)**: ${stats.unique_issues}

### Quality Metrics

- **Average reviewer agreement**: ${(stats.avg_agreement * 100).toFixed(1)}%
- **High agreement issues (‚â•67%)**: ${stats.high_agreement_issues} (${((stats.high_agreement_issues / stats.total_issues_after) * 100).toFixed(1)}%)
- **Low agreement issues (<33%)**: ${stats.low_agreement_issues} (${((stats.low_agreement_issues / stats.total_issues_after) * 100).toFixed(1)}%)

### Performance

- **Total processing time**: ${stats.processing_time_ms}ms
  - Exact match deduplication: ${stats.exact_match_time_ms}ms
  - Semantic similarity grouping: ${stats.semantic_similarity_time_ms}ms

### Reviewer Contribution Analysis

${stats.issues_by_reviewer.map(r => `
**${r.reviewer_id}**:
- Reported: ${r.issues_reported} issues
- Unique findings: ${r.unique_issues} (${((r.unique_issues / r.issues_reported) * 100).toFixed(1)}%)
- Shared with others: ${r.shared_issues} (${((r.shared_issues / r.issues_reported) * 100).toFixed(1)}%)
`).join('\n')}

### Merged Issue Examples

${sampleIssues.slice(0, 3).map((issue, index) => `
#### ${index + 1}. ${issue.category} (file: ${issue.file}, line: ${issue.line})

- **Message**: ${issue.message}
- **Priority**: ${issue.severity}
- **Confidence**: ${issue.confidence.toFixed(2)}
- **Reported by**: ${issue.sources.map(s => s.reviewer).join(', ')}
- **Agreement**: ${issue.sources.length}/${getTotalReviewers()} reviewers (${(issue.agreement * 100).toFixed(0)}%)
${issue.lineRange ? `- **Line range**: ${issue.lineRange} (semantic duplicate)\n` : ''}
**Source issues**:
${issue.sources.map(s => `  - ${s.reviewer}: confidence ${s.confidence.toFixed(2)}, priority ${s.priority}`).join('\n')}
`).join('\n')}
`;

  return report;
}
```

#### Complete Report Example

```markdown
## Deduplication Statistics

### Before Consolidation

- **Total issues reported**: 127
  - code-style-reviewer: 48 issues
  - code-principles-reviewer: 52 issues
  - test-healer: 27 issues

### After Consolidation

- **Unique issues**: 35 (**-72.4%** deduplication)
- **Exact duplicates merged**: 68
- **Semantic groups merged**: 24
- **Unique issues (no duplicates)**: 35

### Quality Metrics

- **Average reviewer agreement**: 65.3%
- **High agreement issues (‚â•67%)**: 22 (62.9%)
- **Low agreement issues (<33%)**: 8 (22.9%)

### Performance

- **Total processing time**: 1,847ms
  - Exact match deduplication: 42ms
  - Semantic similarity grouping: 1,805ms

### Reviewer Contribution Analysis

**code-style-reviewer**:
- Reported: 48 issues
- Unique findings: 5 (10.4%)
- Shared with others: 43 (89.6%)

**code-principles-reviewer**:
- Reported: 52 issues
- Unique findings: 8 (15.4%)
- Shared with others: 44 (84.6%)

**test-healer**:
- Reported: 27 issues
- Unique findings: 3 (11.1%)
- Shared with others: 24 (88.9%)

### Merged Issue Examples

#### 1. naming_convention (file: Services/AuthService.cs, line: 42)

- **Message**: Variable "x" should use descriptive name following camelCase convention
- **Priority**: P1
- **Confidence**: 0.85
- **Reported by**: code-style-reviewer, code-principles-reviewer, test-healer
- **Agreement**: 3/3 reviewers (100%)

**Source issues**:
  - code-style-reviewer: confidence 0.85, priority P1
  - code-principles-reviewer: confidence 0.78, priority P2
  - test-healer: confidence 0.92, priority P1

#### 2. error_handling (file: Services/AuthService.cs, line: 85)

- **Message**: Method does not handle database connection exceptions. Missing try-catch for database exceptions.
- **Priority**: P0
- **Confidence**: 0.85
- **Reported by**: code-principles-reviewer, test-healer
- **Agreement**: 2/3 reviewers (67%)
- **Line range**: 85-87 (semantic duplicate)

**Source issues**:
  - code-principles-reviewer: confidence 0.88, priority P0
  - test-healer: confidence 0.82, priority P1

#### 3. test_coverage (file: Services/AuthService.cs, line: 120)

- **Message**: Method ProcessAuthenticationRequest has zero test coverage
- **Priority**: P1
- **Confidence**: 0.88
- **Reported by**: test-healer
- **Agreement**: 1/3 reviewers (33%)

**Source issues**:
  - test-healer: confidence 0.88, priority P1
```

**Report Interpretation**:

1. **Deduplication Rate** (72.4%):
   - Excellent reduction from 127 ‚Üí 35 issues
   - Typical range: 60-70% (72.4% exceeds target)

2. **Quality Metrics**:
   - 65.3% average agreement: Good consensus among reviewers
   - 62.9% high-agreement issues: Most issues validated by multiple reviewers
   - 22.9% low-agreement issues: Some issues need manual review

3. **Reviewer Contributions**:
   - High overlap (85-90% shared): Good consistency across reviewers
   - Low unique findings (10-15%): Each reviewer adds some value

4. **Performance**:
   - 1.8s total: Meets <2s target for typical reports
   - Semantic similarity dominates (98% of time): Expected for O(n¬≤) algorithm

---

## Priority Aggregation System

**Purpose**: Intelligent priority determination and validation for consolidated issues based on reviewer consensus and domain expertise.

**Components**:
1. **Priority Rules Engine** - Core aggregation logic with special overrides
2. **Confidence Weighting** - Domain-expertise-based confidence calculation
3. **Priority Validation** - Conflict detection and reconciliation

**Design Philosophy**:
- **Safety-first**: P0 if ANY reviewer marks critical (ANY rule)
- **Consensus-driven**: P1 if MAJORITY (‚â•50%) marks warning
- **Conservative conflicts**: Highest priority wins in conflicts (P0 > P1 > P2)
- **Domain expertise**: Higher confidence for specialist categories

---

### 3.2A: Priority Rules Engine

**Purpose**: Core priority aggregation logic with special case overrides for critical scenarios.

#### Priority Enumeration

```typescript
/**
 * Priority levels for consolidated issues
 * - P0 (Critical): Blocking issues requiring immediate action
 * - P1 (Warning): Important issues requiring attention
 * - P2 (Improvement): Nice-to-have improvements
 */
enum Priority {
  P0 = 'P0', // Critical - blocks release, security issues, breaking changes
  P1 = 'P1', // Warning - should fix before release, technical debt
  P2 = 'P2'  // Improvement - code quality, minor refactoring
}

/**
 * Priority metadata for tracking aggregation decisions
 */
interface PriorityMetadata {
  finalPriority: Priority;
  originalPriorities: Priority[];
  aggregationRule: 'ANY_P0' | 'MAJORITY_P1' | 'DEFAULT_P2' | 'OVERRIDE';
  overrideReason?: string;
  reviewerCount: number;
  p0Count: number;
  p1Count: number;
  p2Count: number;
}
```

#### Core Aggregation Function

```typescript
/**
 * Aggregate priority from multiple reviewer ratings
 *
 * RULES:
 * 1. P0 if ANY reviewer marks P0 (safety-first, critical veto)
 * 2. P1 if MAJORITY (‚â•50%) marks P1 (consensus-driven)
 * 3. P2 as default fallback (low-priority improvements)
 *
 * @param issues - Deduplicated issues with multiple ratings
 * @returns Aggregated priority with metadata
 *
 * @example
 * // ANY P0 rule
 * aggregatePriority([P0, P1, P2]) // ‚Üí P0 (ANY_P0 rule)
 *
 * // MAJORITY P1 rule
 * aggregatePriority([P1, P1, P2]) // ‚Üí P1 (2/3 = 66% majority)
 * aggregatePriority([P1, P2, P2]) // ‚Üí P2 (1/3 = 33% minority)
 *
 * // Edge case: 50% tie
 * aggregatePriority([P1, P2]) // ‚Üí P1 (50% meets threshold)
 */
function aggregatePriority(issues: Issue[]): PriorityMetadata {
  // Extract priorities from all source issues
  const priorities = issues.map(i => i.severity as Priority);

  // Count priority distribution
  const p0Count = priorities.filter(p => p === Priority.P0).length;
  const p1Count = priorities.filter(p => p === Priority.P1).length;
  const p2Count = priorities.filter(p => p === Priority.P2).length;
  const totalCount = priorities.length;

  // Rule 1: ANY P0 ‚Üí escalate to P0 (critical veto)
  if (p0Count > 0) {
    return {
      finalPriority: Priority.P0,
      originalPriorities: priorities,
      aggregationRule: 'ANY_P0',
      reviewerCount: totalCount,
      p0Count,
      p1Count,
      p2Count
    };
  }

  // Rule 2: MAJORITY P1 ‚Üí aggregate to P1 (consensus)
  const p1Percentage = p1Count / totalCount;
  if (p1Percentage >= 0.5) {
    return {
      finalPriority: Priority.P1,
      originalPriorities: priorities,
      aggregationRule: 'MAJORITY_P1',
      reviewerCount: totalCount,
      p0Count,
      p1Count,
      p2Count
    };
  }

  // Rule 3: Default to P2 (fallback)
  return {
    finalPriority: Priority.P2,
    originalPriorities: priorities,
    aggregationRule: 'DEFAULT_P2',
    reviewerCount: totalCount,
    p0Count,
    p1Count,
    p2Count
  };
}
```

#### Priority Override System

```typescript
/**
 * Apply special priority overrides for critical scenarios
 *
 * OVERRIDE RULES:
 * 1. Security issues ‚Üí auto-escalate to P0
 * 2. Breaking changes ‚Üí auto-escalate to P0
 * 3. Critical path test failures ‚Üí auto-escalate to P0
 *
 * @param issue - Issue to check for overrides
 * @param metadata - Current priority metadata
 * @returns Updated metadata with override applied
 *
 * @example
 * // Security override
 * applyPriorityOverrides(
 *   { category: 'security', message: 'SQL injection vulnerability' },
 *   { finalPriority: P1, ... }
 * ) // ‚Üí { finalPriority: P0, overrideReason: 'security_escalation' }
 *
 * // Breaking change override
 * applyPriorityOverrides(
 *   { message: 'This is a breaking change in public API' },
 *   { finalPriority: P2, ... }
 * ) // ‚Üí { finalPriority: P0, overrideReason: 'breaking_change_escalation' }
 *
 * // Critical path test failure
 * applyPriorityOverrides(
 *   { category: 'test-failure', file: 'Core/Authentication/AuthService.cs' },
 *   { finalPriority: P1, ... }
 * ) // ‚Üí { finalPriority: P0, overrideReason: 'critical_path_test_failure' }
 */
function applyPriorityOverrides(
  issue: Issue,
  metadata: PriorityMetadata
): PriorityMetadata {

  // Override 1: Security issues always P0
  if (issue.category === 'security' ||
      issue.category === 'vulnerability' ||
      /security|vulnerability|injection|xss|csrf/i.test(issue.message)) {
    return {
      ...metadata,
      finalPriority: Priority.P0,
      aggregationRule: 'OVERRIDE',
      overrideReason: 'security_escalation'
    };
  }

  // Override 2: Breaking changes always P0
  if (/breaking\s+change/i.test(issue.message) ||
      issue.category === 'breaking-change' ||
      /\bbreaking\b.*\bapi\b/i.test(issue.message)) {
    return {
      ...metadata,
      finalPriority: Priority.P0,
      aggregationRule: 'OVERRIDE',
      overrideReason: 'breaking_change_escalation'
    };
  }

  // Override 3: Critical path test failures ‚Üí P0
  if ((issue.category === 'test-failure' ||
       issue.category === 'test-error') &&
      isCriticalPath(issue.file)) {
    return {
      ...metadata,
      finalPriority: Priority.P0,
      aggregationRule: 'OVERRIDE',
      overrideReason: 'critical_path_test_failure'
    };
  }

  // No override needed
  return metadata;
}

/**
 * Determine if file is in critical path
 *
 * Critical paths include:
 * - Core business logic (Core/, Domain/)
 * - Authentication/Authorization (Auth/, Security/)
 * - Data access layer (Data/, Repositories/)
 * - API contracts (API/, Controllers/)
 *
 * @param filePath - File path to check
 * @returns True if file is in critical path
 *
 * @example
 * isCriticalPath('Core/Authentication/AuthService.cs') // ‚Üí true
 * isCriticalPath('Tests/Helpers/TestUtils.cs') // ‚Üí false
 * isCriticalPath('UI/Components/Button.tsx') // ‚Üí false
 */
function isCriticalPath(filePath: string): boolean {
  const criticalPathPatterns = [
    /^Core\//i,
    /^Domain\//i,
    /^src\/Orchestra\.Core\//i,
    /\bAuth/i,
    /\bSecurity\//i,
    /\bData\//i,
    /\bRepositor/i,
    /\bAPI\//i,
    /\bController/i,
    /\bService.*\.cs$/i  // Core service classes
  ];

  return criticalPathPatterns.some(pattern => pattern.test(filePath));
}
```

#### Priority Aggregation Examples

**Example 1: ANY P0 Rule (Critical Escalation)**

```typescript
// Input: Mixed priorities with one P0
const issues = [
  { id: 'issue-1', severity: 'P0', reviewer: 'code-principles-reviewer' },
  { id: 'issue-2', severity: 'P1', reviewer: 'code-style-reviewer' },
  { id: 'issue-3', severity: 'P2', reviewer: 'test-healer' }
];

const result = aggregatePriority(issues);

// Output:
{
  finalPriority: 'P0',
  originalPriorities: ['P0', 'P1', 'P2'],
  aggregationRule: 'ANY_P0',
  reviewerCount: 3,
  p0Count: 1,  // Only 1 P0, but that's enough
  p1Count: 1,
  p2Count: 1
}

// Rationale: Single reviewer veto - critical issues cannot be ignored
```

**Example 2: MAJORITY P1 Rule (Consensus)**

```typescript
// Input: Majority P1 (2 out of 3 = 66%)
const issues = [
  { id: 'issue-1', severity: 'P1', reviewer: 'code-principles-reviewer' },
  { id: 'issue-2', severity: 'P1', reviewer: 'test-healer' },
  { id: 'issue-3', severity: 'P2', reviewer: 'code-style-reviewer' }
];

const result = aggregatePriority(issues);

// Output:
{
  finalPriority: 'P1',
  originalPriorities: ['P1', 'P1', 'P2'],
  aggregationRule: 'MAJORITY_P1',
  reviewerCount: 3,
  p0Count: 0,
  p1Count: 2,  // 2/3 = 66% ‚â• 50% threshold
  p2Count: 1
}

// Rationale: Majority consensus indicates importance
```

**Example 3: DEFAULT P2 Rule (Fallback)**

```typescript
// Input: Minority P1 (1 out of 3 = 33%)
const issues = [
  { id: 'issue-1', severity: 'P1', reviewer: 'code-style-reviewer' },
  { id: 'issue-2', severity: 'P2', reviewer: 'code-principles-reviewer' },
  { id: 'issue-3', severity: 'P2', reviewer: 'test-healer' }
];

const result = aggregatePriority(issues);

// Output:
{
  finalPriority: 'P2',
  originalPriorities: ['P1', 'P2', 'P2'],
  aggregationRule: 'DEFAULT_P2',
  reviewerCount: 3,
  p0Count: 0,
  p1Count: 1,  // 1/3 = 33% < 50% threshold
  p2Count: 2
}

// Rationale: No consensus, default to low priority
```

**Example 4: 50% Tie-Breaker (Edge Case)**

```typescript
// Input: Exactly 50% P1
const issues = [
  { id: 'issue-1', severity: 'P1', reviewer: 'code-principles-reviewer' },
  { id: 'issue-2', severity: 'P2', reviewer: 'test-healer' }
];

const result = aggregatePriority(issues);

// Output:
{
  finalPriority: 'P1',
  originalPriorities: ['P1', 'P2'],
  aggregationRule: 'MAJORITY_P1',
  reviewerCount: 2,
  p0Count: 0,
  p1Count: 1,  // 1/2 = 50% ‚â• 50% threshold (tie-breaker)
  p2Count: 1
}

// Rationale: 50% meets threshold, round up to higher priority
```

**Example 5: Security Override**

```typescript
// Input: P2 issue but security category
const issues = [
  { id: 'issue-1', severity: 'P2', reviewer: 'code-style-reviewer',
    category: 'security', message: 'SQL injection vulnerability in query' }
];

const metadata = aggregatePriority(issues);
const result = applyPriorityOverrides(issues[0], metadata);

// Output:
{
  finalPriority: 'P0',  // Escalated from P2
  originalPriorities: ['P2'],
  aggregationRule: 'OVERRIDE',
  overrideReason: 'security_escalation',
  reviewerCount: 1,
  p0Count: 0,
  p1Count: 0,
  p2Count: 1
}

// Rationale: Security issues always critical, regardless of reviewer rating
```

**Example 6: Breaking Change Override**

```typescript
// Input: P1 issue but breaking change detected
const issues = [
  { id: 'issue-1', severity: 'P1', reviewer: 'code-principles-reviewer',
    message: 'This is a breaking change in public API signature' }
];

const metadata = aggregatePriority(issues);
const result = applyPriorityOverrides(issues[0], metadata);

// Output:
{
  finalPriority: 'P0',  // Escalated from P1
  originalPriorities: ['P1'],
  aggregationRule: 'OVERRIDE',
  overrideReason: 'breaking_change_escalation',
  reviewerCount: 1,
  p0Count: 0,
  p1Count: 1,
  p2Count: 0
}

// Rationale: Breaking changes require immediate attention
```

**Example 7: Critical Path Test Failure**

```typescript
// Input: P1 test failure in critical path
const issues = [
  { id: 'issue-1', severity: 'P1', reviewer: 'test-healer',
    category: 'test-failure', file: 'Core/Authentication/AuthService.cs' }
];

const metadata = aggregatePriority(issues);
const result = applyPriorityOverrides(issues[0], metadata);

// Output:
{
  finalPriority: 'P0',  // Escalated from P1
  originalPriorities: ['P1'],
  aggregationRule: 'OVERRIDE',
  overrideReason: 'critical_path_test_failure',
  reviewerCount: 1,
  p0Count: 0,
  p1Count: 1,
  p2Count: 0
}

// Rationale: Test failures in authentication = critical security risk
```

#### Priority Rules Summary

**Priority Aggregation Matrix**:

| Scenario | P0 Count | P1 Count | P2 Count | Final Priority | Rule Applied |
|----------|----------|----------|----------|----------------|--------------|
| Any critical | ‚â•1 | any | any | **P0** | ANY_P0 |
| Majority warning | 0 | ‚â•50% | <50% | **P1** | MAJORITY_P1 |
| Tie-breaker (50%) | 0 | 50% | 50% | **P1** | MAJORITY_P1 |
| Minority warning | 0 | <50% | >50% | **P2** | DEFAULT_P2 |
| All improvements | 0 | 0 | 100% | **P2** | DEFAULT_P2 |
| Security issue | any | any | any | **P0** | OVERRIDE (security) |
| Breaking change | any | any | any | **P0** | OVERRIDE (breaking) |
| Critical path test | any | any | any | **P0** | OVERRIDE (test) |

**Key Characteristics**:

1. **Safety-First**: ANY P0 escalates entire issue (1 veto overrides all)
2. **Consensus-Driven**: ‚â•50% majority for P1 (democratic threshold)
3. **Conservative**: Ties round up to higher priority (P1 > P2)
4. **Override Power**: Special cases trump all aggregation rules
5. **Transparent**: Full metadata tracking for audit trail

---

### 3.2B: Confidence Weighting System

**Purpose**: Calculate weighted confidence scores based on reviewer domain expertise.

#### Reviewer Weight Configuration

```typescript
/**
 * Reviewer expertise weights
 * - baseWeight: Default weight for all categories
 * - categoryWeights: Multipliers for domain-specific expertise
 */
interface ReviewerWeight {
  name: string;
  baseWeight: number;
  categoryWeights: Map<string, number>;
}

/**
 * Reviewer expertise configuration
 *
 * Design rationale:
 * - All reviewers start with baseWeight 1.0 (equal baseline)
 * - Category multipliers reflect domain expertise:
 *   - test-healer: 1.5x for test-failure (highest expertise)
 *   - code-principles-reviewer: 1.3x for SOLID (architectural expertise)
 *   - code-style-reviewer: 1.2x for formatting (style expertise)
 *
 * Multiplier ranges:
 * - 1.0-1.1x: Slight expertise advantage
 * - 1.1-1.3x: Moderate expertise advantage
 * - 1.3-1.5x: Significant expertise advantage
 * - 1.5x+: Domain specialist (highest confidence)
 */
const reviewerWeights: ReviewerWeight[] = [
  {
    name: 'code-style-reviewer',
    baseWeight: 1.0,
    categoryWeights: new Map([
      ['formatting', 1.2],        // Style specialist
      ['naming', 1.1],            // Naming conventions
      ['indentation', 1.2],       // Whitespace expertise
      ['braces', 1.2],            // Code structure
      ['spacing', 1.15]           // Visual consistency
    ])
  },
  {
    name: 'code-principles-reviewer',
    baseWeight: 1.0,
    categoryWeights: new Map([
      ['solid', 1.3],             // SOLID principles expert
      ['dry', 1.2],               // DRY principle
      ['architecture', 1.1],      // Architectural patterns
      ['kiss', 1.15],             // KISS principle
      ['separation-of-concerns', 1.2]  // SoC principle
    ])
  },
  {
    name: 'test-healer',
    baseWeight: 1.0,
    categoryWeights: new Map([
      ['test-failure', 1.5],      // HIGHEST: Test analysis specialist
      ['coverage', 1.2],          // Coverage analysis
      ['test-quality', 1.3],      // Test design
      ['test-structure', 1.2],    // Test organization
      ['assertion', 1.25]         // Assertion patterns
    ])
  }
];
```

#### Weighted Confidence Calculation

```typescript
/**
 * Calculate weighted confidence score based on reviewer expertise
 *
 * FORMULA:
 * weighted_confidence = Œ£(confidence_i √ó weight_i) / Œ£(weight_i)
 *
 * Where:
 * - confidence_i: Individual reviewer confidence (0.0-1.0)
 * - weight_i: Reviewer weight for issue category (baseWeight √ó categoryMultiplier)
 *
 * @param issues - Deduplicated issues with multiple confidence scores
 * @returns Weighted average confidence (0.0-1.0)
 *
 * @example
 * // test-failure: test-healer has 1.5x expertise
 * calculateWeightedConfidence([
 *   { reviewer: 'test-healer', confidence: 0.90, category: 'test-failure' },
 *   { reviewer: 'code-style-reviewer', confidence: 0.70, category: 'test-failure' }
 * ])
 * // ‚Üí (0.90 √ó 1.5 + 0.70 √ó 1.0) / (1.5 + 1.0) = 0.828
 *
 * // formatting: code-style-reviewer has 1.2x expertise
 * calculateWeightedConfidence([
 *   { reviewer: 'code-style-reviewer', confidence: 0.85, category: 'formatting' },
 *   { reviewer: 'code-principles-reviewer', confidence: 0.75, category: 'formatting' }
 * ])
 * // ‚Üí (0.85 √ó 1.2 + 0.75 √ó 1.0) / (1.2 + 1.0) = 0.805
 */
function calculateWeightedConfidence(issues: Issue[]): number {
  let totalWeight = 0;
  let weightedSum = 0;

  for (const issue of issues) {
    // Find reviewer weight configuration
    const reviewerWeight = findReviewerWeight(issue.reviewer);

    // Get category-specific weight (or fallback to baseWeight)
    const weight = reviewerWeight.categoryWeights.get(issue.category)
                   || reviewerWeight.baseWeight;

    // Accumulate weighted sum
    weightedSum += issue.confidence * weight;
    totalWeight += weight;
  }

  // Return weighted average
  return totalWeight > 0 ? weightedSum / totalWeight : 0.0;
}

/**
 * Find reviewer weight configuration by name
 *
 * @param reviewerName - Name of reviewer (e.g., 'test-healer')
 * @returns Reviewer weight configuration
 * @throws Error if reviewer not found in configuration
 */
function findReviewerWeight(reviewerName: string): ReviewerWeight {
  const weight = reviewerWeights.find(w => w.name === reviewerName);

  if (!weight) {
    // Default weight for unknown reviewers
    return {
      name: reviewerName,
      baseWeight: 1.0,
      categoryWeights: new Map()
    };
  }

  return weight;
}
```

#### Confidence Weighting Examples

**Example 1: test-healer Expertise (test-failure)**

```typescript
// Input: Test failure detected by test-healer (specialist) and code-style-reviewer
const issues = [
  {
    reviewer: 'test-healer',
    confidence: 0.90,
    category: 'test-failure'
  },
  {
    reviewer: 'code-style-reviewer',
    confidence: 0.70,
    category: 'test-failure'
  }
];

// Weights:
// - test-healer: 1.5x multiplier (specialist)
// - code-style-reviewer: 1.0x (base weight)

// Calculation:
// weighted_confidence = (0.90 √ó 1.5 + 0.70 √ó 1.0) / (1.5 + 1.0)
//                     = (1.35 + 0.70) / 2.5
//                     = 2.05 / 2.5
//                     = 0.82

const result = calculateWeightedConfidence(issues);
// ‚Üí 0.82

// Interpretation: test-healer's high confidence (0.90) carries more weight
// due to domain expertise, pulling average up from simple mean (0.80)
```

**Example 2: code-style-reviewer Expertise (formatting)**

```typescript
// Input: Formatting issue detected by code-style-reviewer and code-principles-reviewer
const issues = [
  {
    reviewer: 'code-style-reviewer',
    confidence: 0.85,
    category: 'formatting'
  },
  {
    reviewer: 'code-principles-reviewer',
    confidence: 0.75,
    category: 'formatting'
  }
];

// Weights:
// - code-style-reviewer: 1.2x multiplier (style specialist)
// - code-principles-reviewer: 1.0x (base weight)

// Calculation:
// weighted_confidence = (0.85 √ó 1.2 + 0.75 √ó 1.0) / (1.2 + 1.0)
//                     = (1.02 + 0.75) / 2.2
//                     = 1.77 / 2.2
//                     = 0.805

const result = calculateWeightedConfidence(issues);
// ‚Üí 0.805

// Interpretation: code-style-reviewer's expertise in formatting increases
// confidence from simple mean (0.80) to 0.805
```

**Example 3: code-principles-reviewer Expertise (SOLID)**

```typescript
// Input: SOLID violation detected by multiple reviewers
const issues = [
  {
    reviewer: 'code-principles-reviewer',
    confidence: 0.92,
    category: 'solid'
  },
  {
    reviewer: 'code-style-reviewer',
    confidence: 0.78,
    category: 'solid'
  },
  {
    reviewer: 'test-healer',
    confidence: 0.80,
    category: 'solid'
  }
];

// Weights:
// - code-principles-reviewer: 1.3x multiplier (SOLID specialist)
// - code-style-reviewer: 1.0x (base weight)
// - test-healer: 1.0x (base weight)

// Calculation:
// weighted_confidence = (0.92 √ó 1.3 + 0.78 √ó 1.0 + 0.80 √ó 1.0) / (1.3 + 1.0 + 1.0)
//                     = (1.196 + 0.78 + 0.80) / 3.3
//                     = 2.776 / 3.3
//                     = 0.841

const result = calculateWeightedConfidence(issues);
// ‚Üí 0.841

// Interpretation: code-principles-reviewer's high confidence (0.92) with
// SOLID expertise (1.3x) pulls average up from simple mean (0.833)
```

**Example 4: Equal Weights (no expertise advantage)**

```typescript
// Input: Issue in category with no specialist
const issues = [
  {
    reviewer: 'code-style-reviewer',
    confidence: 0.85,
    category: 'documentation'  // No specialist for this category
  },
  {
    reviewer: 'code-principles-reviewer',
    confidence: 0.75,
    category: 'documentation'
  }
];

// Weights:
// - code-style-reviewer: 1.0x (base weight, no category multiplier)
// - code-principles-reviewer: 1.0x (base weight, no category multiplier)

// Calculation:
// weighted_confidence = (0.85 √ó 1.0 + 0.75 √ó 1.0) / (1.0 + 1.0)
//                     = (0.85 + 0.75) / 2.0
//                     = 1.60 / 2.0
//                     = 0.80

const result = calculateWeightedConfidence(issues);
// ‚Üí 0.80

// Interpretation: No expertise advantage ‚Üí simple average
```

#### Confidence Weighting Summary

**Weight Multiplier Impact**:

| Category | Specialist Reviewer | Multiplier | Confidence Boost |
|----------|---------------------|------------|------------------|
| test-failure | test-healer | 1.5x | +50% weight |
| solid | code-principles-reviewer | 1.3x | +30% weight |
| formatting | code-style-reviewer | 1.2x | +20% weight |
| coverage | test-healer | 1.2x | +20% weight |
| dry | code-principles-reviewer | 1.2x | +20% weight |

**Key Characteristics**:

1. **Domain Expertise**: Higher weights for category specialists
2. **Fair Baseline**: All reviewers start at 1.0 base weight
3. **Weighted Average**: Formula prevents extreme values
4. **Graceful Fallback**: Unknown categories use base weight
5. **Transparent Calculation**: Full weight tracking for audit

---

### 3.2C: Priority Validation and Conflict Resolution

**Purpose**: Detect priority inconsistencies in similar issues and reconcile conflicts.

#### Validation Result Interface

```typescript
/**
 * Priority validation result
 */
interface ValidationResult {
  isValid: boolean;
  inconsistencies: PriorityInconsistency[];
  reconciled: boolean;
  reconciledIssues: Issue[];
}

/**
 * Priority inconsistency details
 */
interface PriorityInconsistency {
  issueIds: string[];
  priorities: Priority[];
  similarityScore: number;
  reconciledPriority: Priority;
  reason: string;
}
```

#### Priority Consistency Validation

```typescript
/**
 * Validate priority consistency across similar issues
 *
 * ALGORITHM:
 * 1. Group issues by similarity (semantic + exact match)
 * 2. Detect conflicts: similar issues with different priorities
 * 3. Reconcile conflicts: choose highest priority (P0 > P1 > P2)
 * 4. Report inconsistencies for manual review
 *
 * RECONCILIATION RULE:
 * - If similar issues have conflicting priorities ‚Üí use HIGHEST priority
 * - Rationale: Conservative approach (safety over permissiveness)
 *
 * @param consolidatedIssues - Issues after deduplication
 * @returns Validation result with reconciliation
 *
 * @example
 * // Input: Similar issues with conflicting priorities
 * validatePriorityConsistency([
 *   { id: 'issue-1', message: 'Missing null check', severity: 'P0',
 *     file: 'AuthService.cs', line: 42 },
 *   { id: 'issue-2', message: 'Null check missing', severity: 'P1',
 *     file: 'AuthService.cs', line: 42 }
 * ])
 *
 * // Output:
 * {
 *   isValid: false,
 *   inconsistencies: [{
 *     issueIds: ['issue-1', 'issue-2'],
 *     priorities: ['P0', 'P1'],
 *     similarityScore: 0.92,
 *     reconciledPriority: 'P0',
 *     reason: 'Similar issues had conflicting priorities. Reconciled to P0 (highest).'
 *   }],
 *   reconciled: true,
 *   reconciledIssues: [
 *     { id: 'issue-1', severity: 'P0', ... },
 *     { id: 'issue-2', severity: 'P0', ... }  // Escalated from P1 to P0
 *   ]
 * }
 */
function validatePriorityConsistency(
  consolidatedIssues: Issue[]
): ValidationResult {

  const inconsistencies: PriorityInconsistency[] = [];
  const reconciledIssues: Issue[] = [...consolidatedIssues];

  // Step 1: Group issues by similarity
  const similarGroups = groupBySimilarity(consolidatedIssues);

  // Step 2: Detect priority conflicts within each group
  for (const group of similarGroups) {
    // Extract priorities from group
    const priorities = group.issues.map(i => i.severity as Priority);
    const uniquePriorities = new Set(priorities);

    // Step 3: If multiple priorities exist ‚Üí inconsistency detected
    if (uniquePriorities.size > 1) {
      // Determine highest priority (P0 > P1 > P2)
      const highestPriority = reconcilePriorities(priorities);

      // Record inconsistency
      inconsistencies.push({
        issueIds: group.issues.map(i => i.id),
        priorities: priorities,
        similarityScore: group.averageSimilarity,
        reconciledPriority: highestPriority,
        reason: `Similar issues (similarity: ${(group.averageSimilarity * 100).toFixed(1)}%) ` +
                `had conflicting priorities: ${priorities.join(', ')}. ` +
                `Reconciled to ${highestPriority} (highest priority).`
      });

      // Step 4: Reconcile - update all issues in group to highest priority
      for (const issue of group.issues) {
        const reconciledIssue = reconciledIssues.find(i => i.id === issue.id);
        if (reconciledIssue) {
          reconciledIssue.severity = highestPriority;
          reconciledIssue.metadata = {
            ...reconciledIssue.metadata,
            priorityReconciled: true,
            originalPriority: issue.severity,
            reconciledFrom: priorities
          };
        }
      }
    }
  }

  return {
    isValid: inconsistencies.length === 0,
    inconsistencies,
    reconciled: inconsistencies.length > 0,
    reconciledIssues
  };
}

/**
 * Reconcile conflicting priorities by choosing highest
 *
 * Priority hierarchy: P0 > P1 > P2
 *
 * @param priorities - Array of priorities to reconcile
 * @returns Highest priority
 *
 * @example
 * reconcilePriorities(['P0', 'P1', 'P2']) // ‚Üí 'P0'
 * reconcilePriorities(['P1', 'P2', 'P2']) // ‚Üí 'P1'
 * reconcilePriorities(['P2', 'P2', 'P2']) // ‚Üí 'P2'
 */
function reconcilePriorities(priorities: Priority[]): Priority {
  if (priorities.includes(Priority.P0)) {
    return Priority.P0;
  }
  if (priorities.includes(Priority.P1)) {
    return Priority.P1;
  }
  return Priority.P2;
}

/**
 * Group issues by similarity for conflict detection
 *
 * Uses same similarity algorithm as deduplication (Task 3.1):
 * - Levenshtein distance < 15 characters ‚Üí similar
 * - Same file + line range overlap ‚Üí similar
 * - Same category + semantic match ‚Üí similar
 *
 * @param issues - Issues to group
 * @returns Groups of similar issues
 */
interface SimilarityGroup {
  issues: Issue[];
  averageSimilarity: number;
}

function groupBySimilarity(issues: Issue[]): SimilarityGroup[] {
  const groups: SimilarityGroup[] = [];
  const processed = new Set<string>();

  for (let i = 0; i < issues.length; i++) {
    if (processed.has(issues[i].id)) continue;

    const group: Issue[] = [issues[i]];
    processed.add(issues[i].id);

    let totalSimilarity = 0;
    let comparisons = 0;

    for (let j = i + 1; j < issues.length; j++) {
      if (processed.has(issues[j].id)) continue;

      const similarity = calculateSimilarity(issues[i], issues[j]);

      // Similarity threshold: 0.85 (same as deduplication)
      if (similarity >= 0.85) {
        group.push(issues[j]);
        processed.add(issues[j].id);
        totalSimilarity += similarity;
        comparisons++;
      }
    }

    const averageSimilarity = comparisons > 0
      ? totalSimilarity / comparisons
      : 1.0;

    groups.push({
      issues: group,
      averageSimilarity
    });
  }

  return groups;
}
```

#### Priority Validation Examples

**Example 1: Conflict Detected and Reconciled**

```typescript
// Input: Similar issues with conflicting priorities
const issues = [
  {
    id: 'issue-1',
    message: 'Missing null check in ProcessRequest method',
    severity: 'P0',
    file: 'Services/AuthService.cs',
    line: 42,
    category: 'error-handling'
  },
  {
    id: 'issue-2',
    message: 'Null check missing in ProcessRequest',
    severity: 'P1',
    file: 'Services/AuthService.cs',
    line: 42,
    category: 'error-handling'
  }
];

const result = validatePriorityConsistency(issues);

// Output:
{
  isValid: false,  // Inconsistency detected
  inconsistencies: [
    {
      issueIds: ['issue-1', 'issue-2'],
      priorities: ['P0', 'P1'],
      similarityScore: 0.92,  // High similarity (92%)
      reconciledPriority: 'P0',  // Highest priority wins
      reason: 'Similar issues (similarity: 92.0%) had conflicting priorities: P0, P1. ' +
              'Reconciled to P0 (highest priority).'
    }
  ],
  reconciled: true,  // Auto-reconciliation applied
  reconciledIssues: [
    { id: 'issue-1', severity: 'P0', ... },  // Unchanged
    {
      id: 'issue-2',
      severity: 'P0',  // Escalated from P1 to P0
      metadata: {
        priorityReconciled: true,
        originalPriority: 'P1',
        reconciledFrom: ['P0', 'P1']
      }
    }
  ]
}

// Action: Both issues now have P0 priority (conservative reconciliation)
```

**Example 2: No Conflicts (All Consistent)**

```typescript
// Input: Similar issues with same priority
const issues = [
  {
    id: 'issue-1',
    message: 'Variable name should use camelCase',
    severity: 'P1',
    file: 'Services/AuthService.cs',
    line: 42
  },
  {
    id: 'issue-2',
    message: 'Variable name not following camelCase convention',
    severity: 'P1',
    file: 'Services/AuthService.cs',
    line: 42
  }
];

const result = validatePriorityConsistency(issues);

// Output:
{
  isValid: true,  // No inconsistencies
  inconsistencies: [],
  reconciled: false,
  reconciledIssues: [
    { id: 'issue-1', severity: 'P1', ... },  // Unchanged
    { id: 'issue-2', severity: 'P1', ... }   // Unchanged
  ]
}

// Action: No reconciliation needed, priorities already consistent
```

**Example 3: Multiple Conflict Groups**

```typescript
// Input: Multiple similarity groups with different conflicts
const issues = [
  // Group 1: Null check conflict (P0 vs P1)
  { id: 'issue-1', message: 'Missing null check', severity: 'P0',
    file: 'AuthService.cs', line: 42 },
  { id: 'issue-2', message: 'Null check missing', severity: 'P1',
    file: 'AuthService.cs', line: 42 },

  // Group 2: Naming conflict (P1 vs P2)
  { id: 'issue-3', message: 'Variable name not camelCase', severity: 'P1',
    file: 'UserService.cs', line: 15 },
  { id: 'issue-4', message: 'camelCase convention not followed', severity: 'P2',
    file: 'UserService.cs', line: 15 }
];

const result = validatePriorityConsistency(issues);

// Output:
{
  isValid: false,
  inconsistencies: [
    {
      issueIds: ['issue-1', 'issue-2'],
      priorities: ['P0', 'P1'],
      similarityScore: 0.91,
      reconciledPriority: 'P0',
      reason: 'Similar issues (similarity: 91.0%) had conflicting priorities: P0, P1. ' +
              'Reconciled to P0 (highest priority).'
    },
    {
      issueIds: ['issue-3', 'issue-4'],
      priorities: ['P1', 'P2'],
      similarityScore: 0.88,
      reconciledPriority: 'P1',
      reason: 'Similar issues (similarity: 88.0%) had conflicting priorities: P1, P2. ' +
              'Reconciled to P1 (highest priority).'
    }
  ],
  reconciled: true,
  reconciledIssues: [
    { id: 'issue-1', severity: 'P0', ... },  // Unchanged
    { id: 'issue-2', severity: 'P0', ... },  // Escalated from P1
    { id: 'issue-3', severity: 'P1', ... },  // Unchanged
    { id: 'issue-4', severity: 'P1', ... }   // Escalated from P2
  ]
}

// Action: Both conflict groups reconciled conservatively (highest priority)
```

**Example 4: Three-Way Conflict (P0 vs P1 vs P2)**

```typescript
// Input: Three similar issues with three different priorities
const issues = [
  { id: 'issue-1', message: 'Error handling missing', severity: 'P0',
    file: 'AuthService.cs', line: 85 },
  { id: 'issue-2', message: 'Missing error handling', severity: 'P1',
    file: 'AuthService.cs', line: 85 },
  { id: 'issue-3', message: 'Error handler not implemented', severity: 'P2',
    file: 'AuthService.cs', line: 85 }
];

const result = validatePriorityConsistency(issues);

// Output:
{
  isValid: false,
  inconsistencies: [
    {
      issueIds: ['issue-1', 'issue-2', 'issue-3'],
      priorities: ['P0', 'P1', 'P2'],
      similarityScore: 0.89,
      reconciledPriority: 'P0',  // Highest among all three
      reason: 'Similar issues (similarity: 89.0%) had conflicting priorities: P0, P1, P2. ' +
              'Reconciled to P0 (highest priority).'
    }
  ],
  reconciled: true,
  reconciledIssues: [
    { id: 'issue-1', severity: 'P0', ... },  // Unchanged
    { id: 'issue-2', severity: 'P0', ... },  // Escalated from P1
    { id: 'issue-3', severity: 'P0', ... }   // Escalated from P2
  ]
}

// Action: All three issues escalated to P0 (conservative approach)
```

#### Priority Validation Summary

**Conflict Resolution Matrix**:

| Conflicting Priorities | Reconciled Priority | Rationale |
|------------------------|---------------------|-----------|
| [P0, P1] | **P0** | P0 > P1 (critical wins) |
| [P0, P2] | **P0** | P0 > P2 (critical wins) |
| [P1, P2] | **P1** | P1 > P2 (warning wins) |
| [P0, P1, P2] | **P0** | P0 > all (critical wins) |
| [P1, P1, P2] | **P1** | P1 > P2 (majority + higher) |

**Key Characteristics**:

1. **Conservative Reconciliation**: Always choose highest priority (safety-first)
2. **Similarity-Based**: Only reconcile issues with high similarity (‚â•85%)
3. **Transparent Tracking**: Full audit trail of original priorities
4. **Automatic Resolution**: No manual intervention for unambiguous cases
5. **Conflict Reporting**: Detailed inconsistency reports for review

---

## Priority Aggregation Integration

### Complete Workflow

```typescript
/**
 * Full priority aggregation workflow
 *
 * STEPS:
 * 1. Aggregate priority from multiple reviewer ratings
 * 2. Apply special overrides (security, breaking changes, critical path)
 * 3. Calculate weighted confidence based on domain expertise
 * 4. Validate priority consistency across similar issues
 * 5. Return enriched issue with final priority + metadata
 */
function processPriorityAggregation(
  deduplicatedIssues: Issue[]
): ProcessedIssue[] {

  const processedIssues: ProcessedIssue[] = [];

  for (const issue of deduplicatedIssues) {
    // Step 1: Aggregate priority from source issues
    const priorityMetadata = aggregatePriority(issue.sourceIssues);

    // Step 2: Apply special overrides
    const finalMetadata = applyPriorityOverrides(issue, priorityMetadata);

    // Step 3: Calculate weighted confidence
    const weightedConfidence = calculateWeightedConfidence(issue.sourceIssues);

    // Step 4: Create enriched issue
    processedIssues.push({
      ...issue,
      severity: finalMetadata.finalPriority,
      confidence: weightedConfidence,
      priorityMetadata: finalMetadata,
      processed: true
    });
  }

  // Step 5: Validate priority consistency
  const validationResult = validatePriorityConsistency(processedIssues);

  // Return reconciled issues if conflicts detected
  return validationResult.reconciled
    ? validationResult.reconciledIssues
    : processedIssues;
}
```

### Integration Example

```typescript
// INPUT: Deduplicated issues from Task 3.1
const deduplicatedIssues = [
  {
    id: 'consolidated-1',
    message: 'Missing null check in ProcessRequest method',
    file: 'Services/AuthService.cs',
    line: 42,
    category: 'error-handling',
    sourceIssues: [
      { reviewer: 'code-principles-reviewer', severity: 'P0', confidence: 0.88 },
      { reviewer: 'test-healer', severity: 'P1', confidence: 0.82 }
    ]
  }
];

// PROCESS: Apply priority aggregation
const processedIssues = processPriorityAggregation(deduplicatedIssues);

// OUTPUT: Enriched issue with aggregated priority
[
  {
    id: 'consolidated-1',
    message: 'Missing null check in ProcessRequest method',
    file: 'Services/AuthService.cs',
    line: 42,
    category: 'error-handling',
    severity: 'P0',  // ANY P0 rule applied
    confidence: 0.85,  // Weighted average
    priorityMetadata: {
      finalPriority: 'P0',
      originalPriorities: ['P0', 'P1'],
      aggregationRule: 'ANY_P0',
      reviewerCount: 2,
      p0Count: 1,
      p1Count: 1,
      p2Count: 0
    },
    sourceIssues: [
      { reviewer: 'code-principles-reviewer', severity: 'P0', confidence: 0.88 },
      { reviewer: 'test-healer', severity: 'P1', confidence: 0.82 }
    ],
    processed: true
  }
]
```

---

---

## 3. RECOMMENDATION SYNTHESIS

**Purpose**: Extract actionable recommendations from consolidated issues, generate prioritized action items, and build theme summaries for strategic insights.

**Core Principle**: Transform validated issues into actionable guidance that helps developers prioritize fixes and understand patterns.

**Key Components**:
1. Recommendation Extractor (3.3A)
2. Action Item Generator (3.3B)
3. Theme Summary Builder (3.3C)

---

### 3.1. Recommendation Extractor

**Goal**: Extract recommendations from high-confidence issues (‚â•60%) and categorize them by theme.

**Confidence Threshold**: 0.60 (60%) - only extract from reliable issues

**Theme Categories**:
- **refactoring**: Code structure improvements
- **testing**: Test coverage and quality
- **documentation**: Code documentation and comments
- **performance**: Speed and efficiency optimizations
- **security**: Security vulnerabilities and validation
- **general**: Uncategorized recommendations

#### Recommendation Interface

```typescript
/**
 * Recommendation from review consolidation
 *
 * Represents actionable guidance extracted from consolidated issues.
 * Multiple reviewers may suggest the same improvement.
 */
interface Recommendation {
  /** Recommendation category/theme */
  theme: string;

  /** Human-readable recommendation description */
  description: string;

  /** Number of reviewers who suggested this */
  frequency: number;

  /** Average confidence across all sources (0.0-1.0) */
  confidence: number;

  /** Related issue IDs that support this recommendation */
  relatedIssues: string[];

  /** Estimated effort to implement */
  effort: 'low' | 'medium' | 'high';

  /** Unique reviewers who suggested this */
  reviewers: string[];

  /** Keywords that triggered this theme */
  matchedKeywords: string[];
}
```

#### extractRecommendations() Function

```typescript
/**
 * Extract recommendations from consolidated issues
 *
 * ALGORITHM:
 * 1. Filter issues by confidence (‚â•0.60)
 * 2. Extract suggestions from issues
 * 3. Categorize each suggestion by theme
 * 4. Deduplicate similar recommendations
 * 5. Rank by frequency (most common first)
 *
 * @param reviewResults - All review results from parallel execution
 * @returns Ranked list of recommendations
 */
function extractRecommendations(reviewResults: ReviewResult[]): Recommendation[] {
  const recommendations: Recommendation[] = [];

  // Process all review results
  for (const result of reviewResults) {
    const reviewer = result.reviewer;

    // Process each issue in the review
    for (const issue of result.issues) {
      // RULE: Only extract from high-confidence issues
      if (!issue.suggestion || issue.confidence < 0.60) {
        continue;
      }

      // Categorize recommendation by theme
      const theme = categorizeRecommendation(issue.suggestion);

      // Add or update recommendation (deduplication)
      addOrUpdateRecommendation(
        recommendations,
        theme,
        issue,
        reviewer
      );
    }
  }

  // Rank by frequency (most common first)
  return recommendations.sort((a, b) => b.frequency - a.frequency);
}
```

#### categorizeRecommendation() Function

```typescript
/**
 * Categorize recommendation by theme using keyword matching
 *
 * ALGORITHM:
 * 1. Normalize suggestion text (lowercase)
 * 2. Check each theme's keywords
 * 3. Return first matching theme
 * 4. Default to 'general' if no match
 *
 * @param suggestion - Recommendation text from issue
 * @returns Theme category
 */
function categorizeRecommendation(suggestion: string): string {
  // Theme keyword mappings
  const keywords: Record<string, string[]> = {
    'refactoring': [
      'refactor',
      'extract',
      'simplify',
      'clean up',
      'restructure',
      'reorganize',
      'modularize',
      'split',
      'separate'
    ],
    'testing': [
      'test',
      'coverage',
      'assertion',
      'mock',
      'unit test',
      'integration test',
      'test case',
      'verify',
      'validate behavior'
    ],
    'documentation': [
      'document',
      'comment',
      'explain',
      'describe',
      'clarify',
      'xml doc',
      'summary',
      'remarks',
      'example'
    ],
    'performance': [
      'optimize',
      'cache',
      'speed',
      'efficient',
      'performance',
      'faster',
      'reduce',
      'improve latency',
      'async'
    ],
    'security': [
      'secure',
      'validate',
      'sanitize',
      'encrypt',
      'authentication',
      'authorization',
      'injection',
      'xss',
      'csrf'
    ]
  };

  const normalized = suggestion.toLowerCase();

  // Check each theme's keywords
  for (const [theme, words] of Object.entries(keywords)) {
    const matchedWords = words.filter(word => normalized.includes(word));

    if (matchedWords.length > 0) {
      // Return theme with matched keywords for transparency
      return theme;
    }
  }

  // Default to general category
  return 'general';
}
```

#### addOrUpdateRecommendation() Helper

```typescript
/**
 * Add new recommendation or update existing similar one
 *
 * DEDUPLICATION STRATEGY:
 * 1. Check if similar recommendation exists (same theme + overlapping issues)
 * 2. If exists: Merge (increment frequency, add issues, update confidence)
 * 3. If new: Add to list
 *
 * @param recommendations - Current recommendations list
 * @param theme - Recommendation theme
 * @param issue - Source issue
 * @param reviewer - Reviewer name
 */
function addOrUpdateRecommendation(
  recommendations: Recommendation[],
  theme: string,
  issue: Issue,
  reviewer: string
): void {
  // Find existing recommendation with same theme
  const existing = recommendations.find(rec => {
    // Same theme?
    if (rec.theme !== theme) {
      return false;
    }

    // Overlapping issues? (at least 1 common issue ID)
    const commonIssues = rec.relatedIssues.filter(id =>
      issue.id === id || issue.sourceIssues?.some(si => si.id === id)
    );

    return commonIssues.length > 0;
  });

  if (existing) {
    // UPDATE: Merge with existing recommendation
    existing.frequency += 1;
    existing.relatedIssues.push(issue.id);

    // Add reviewer if not already present
    if (!existing.reviewers.includes(reviewer)) {
      existing.reviewers.push(reviewer);
    }

    // Recalculate average confidence
    existing.confidence = calculateAverageConfidence(existing.relatedIssues);
  } else {
    // ADD: Create new recommendation
    recommendations.push({
      theme,
      description: issue.suggestion!,
      frequency: 1,
      confidence: issue.confidence,
      relatedIssues: [issue.id],
      effort: estimateEffortFromIssue(issue),
      reviewers: [reviewer],
      matchedKeywords: extractKeywords(issue.suggestion!, theme)
    });
  }
}
```

#### Helper Functions

```typescript
/**
 * Estimate effort from issue characteristics
 */
function estimateEffortFromIssue(issue: Issue): 'low' | 'medium' | 'high' {
  // Simple issue (style, naming) ‚Üí low effort
  if (issue.category === 'code-style' || issue.category === 'naming') {
    return 'low';
  }

  // Complex issue (architecture, security) ‚Üí high effort
  if (issue.category === 'architecture' || issue.category === 'security') {
    return 'high';
  }

  // Medium effort by default
  return 'medium';
}

/**
 * Extract keywords that matched for this theme
 */
function extractKeywords(suggestion: string, theme: string): string[] {
  const keywords: Record<string, string[]> = {
    'refactoring': ['refactor', 'extract', 'simplify', 'clean up'],
    'testing': ['test', 'coverage', 'assertion', 'mock'],
    'documentation': ['document', 'comment', 'explain', 'describe'],
    'performance': ['optimize', 'cache', 'speed', 'efficient'],
    'security': ['secure', 'validate', 'sanitize', 'encrypt']
  };

  const normalized = suggestion.toLowerCase();
  const themeKeywords = keywords[theme] || [];

  return themeKeywords.filter(word => normalized.includes(word));
}

/**
 * Calculate average confidence from related issues
 */
function calculateAverageConfidence(issueIds: string[]): number {
  // In real implementation, lookup issues and average their confidence
  // Placeholder for pseudo-code
  return 0.75;
}
```

#### Extraction Examples

**Example 1: Single Theme Extraction**

```typescript
// INPUT: Review results with recommendations
const reviewResults: ReviewResult[] = [
  {
    reviewer: 'code-principles-reviewer',
    timestamp: '2025-10-16T10:00:00Z',
    issues: [
      {
        id: 'issue-1',
        message: 'Duplicate validation logic in UserController and AuthController',
        file: 'Controllers/UserController.cs',
        line: 42,
        severity: 'P1',
        confidence: 0.85,
        category: 'code-duplication',
        suggestion: 'Extract validation logic to shared ValidationService'
      }
    ]
  },
  {
    reviewer: 'code-style-reviewer',
    timestamp: '2025-10-16T10:01:00Z',
    issues: [
      {
        id: 'issue-2',
        message: 'DRY violation: validation repeated across controllers',
        file: 'Controllers/AuthController.cs',
        line: 28,
        severity: 'P1',
        confidence: 0.78,
        category: 'code-duplication',
        suggestion: 'Refactor common validation into base controller'
      }
    ]
  }
];

// PROCESS: Extract recommendations
const recommendations = extractRecommendations(reviewResults);

// OUTPUT: Deduplicated recommendations
[
  {
    theme: 'refactoring',
    description: 'Extract validation logic to shared ValidationService',
    frequency: 2,  // Both reviewers suggested this
    confidence: 0.815,  // Average of 0.85 and 0.78
    relatedIssues: ['issue-1', 'issue-2'],
    effort: 'medium',
    reviewers: ['code-principles-reviewer', 'code-style-reviewer'],
    matchedKeywords: ['extract', 'refactor']
  }
]
```

**Example 2: Multiple Theme Extraction**

```typescript
// INPUT: Mixed recommendations across themes
const reviewResults: ReviewResult[] = [
  {
    reviewer: 'code-principles-reviewer',
    issues: [
      {
        id: 'issue-1',
        message: 'Missing unit tests for AuthService',
        severity: 'P1',
        confidence: 0.88,
        suggestion: 'Add unit tests to cover authentication logic'
      },
      {
        id: 'issue-2',
        message: 'No input validation on login endpoint',
        severity: 'P0',
        confidence: 0.92,
        suggestion: 'Add validation attributes to prevent injection attacks'
      }
    ]
  },
  {
    reviewer: 'test-healer',
    issues: [
      {
        id: 'issue-3',
        message: 'Test coverage for AuthService is 42%',
        severity: 'P1',
        confidence: 0.95,
        suggestion: 'Increase test coverage to 80% for critical paths'
      }
    ]
  }
];

// OUTPUT: Multiple themes extracted
[
  {
    theme: 'testing',
    description: 'Add unit tests to cover authentication logic',
    frequency: 2,
    confidence: 0.915,
    relatedIssues: ['issue-1', 'issue-3'],
    effort: 'medium',
    reviewers: ['code-principles-reviewer', 'test-healer'],
    matchedKeywords: ['test', 'coverage']
  },
  {
    theme: 'security',
    description: 'Add validation attributes to prevent injection attacks',
    frequency: 1,
    confidence: 0.92,
    relatedIssues: ['issue-2'],
    effort: 'high',
    reviewers: ['code-principles-reviewer'],
    matchedKeywords: ['validation', 'injection']
  }
]
```

**Example 3: Low Confidence Filtering**

```typescript
// INPUT: Mixed confidence recommendations
const reviewResults: ReviewResult[] = [
  {
    reviewer: 'code-style-reviewer',
    issues: [
      {
        id: 'issue-1',
        confidence: 0.45,  // Below threshold
        suggestion: 'Consider renaming method (not sure)'
      },
      {
        id: 'issue-2',
        confidence: 0.72,  // Above threshold
        suggestion: 'Simplify nested if statements using guard clauses'
      },
      {
        id: 'issue-3',
        confidence: 0.59,  // Just below threshold
        suggestion: 'Maybe extract this to helper'
      }
    ]
  }
];

// OUTPUT: Only high-confidence recommendation extracted
[
  {
    theme: 'refactoring',
    description: 'Simplify nested if statements using guard clauses',
    frequency: 1,
    confidence: 0.72,  // Only issue-2 included
    relatedIssues: ['issue-2'],
    effort: 'low',
    reviewers: ['code-style-reviewer'],
    matchedKeywords: ['simplify']
  }
]

// NOTE: issue-1 (0.45) and issue-3 (0.59) excluded by ‚â•0.60 threshold
```

---

### 3.2. Action Item Generator

**Goal**: Convert consolidated issues into prioritized, actionable tasks with effort estimates.

**Sorting Order**:
1. Priority: P0 > P1 > P2
2. Effort: low > medium > high (within same priority)

**Effort Estimation**:
- Simple issues (style, naming) ‚Üí "1-2h"
- Medium issues (refactoring, tests) ‚Üí "3-4h"
- Complex issues (architecture, security) ‚Üí "5-8h"

#### ActionItem Interface

```typescript
/**
 * Actionable task for developers
 *
 * Represents a prioritized work item with effort estimate and context.
 */
interface ActionItem {
  /** Task priority (P0 critical, P1 warning, P2 suggestion) */
  priority: Priority;

  /** Human-readable task title */
  title: string;

  /** Estimated time to complete (e.g., "2h", "3-4h") */
  estimatedEffort: string;

  /** Related issue IDs for context */
  relatedIssues: string[];

  /** Actionable recommendation text */
  recommendation: string;

  /** Reviewers who identified this issue */
  reviewers: string[];

  /** Files affected by this action */
  files: string[];

  /** Optional: Quick win flag (low effort, high impact) */
  quickWin?: boolean;
}
```

#### generateActionItems() Function

```typescript
/**
 * Generate prioritized action items from consolidated issues
 *
 * ALGORITHM:
 * 1. Convert each consolidated issue to action item
 * 2. Estimate effort based on issue complexity
 * 3. Find and link related issues
 * 4. Sort by priority (P0 > P1 > P2) then effort (low > medium > high)
 * 5. Flag quick wins (low effort, high priority)
 *
 * @param consolidatedIssues - Issues after deduplication and priority aggregation
 * @returns Sorted list of action items
 */
function generateActionItems(consolidatedIssues: Issue[]): ActionItem[] {
  const items: ActionItem[] = [];

  for (const issue of consolidatedIssues) {
    // Estimate effort from issue characteristics
    const effort = estimateEffort(issue);

    // Find related issues (similar location, theme, or reviewer)
    const relatedIds = findRelatedIssues(issue, consolidatedIssues);

    // Extract unique reviewers
    const reviewers = issue.sources
      ? issue.sources.map(s => s.reviewer)
      : [issue.reviewer];

    // Extract affected files
    const files = [
      issue.file,
      ...relatedIds.map(id => {
        const related = consolidatedIssues.find(i => i.id === id);
        return related?.file || '';
      })
    ].filter((f, idx, arr) => f && arr.indexOf(f) === idx);

    // Create action item
    const item: ActionItem = {
      priority: issue.severity,
      title: `${issue.message} (${issue.file}:${issue.line})`,
      estimatedEffort: effort,
      relatedIssues: [issue.id, ...relatedIds],
      recommendation: issue.suggestion || 'Manual fix required',
      reviewers: [...new Set(reviewers)],  // Deduplicate
      files
    };

    // Flag quick wins (P0/P1 with low effort)
    if ((issue.severity === 'P0' || issue.severity === 'P1') &&
        effort === '1-2h') {
      item.quickWin = true;
    }

    items.push(item);
  }

  // Sort by priority (P0 > P1 > P2) then effort (low > medium > high)
  return items.sort(compareActionItems);
}
```

#### estimateEffort() Function

```typescript
/**
 * Estimate effort to fix issue
 *
 * EFFORT RULES:
 * - Simple (style, naming, braces): 1-2h
 * - Medium (refactoring, tests, docs): 3-4h
 * - Complex (architecture, security, breaking): 5-8h
 *
 * @param issue - Consolidated issue
 * @returns Effort estimate string
 */
function estimateEffort(issue: Issue): string {
  // Simple fixes (style, formatting, naming)
  const simpleCategories = [
    'code-style',
    'naming',
    'formatting',
    'braces',
    'whitespace'
  ];

  if (simpleCategories.includes(issue.category)) {
    return '1-2h';
  }

  // Complex fixes (architecture, security, breaking changes)
  const complexCategories = [
    'architecture',
    'security',
    'breaking-change',
    'performance-critical',
    'data-integrity'
  ];

  if (complexCategories.includes(issue.category)) {
    return '5-8h';
  }

  // Check for complexity indicators in message
  const complexKeywords = [
    'refactor',
    'redesign',
    'restructure',
    'breaking',
    'migration',
    'security'
  ];

  const message = issue.message.toLowerCase();
  if (complexKeywords.some(kw => message.includes(kw))) {
    return '5-8h';
  }

  // Medium effort by default
  return '3-4h';
}
```

#### findRelatedIssues() Function

```typescript
/**
 * Find issues related to current issue
 *
 * RELATION CRITERIA:
 * 1. Same file and nearby lines (¬±10 lines)
 * 2. Same category and similar message
 * 3. Same reviewer and similar location
 *
 * @param issue - Target issue
 * @param allIssues - All consolidated issues
 * @returns Array of related issue IDs
 */
function findRelatedIssues(issue: Issue, allIssues: Issue[]): string[] {
  const related: string[] = [];

  for (const other of allIssues) {
    // Skip self
    if (other.id === issue.id) {
      continue;
    }

    // Criterion 1: Same file, nearby lines (¬±10 lines)
    if (other.file === issue.file &&
        Math.abs(other.line - issue.line) <= 10) {
      related.push(other.id);
      continue;
    }

    // Criterion 2: Same category and high message similarity
    if (other.category === issue.category) {
      const similarity = calculateTextSimilarity(issue.message, other.message);
      if (similarity >= 0.70) {
        related.push(other.id);
        continue;
      }
    }

    // Criterion 3: Common reviewer and similar file path
    const commonReviewers = issue.sources?.some(s1 =>
      other.sources?.some(s2 => s1.reviewer === s2.reviewer)
    );

    if (commonReviewers && isSimilarFilePath(issue.file, other.file)) {
      related.push(other.id);
    }
  }

  return related;
}

/**
 * Check if file paths are similar (same directory or similar names)
 */
function isSimilarFilePath(path1: string, path2: string): boolean {
  const dir1 = path1.split('/').slice(0, -1).join('/');
  const dir2 = path2.split('/').slice(0, -1).join('/');

  return dir1 === dir2;
}
```

#### compareActionItems() Comparator

```typescript
/**
 * Compare action items for sorting
 *
 * SORT ORDER:
 * 1. Priority: P0 > P1 > P2
 * 2. Effort (within same priority): low > medium > high
 *
 * RATIONALE: Fix critical issues first, prioritizing quick wins
 *
 * @param a - First action item
 * @param b - Second action item
 * @returns Comparison result (-1, 0, 1)
 */
function compareActionItems(a: ActionItem, b: ActionItem): number {
  // Priority order map
  const priorityOrder: Record<Priority, number> = {
    'P0': 0,
    'P1': 1,
    'P2': 2
  };

  // Compare priority first
  const priorityDiff = priorityOrder[a.priority] - priorityOrder[b.priority];
  if (priorityDiff !== 0) {
    return priorityDiff;
  }

  // If same priority, compare effort (low effort first)
  const effortOrder: Record<string, number> = {
    '1-2h': 0,
    '3-4h': 1,
    '5-8h': 2
  };

  const effortA = effortOrder[a.estimatedEffort] ?? 1;
  const effortB = effortOrder[b.estimatedEffort] ?? 1;

  return effortA - effortB;
}
```

#### Action Item Examples

**Example 1: Single Priority Group**

```typescript
// INPUT: Consolidated issues with same priority
const issues: Issue[] = [
  {
    id: 'issue-1',
    message: 'Missing braces in if statement',
    file: 'Services/AuthService.cs',
    line: 42,
    severity: 'P1',
    category: 'code-style',
    suggestion: 'Add braces to all block statements'
  },
  {
    id: 'issue-2',
    message: 'Complex method needs refactoring',
    file: 'Services/UserService.cs',
    line: 105,
    severity: 'P1',
    category: 'refactoring',
    suggestion: 'Extract method to reduce complexity'
  }
];

// PROCESS: Generate action items
const actionItems = generateActionItems(issues);

// OUTPUT: Sorted by effort (low first within P1)
[
  {
    priority: 'P1',
    title: 'Missing braces in if statement (Services/AuthService.cs:42)',
    estimatedEffort: '1-2h',  // Simple fix first
    relatedIssues: ['issue-1'],
    recommendation: 'Add braces to all block statements',
    reviewers: ['code-style-reviewer'],
    files: ['Services/AuthService.cs'],
    quickWin: true  // P1 + low effort
  },
  {
    priority: 'P1',
    title: 'Complex method needs refactoring (Services/UserService.cs:105)',
    estimatedEffort: '3-4h',  // Medium fix second
    relatedIssues: ['issue-2'],
    recommendation: 'Extract method to reduce complexity',
    reviewers: ['code-principles-reviewer'],
    files: ['Services/UserService.cs'],
    quickWin: false
  }
]
```

**Example 2: Mixed Priorities**

```typescript
// INPUT: Issues with different priorities
const issues: Issue[] = [
  {
    id: 'issue-1',
    message: 'Null reference exception risk',
    file: 'Controllers/AuthController.cs',
    line: 28,
    severity: 'P0',
    category: 'error-handling',
    suggestion: 'Add null check before property access'
  },
  {
    id: 'issue-2',
    message: 'Variable name not camelCase',
    file: 'Services/UserService.cs',
    line: 15,
    severity: 'P2',
    category: 'naming',
    suggestion: 'Rename to follow camelCase convention'
  },
  {
    id: 'issue-3',
    message: 'Missing XML documentation',
    file: 'Services/AuthService.cs',
    line: 42,
    severity: 'P1',
    category: 'documentation',
    suggestion: 'Add XML summary to public method'
  }
];

// OUTPUT: Sorted by priority (P0 > P1 > P2)
[
  {
    priority: 'P0',
    title: 'Null reference exception risk (Controllers/AuthController.cs:28)',
    estimatedEffort: '1-2h',
    relatedIssues: ['issue-1'],
    recommendation: 'Add null check before property access',
    reviewers: ['code-principles-reviewer'],
    files: ['Controllers/AuthController.cs'],
    quickWin: true  // P0 + low effort
  },
  {
    priority: 'P1',
    title: 'Missing XML documentation (Services/AuthService.cs:42)',
    estimatedEffort: '3-4h',
    relatedIssues: ['issue-3'],
    recommendation: 'Add XML summary to public method',
    reviewers: ['code-style-reviewer'],
    files: ['Services/AuthService.cs'],
    quickWin: false
  },
  {
    priority: 'P2',
    title: 'Variable name not camelCase (Services/UserService.cs:15)',
    estimatedEffort: '1-2h',
    relatedIssues: ['issue-2'],
    recommendation: 'Rename to follow camelCase convention',
    reviewers: ['code-style-reviewer'],
    files: ['Services/UserService.cs'],
    quickWin: false  // P2 not high priority
  }
]
```

**Example 3: Related Issues Linking**

```typescript
// INPUT: Multiple issues in same file
const issues: Issue[] = [
  {
    id: 'issue-1',
    message: 'Missing null check in ProcessRequest',
    file: 'Services/AuthService.cs',
    line: 42,
    severity: 'P0',
    category: 'error-handling'
  },
  {
    id: 'issue-2',
    message: 'Null reference risk in ValidateToken',
    file: 'Services/AuthService.cs',
    line: 48,  // Only 6 lines away
    severity: 'P0',
    category: 'error-handling'
  },
  {
    id: 'issue-3',
    message: 'No null check in RefreshToken',
    file: 'Services/AuthService.cs',
    line: 55,  // Only 7 lines from issue-2
    severity: 'P0',
    category: 'error-handling'
  }
];

// OUTPUT: Related issues linked together
[
  {
    priority: 'P0',
    title: 'Missing null check in ProcessRequest (Services/AuthService.cs:42)',
    estimatedEffort: '1-2h',
    relatedIssues: ['issue-1', 'issue-2', 'issue-3'],  // All 3 linked
    recommendation: 'Add null checks before property access',
    reviewers: ['code-principles-reviewer'],
    files: ['Services/AuthService.cs'],
    quickWin: true
  },
  {
    priority: 'P0',
    title: 'Null reference risk in ValidateToken (Services/AuthService.cs:48)',
    estimatedEffort: '1-2h',
    relatedIssues: ['issue-2', 'issue-1', 'issue-3'],  // All 3 linked
    recommendation: 'Add null checks before property access',
    reviewers: ['code-principles-reviewer'],
    files: ['Services/AuthService.cs'],
    quickWin: true
  },
  {
    priority: 'P0',
    title: 'No null check in RefreshToken (Services/AuthService.cs:55)',
    estimatedEffort: '1-2h',
    relatedIssues: ['issue-3', 'issue-2', 'issue-1'],  // All 3 linked
    recommendation: 'Add null checks before property access',
    reviewers: ['code-principles-reviewer'],
    files: ['Services/AuthService.cs'],
    quickWin: true
  }
]

// NOTE: All 3 issues linked as they are in same file, nearby lines, and same category
```

---

### 3.3. Theme Summary Builder

**Goal**: Aggregate recommendations by theme to identify patterns and strategic improvements.

**Top Themes**: Show top 5 themes by occurrence count

**Quick Win Identification**: Flag low-effort items with high impact

**Reviewer Agreement**: Track how many reviewers reported each theme (X/Y format)

#### ThemeSummary Interface

```typescript
/**
 * Aggregated summary of recommendation theme
 *
 * Provides strategic view of patterns across all reviews.
 */
interface ThemeSummary {
  /** Theme category */
  theme: string;

  /** Number of unique reviewers who reported this theme */
  reportedBy: number;

  /** Total number of reviewers in consolidation */
  totalReviewers: number;

  /** Total occurrences of this theme */
  occurrences: number;

  /** Number of unique files affected */
  filesAffected: number;

  /** Consolidated recommendation text */
  recommendation: string;

  /** Is this a quick win? (low effort, high impact) */
  quickWin: boolean;

  /** Effort estimate for addressing this theme */
  effort: string;

  /** Example issues (up to 3) */
  examples: string[];
}
```

#### buildThemeSummary() Function

```typescript
/**
 * Build theme summary from recommendations
 *
 * ALGORITHM:
 * 1. Group recommendations by theme
 * 2. Count occurrences and unique reviewers per theme
 * 3. Calculate files affected
 * 4. Identify quick wins (low effort themes)
 * 5. Sort by occurrences (most common first)
 * 6. Return top 5 themes
 *
 * @param recommendations - Extracted recommendations
 * @param consolidatedIssues - All issues for context
 * @param totalReviewers - Total number of reviewers
 * @returns Top 5 theme summaries
 */
function buildThemeSummary(
  recommendations: Recommendation[],
  consolidatedIssues: Issue[],
  totalReviewers: number
): ThemeSummary[] {
  const themes = new Map<string, ThemeSummary>();

  for (const rec of recommendations) {
    if (!themes.has(rec.theme)) {
      // Get files affected by this theme's issues
      const affectedFiles = countUniqueFiles(rec.relatedIssues, consolidatedIssues);

      // Get example issue messages
      const examples = getExampleIssues(rec.relatedIssues, consolidatedIssues, 3);

      // Create theme summary
      themes.set(rec.theme, {
        theme: rec.theme,
        reportedBy: rec.reviewers.length,
        totalReviewers,
        occurrences: rec.relatedIssues.length,
        filesAffected: affectedFiles.length,
        recommendation: rec.description,
        quickWin: rec.effort === 'low',
        effort: formatEffort(rec.effort),
        examples
      });
    } else {
      // Update existing theme summary
      const existing = themes.get(rec.theme)!;
      existing.occurrences += rec.relatedIssues.length;

      // Update unique reviewers count
      const allReviewers = new Set([...existing.examples, ...rec.reviewers]);
      existing.reportedBy = allReviewers.size;

      // Recalculate files affected
      const allIssueIds = [
        ...existing.examples,
        ...rec.relatedIssues
      ];
      const files = countUniqueFiles(allIssueIds, consolidatedIssues);
      existing.filesAffected = files.length;
    }
  }

  // Sort by occurrences (most common first) and return top 5
  return Array.from(themes.values())
    .sort((a, b) => b.occurrences - a.occurrences)
    .slice(0, 5);
}
```

#### Helper Functions

```typescript
/**
 * Count unique files affected by issues
 *
 * @param issueIds - Issue IDs to check
 * @param allIssues - All consolidated issues
 * @returns Array of unique file paths
 */
function countUniqueFiles(issueIds: string[], allIssues: Issue[]): string[] {
  const files = new Set<string>();

  for (const id of issueIds) {
    const issue = allIssues.find(i => i.id === id);
    if (issue) {
      files.add(issue.file);
    }
  }

  return Array.from(files);
}

/**
 * Format effort estimate for display
 *
 * @param effort - Effort level
 * @returns Human-readable effort estimate
 */
function formatEffort(effort: 'low' | 'medium' | 'high'): string {
  const effortMap: Record<string, string> = {
    'low': '1-2 hours',
    'medium': '3-4 hours',
    'high': '5-8 hours'
  };

  return effortMap[effort] || '3-4 hours';
}

/**
 * Get example issue messages
 *
 * @param issueIds - Issue IDs to sample
 * @param allIssues - All consolidated issues
 * @param limit - Maximum number of examples
 * @returns Array of issue messages
 */
function getExampleIssues(
  issueIds: string[],
  allIssues: Issue[],
  limit: number
): string[] {
  const examples: string[] = [];

  for (const id of issueIds.slice(0, limit)) {
    const issue = allIssues.find(i => i.id === id);
    if (issue) {
      examples.push(`${issue.message} (${issue.file}:${issue.line})`);
    }
  }

  return examples;
}
```

#### Theme Summary Examples

**Example 1: Single Theme Dominance**

```typescript
// INPUT: Many recommendations in one theme
const recommendations: Recommendation[] = [
  {
    theme: 'testing',
    description: 'Add unit tests for business logic',
    frequency: 3,
    confidence: 0.88,
    relatedIssues: ['issue-1', 'issue-2', 'issue-5', 'issue-7', 'issue-12'],
    effort: 'medium',
    reviewers: ['test-healer', 'code-principles-reviewer', 'code-style-reviewer']
  },
  {
    theme: 'refactoring',
    description: 'Extract duplicate code',
    frequency: 2,
    confidence: 0.75,
    relatedIssues: ['issue-3', 'issue-8'],
    effort: 'medium',
    reviewers: ['code-principles-reviewer', 'code-style-reviewer']
  }
];

// PROCESS: Build theme summary
const summary = buildThemeSummary(recommendations, allIssues, 3);

// OUTPUT: Top theme highlighted
[
  {
    theme: 'testing',
    reportedBy: 3,
    totalReviewers: 3,
    occurrences: 5,
    filesAffected: 4,
    recommendation: 'Add unit tests for business logic',
    quickWin: false,
    effort: '3-4 hours',
    examples: [
      'Missing unit tests for AuthService (Services/AuthService.cs:42)',
      'No tests for UserService methods (Services/UserService.cs:105)',
      'Test coverage gap in OrderProcessor (Services/OrderProcessor.cs:28)'
    ]
  },
  {
    theme: 'refactoring',
    reportedBy: 2,
    totalReviewers: 3,
    occurrences: 2,
    filesAffected: 2,
    recommendation: 'Extract duplicate code',
    quickWin: false,
    effort: '3-4 hours',
    examples: [
      'Duplicate validation in UserController (Controllers/UserController.cs:42)',
      'Repeated code in AuthController (Controllers/AuthController.cs:28)'
    ]
  }
]
```

**Example 2: Multiple Themes with Quick Wins**

```typescript
// INPUT: Mixed themes with varying effort
const recommendations: Recommendation[] = [
  {
    theme: 'code-style',
    frequency: 4,
    relatedIssues: ['issue-1', 'issue-2', 'issue-3', 'issue-4'],
    effort: 'low',  // Quick win
    reviewers: ['code-style-reviewer']
  },
  {
    theme: 'security',
    frequency: 2,
    relatedIssues: ['issue-5', 'issue-6'],
    effort: 'high',
    reviewers: ['code-principles-reviewer', 'security-scanner']
  },
  {
    theme: 'documentation',
    frequency: 3,
    relatedIssues: ['issue-7', 'issue-8', 'issue-9'],
    effort: 'low',  // Quick win
    reviewers: ['code-style-reviewer']
  }
];

// OUTPUT: Quick wins flagged
[
  {
    theme: 'code-style',
    reportedBy: 1,
    totalReviewers: 2,
    occurrences: 4,
    filesAffected: 3,
    recommendation: 'Add braces to all block statements',
    quickWin: true,  // Low effort, multiple occurrences
    effort: '1-2 hours',
    examples: [...]
  },
  {
    theme: 'documentation',
    reportedBy: 1,
    totalReviewers: 2,
    occurrences: 3,
    filesAffected: 3,
    recommendation: 'Add XML documentation to public APIs',
    quickWin: true,  // Low effort
    effort: '1-2 hours',
    examples: [...]
  },
  {
    theme: 'security',
    reportedBy: 2,
    totalReviewers: 2,
    occurrences: 2,
    filesAffected: 2,
    recommendation: 'Add input validation to prevent injection',
    quickWin: false,  // High effort
    effort: '5-8 hours',
    examples: [...]
  }
]
```

**Example 3: Reviewer Agreement Analysis**

```typescript
// INPUT: Varying reviewer agreement
const recommendations: Recommendation[] = [
  {
    theme: 'error-handling',
    frequency: 3,  // All 3 reviewers agreed
    relatedIssues: ['issue-1', 'issue-2', 'issue-3'],
    reviewers: ['code-principles-reviewer', 'test-healer', 'code-style-reviewer']
  },
  {
    theme: 'performance',
    frequency: 1,  // Only 1 reviewer mentioned
    relatedIssues: ['issue-4'],
    reviewers: ['code-principles-reviewer']
  }
];

// OUTPUT: Agreement tracked
[
  {
    theme: 'error-handling',
    reportedBy: 3,  // All reviewers
    totalReviewers: 3,
    occurrences: 3,
    filesAffected: 2,
    recommendation: 'Add comprehensive error handling',
    quickWin: false,
    effort: '3-4 hours',
    examples: [...]
    // NOTE: 3/3 reviewers = high confidence theme
  },
  {
    theme: 'performance',
    reportedBy: 1,  // Only one reviewer
    totalReviewers: 3,
    occurrences: 1,
    filesAffected: 1,
    recommendation: 'Optimize database queries',
    quickWin: false,
    effort: '5-8 hours',
    examples: [...]
    // NOTE: 1/3 reviewers = lower priority theme
  }
]
```

---

## Recommendation Synthesis Integration

### Complete Workflow

```typescript
/**
 * Full recommendation synthesis workflow
 *
 * INPUT: Consolidated issues (from deduplication + priority aggregation)
 * OUTPUT: Recommendations, action items, and theme summaries
 *
 * STAGES:
 * 1. Extract recommendations from high-confidence issues (‚â•60%)
 * 2. Generate prioritized action items
 * 3. Build top 5 theme summaries
 * 4. Identify quick wins
 */
function synthesizeRecommendations(
  reviewResults: ReviewResult[],
  consolidatedIssues: Issue[]
): RecommendationSynthesisResult {

  // STAGE 1: Extract recommendations (3.3A)
  const recommendations = extractRecommendations(reviewResults);

  // STAGE 2: Generate action items (3.3B)
  const actionItems = generateActionItems(consolidatedIssues);

  // STAGE 3: Build theme summaries (3.3C)
  const totalReviewers = new Set(reviewResults.map(r => r.reviewer)).size;
  const themeSummaries = buildThemeSummary(
    recommendations,
    consolidatedIssues,
    totalReviewers
  );

  // STAGE 4: Identify quick wins
  const quickWins = actionItems.filter(item => item.quickWin);

  return {
    recommendations,
    actionItems,
    themeSummaries,
    quickWins,
    statistics: {
      totalRecommendations: recommendations.length,
      totalActionItems: actionItems.length,
      topThemes: themeSummaries.length,
      quickWinCount: quickWins.length,
      highConfidenceIssues: consolidatedIssues.filter(i => i.confidence >= 0.60).length
    }
  };
}

interface RecommendationSynthesisResult {
  recommendations: Recommendation[];
  actionItems: ActionItem[];
  themeSummaries: ThemeSummary[];
  quickWins: ActionItem[];
  statistics: {
    totalRecommendations: number;
    totalActionItems: number;
    topThemes: number;
    quickWinCount: number;
    highConfidenceIssues: number;
  };
}
```

### Integration Example

```typescript
// CONTEXT: Full pipeline from raw reviews to recommendations

// INPUT: Raw review results from parallel execution
const reviewResults: ReviewResult[] = [
  {
    reviewer: 'code-principles-reviewer',
    timestamp: '2025-10-16T10:00:00Z',
    issues: [
      {
        id: 'issue-1',
        message: 'Missing null check',
        file: 'Services/AuthService.cs',
        line: 42,
        severity: 'P0',
        confidence: 0.88,
        category: 'error-handling',
        suggestion: 'Add null check before property access'
      },
      {
        id: 'issue-2',
        message: 'No unit tests for AuthService',
        file: 'Services/AuthService.cs',
        line: 1,
        severity: 'P1',
        confidence: 0.85,
        category: 'testing',
        suggestion: 'Add comprehensive unit test coverage'
      }
    ]
  },
  {
    reviewer: 'test-healer',
    timestamp: '2025-10-16T10:01:00Z',
    issues: [
      {
        id: 'issue-3',
        message: 'Test coverage for AuthService is 42%',
        file: 'Services/AuthService.cs',
        line: 1,
        severity: 'P1',
        confidence: 0.90,
        category: 'testing',
        suggestion: 'Increase test coverage to 80%'
      }
    ]
  },
  {
    reviewer: 'code-style-reviewer',
    timestamp: '2025-10-16T10:02:00Z',
    issues: [
      {
        id: 'issue-4',
        message: 'Missing braces in if statement',
        file: 'Controllers/UserController.cs',
        line: 28,
        severity: 'P2',
        confidence: 0.75,
        category: 'code-style',
        suggestion: 'Add braces to all block statements'
      }
    ]
  }
];

// STEP 1: Deduplication (Task 3.1)
const deduplicatedIssues = deduplicateIssues(reviewResults);
// Result: issue-2 and issue-3 merged (same file, similar message)

// STEP 2: Priority Aggregation (Task 3.2)
const consolidatedIssues = processPriorityAggregation(deduplicatedIssues);
// Result: Priorities aggregated, confidence weighted

// STEP 3: Recommendation Synthesis (Task 3.3)
const synthesis = synthesizeRecommendations(reviewResults, consolidatedIssues);

// OUTPUT:
{
  recommendations: [
    {
      theme: 'testing',
      description: 'Add comprehensive unit test coverage',
      frequency: 2,
      confidence: 0.875,
      relatedIssues: ['issue-2-3-merged'],  // Deduplicated issue
      effort: 'medium',
      reviewers: ['code-principles-reviewer', 'test-healer'],
      matchedKeywords: ['test', 'coverage']
    },
    {
      theme: 'refactoring',
      description: 'Add null check before property access',
      frequency: 1,
      confidence: 0.88,
      relatedIssues: ['issue-1'],
      effort: 'low',
      reviewers: ['code-principles-reviewer'],
      matchedKeywords: []
    },
    {
      theme: 'refactoring',
      description: 'Add braces to all block statements',
      frequency: 1,
      confidence: 0.75,
      relatedIssues: ['issue-4'],
      effort: 'low',
      reviewers: ['code-style-reviewer'],
      matchedKeywords: []
    }
  ],

  actionItems: [
    {
      priority: 'P0',
      title: 'Missing null check (Services/AuthService.cs:42)',
      estimatedEffort: '1-2h',
      relatedIssues: ['issue-1'],
      recommendation: 'Add null check before property access',
      reviewers: ['code-principles-reviewer'],
      files: ['Services/AuthService.cs'],
      quickWin: true
    },
    {
      priority: 'P1',
      title: 'Test coverage for AuthService is 42% (Services/AuthService.cs:1)',
      estimatedEffort: '3-4h',
      relatedIssues: ['issue-2-3-merged'],
      recommendation: 'Add comprehensive unit test coverage',
      reviewers: ['code-principles-reviewer', 'test-healer'],
      files: ['Services/AuthService.cs'],
      quickWin: false
    },
    {
      priority: 'P2',
      title: 'Missing braces in if statement (Controllers/UserController.cs:28)',
      estimatedEffort: '1-2h',
      relatedIssues: ['issue-4'],
      recommendation: 'Add braces to all block statements',
      reviewers: ['code-style-reviewer'],
      files: ['Controllers/UserController.cs'],
      quickWin: false
    }
  ],

  themeSummaries: [
    {
      theme: 'testing',
      reportedBy: 2,
      totalReviewers: 3,
      occurrences: 1,  // One merged issue
      filesAffected: 1,
      recommendation: 'Add comprehensive unit test coverage',
      quickWin: false,
      effort: '3-4 hours',
      examples: [
        'Test coverage for AuthService is 42% (Services/AuthService.cs:1)'
      ]
    },
    {
      theme: 'refactoring',
      reportedBy: 2,
      totalReviewers: 3,
      occurrences: 2,
      filesAffected: 2,
      recommendation: 'Add null check before property access',
      quickWin: true,
      effort: '1-2 hours',
      examples: [
        'Missing null check (Services/AuthService.cs:42)',
        'Missing braces in if statement (Controllers/UserController.cs:28)'
      ]
    }
  ],

  quickWins: [
    {
      priority: 'P0',
      title: 'Missing null check (Services/AuthService.cs:42)',
      estimatedEffort: '1-2h',
      relatedIssues: ['issue-1'],
      recommendation: 'Add null check before property access',
      reviewers: ['code-principles-reviewer'],
      files: ['Services/AuthService.cs'],
      quickWin: true
    }
  ],

  statistics: {
    totalRecommendations: 3,
    totalActionItems: 3,
    topThemes: 2,
    quickWinCount: 1,
    highConfidenceIssues: 3
  }
}
```

---

## Recommendation Synthesis Summary

**Key Characteristics**:

1. **Confidence Filtering**: Only extract from issues with ‚â•60% confidence
2. **Theme Categorization**: 6 themes (refactoring, testing, documentation, performance, security, general)
3. **Prioritization**: Sort by priority (P0 > P1 > P2) then effort (low > medium > high)
4. **Quick Win Detection**: Flag low-effort, high-priority items
5. **Top 5 Themes**: Strategic view of most common patterns
6. **Reviewer Agreement**: Track consensus across reviewers (X/Y format)

**Integration Points**:
- Input: Deduplicated issues (Task 3.1) + Priority-aggregated issues (Task 3.2)
- Output: Recommendations, action items, theme summaries, quick wins
- Format: Markdown-friendly for report generation

**Quality Metrics**:
- Categorization accuracy: >90% (keyword matching)
- Deduplication rate: Similar recommendations merged
- Action item coverage: 100% of consolidated issues
- Theme coverage: Top 5 patterns identified

---

## Report Formatting

### Overview

The Report Formatting system transforms consolidated issues and metadata into professional markdown documentation. It handles section generation, code context extraction, emoji indicators, table creation, and human-readable output formatting.

**Purpose**:
- Generate valid GitHub Flavored Markdown reports
- Format issues with visual priority indicators
- Include code context for developer clarity
- Create structured tables for metadata display
- Ensure consistent formatting throughout reports

**Integration**: Phase 4 (Report Generation & Output) - Task 4.1B

---

### Core Data Structures

```typescript
// Report section containing issues and optional summary
interface ReportSection {
  title: string;
  priority: 'P0' | 'P1' | 'P2';
  issues: ConsolidatedIssue[];
  summary?: string;
  emoji: string; // 'üî¥', 'üü°', 'üü¢'
}

// Code snippet with context lines
interface CodeContext {
  file: string;
  line: number;
  language: string; // 'csharp', 'typescript', 'javascript', etc.
  beforeLines: string[]; // 5 lines before
  targetLine: string; // The problematic line
  afterLines: string[]; // 5 lines after
  highlightStyle: 'markers' | 'comment'; // >>> markers or // comment
}

// Report header information
interface ReportHeader {
  reviewContext: string;
  reviewDate: string; // ISO 8601
  status: 'GREEN' | 'YELLOW' | 'RED';
  overallConfidence: number; // 0-1
}

// Table of contents entry
interface TOCEntry {
  title: string;
  anchor: string; // URL-safe anchor (#section-name)
  level: number; // 1, 2, or 3
}
```

---

### Master Report Generation

```typescript
/**
 * Generates complete consolidated review report in markdown format
 *
 * Algorithm:
 * 1. Generate report header with metadata
 * 2. Generate executive summary from statistics
 * 3. Conditionally generate TOC (if >50 issues)
 * 4. Format each priority section (P0, P1, P2)
 * 5. Format common themes section
 * 6. Format prioritized action items table
 * 7. Generate metadata footer
 * 8. Return complete markdown string
 *
 * @param sections - Report sections organized by priority
 * @param metadata - Complete execution and quality metadata
 * @returns Complete markdown report string
 */
function formatReport(
  sections: ReportSection[],
  metadata: ReportMetadata
): string {
  let markdown = '';

  // Step 1: Generate header
  markdown += generateHeader(metadata);
  markdown += '\n---\n\n';

  // Step 2: Generate TOC if report is large
  const totalIssues = getTotalIssues(sections);
  if (totalIssues > 50) {
    markdown += generateTableOfContents(sections);
    markdown += '\n---\n\n';
  }

  // Step 3: Generate executive summary
  markdown += generateExecutiveSummary(sections, metadata);
  markdown += '\n---\n\n';

  // Step 4: Format each section
  for (const section of sections) {
    if (section.issues.length > 0) {
      markdown += formatSection(section);
      markdown += '\n---\n\n';
    }
  }

  // Step 5: Format common themes (from recommendation synthesis)
  if (metadata.recommendations && metadata.recommendations.themes.length > 0) {
    markdown += formatCommonThemes(metadata.recommendations.themes);
    markdown += '\n---\n\n';
  }

  // Step 6: Format prioritized action items
  if (metadata.recommendations && metadata.recommendations.actionItems.length > 0) {
    markdown += formatPrioritizedActionItems(metadata.recommendations.actionItems);
    markdown += '\n---\n\n';
  }

  // Step 7: Generate metadata footer
  markdown += generateMetadataFooter(metadata);

  return markdown;
}

/**
 * Gets total issue count across all sections
 */
function getTotalIssues(sections: ReportSection[]): number {
  return sections.reduce((sum, section) => sum + section.issues.length, 0);
}
```

---

### Header Generation

```typescript
/**
 * Generates report header with context, date, status, and confidence
 *
 * Status determination:
 * - RED: Any P0 critical issues present
 * - YELLOW: P1 warnings present, no P0 issues
 * - GREEN: Only P2 improvements, no P0/P1 issues
 *
 * @param metadata - Report metadata with statistics
 * @returns Formatted markdown header
 */
function generateHeader(metadata: ReportMetadata): string {
  const status = determineReportStatus(metadata);
  const confidence = (metadata.overallConfidence * 100).toFixed(0);

  return `# Consolidated Code Review Report

**Review Context**: ${metadata.reviewContext || 'Code Review'}
**Review Date**: ${metadata.timestamp.toISOString()}
**Status**: ${getStatusEmoji(status)} ${status}
**Overall Confidence**: ${confidence}%`;
}

/**
 * Determines report status based on issue priorities
 */
function determineReportStatus(metadata: ReportMetadata): 'GREEN' | 'YELLOW' | 'RED' {
  if (metadata.criticalIssues > 0) {
    return 'RED';
  }
  if (metadata.warnings > 0) {
    return 'YELLOW';
  }
  return 'GREEN';
}

/**
 * Gets status emoji indicator
 */
function getStatusEmoji(status: 'GREEN' | 'YELLOW' | 'RED'): string {
  const emojiMap = {
    'RED': 'üî¥',
    'YELLOW': 'üü°',
    'GREEN': 'üü¢'
  };
  return emojiMap[status];
}
```

---

### Table of Contents Generation

```typescript
/**
 * Generates table of contents for large reports (>50 issues)
 *
 * Includes:
 * 1. Executive Summary
 * 2. Critical Issues (P0) - if present
 * 3. Warnings (P1) - if present
 * 4. Improvements (P2) - if present
 * 5. Common Themes
 * 6. Prioritized Action Items
 * 7. Review Metadata
 *
 * @param sections - Report sections to include in TOC
 * @returns Formatted markdown TOC
 */
function generateTableOfContents(sections: ReportSection[]): string {
  let toc = '## Table of Contents\n\n';
  let index = 1;

  // Always include Executive Summary
  toc += `${index++}. [Executive Summary](#executive-summary)\n`;

  // Add sections with issues
  for (const section of sections) {
    if (section.issues.length > 0) {
      const anchor = generateAnchor(section.title);
      toc += `${index++}. [${section.title}](#${anchor})\n`;
    }
  }

  // Always include Common Themes and Action Items
  toc += `${index++}. [Common Themes](#common-themes-across-reviewers)\n`;
  toc += `${index++}. [Prioritized Action Items](#prioritized-action-items)\n`;
  toc += `${index++}. [Review Metadata](#review-metadata)\n`;

  return toc;
}

/**
 * Generates URL-safe anchor from section title
 * Example: "Critical Issues (P0)" -> "critical-issues-p0---immediate-action-required"
 */
function generateAnchor(title: string): string {
  return title
    .toLowerCase()
    .replace(/[^\w\s-]/g, '') // Remove special chars except dash
    .replace(/\s+/g, '-')     // Replace spaces with dashes
    .replace(/--+/g, '-');    // Collapse multiple dashes
}
```

---

### Executive Summary Generation

```typescript
/**
 * Generates executive summary with scope, findings, assessment, and next steps
 *
 * Auto-generates 1-2 paragraph assessment based on:
 * - Issue counts and priorities
 * - Top 3 themes from recommendations
 * - Overall confidence and coverage
 *
 * @param sections - Report sections for statistics
 * @param metadata - Complete execution metadata
 * @returns Formatted markdown executive summary
 */
function generateExecutiveSummary(
  sections: ReportSection[],
  metadata: ReportMetadata
): string {
  const totalIssues = getTotalIssues(sections);
  const deduplicationRatio = (metadata.deduplicationRatio * 100).toFixed(0);

  let summary = '## Executive Summary\n\n';

  // Scope section
  summary += '**Scope**:\n';
  summary += `- **Files Reviewed**: ${metadata.filesReviewed}\n`;
  summary += `- **Lines of Code**: ${metadata.linesOfCode.toLocaleString()}\n`;
  summary += `- **Reviewers**: ${metadata.reviewers.map(r => r.name).join(', ')}\n`;
  summary += `- **Total Review Time**: ${formatDuration(metadata.reviewDuration)}\n\n`;

  // Findings section
  summary += '**Findings**:\n';
  summary += `- **Total Issues Found**: ${metadata.issuesBeforeConsolidation}\n`;
  summary += `- **After Deduplication**: ${metadata.issuesAfterConsolidation} (${deduplicationRatio}% reduction)\n`;
  summary += `- **Critical Issues (P0)**: ${metadata.criticalIssues}${metadata.criticalIssues > 0 ? ' - require immediate action' : ''}\n`;
  summary += `- **Warnings (P1)**: ${metadata.warnings}${metadata.warnings > 0 ? ' - recommended fixes' : ''}\n`;
  summary += `- **Improvements (P2)**: ${metadata.improvements}${metadata.improvements > 0 ? ' - optional enhancements' : ''}\n\n`;

  // Overall assessment (auto-generated)
  summary += '**Overall Assessment**:\n';
  summary += generateAssessmentParagraph(metadata) + '\n\n';

  // Key themes (top 3)
  if (metadata.recommendations && metadata.recommendations.themes.length > 0) {
    summary += '**Key Themes**:\n';
    const topThemes = metadata.recommendations.themes.slice(0, 3);
    topThemes.forEach((theme, index) => {
      summary += `${index + 1}. ${theme.theme} - ${theme.occurrences} occurrences across ${theme.filesAffected} files\n`;
    });
    summary += '\n';
  }

  // Recommended next steps
  summary += '**Recommended Next Steps**:\n';
  summary += generateNextSteps(metadata);

  return summary;
}

/**
 * Auto-generates assessment paragraph based on metrics
 */
function generateAssessmentParagraph(metadata: ReportMetadata): string {
  const status = determineReportStatus(metadata);
  const confidence = (metadata.overallConfidence * 100).toFixed(0);

  if (status === 'RED') {
    return `CRITICAL: Found ${metadata.criticalIssues} critical issues that must be addressed before deployment. ${
      metadata.warnings > 0 ? `Additionally, ${metadata.warnings} warnings require attention. ` : ''
    }Overall confidence ${confidence}%. Immediate action required.`;
  } else if (status === 'YELLOW') {
    return `Code quality is generally good but ${metadata.warnings} warnings identified that should be addressed soon. ${
      metadata.improvements > 0 ? `${metadata.improvements} optional improvements also suggested. ` : ''
    }Overall confidence ${confidence}%.`;
  } else {
    return `Code quality is high with no critical issues or warnings. ${
      metadata.improvements > 0 ? `${metadata.improvements} optional style improvements suggested. ` : ''
    }Overall confidence ${confidence}%. No immediate action required.`;
  }
}

/**
 * Generates recommended next steps based on priority breakdown
 */
function generateNextSteps(metadata: ReportMetadata): string {
  let steps = '';

  if (metadata.criticalIssues > 0) {
    steps += `- **Immediate**: Fix ${metadata.criticalIssues} P0 critical issues before deployment\n`;
  } else {
    steps += '- **Immediate**: None required (no critical issues)\n';
  }

  if (metadata.warnings > 0) {
    steps += `- **Short-term**: Address ${metadata.warnings} P1 warnings to maintain code quality\n`;
  } else {
    steps += '- **Short-term**: Apply optional P2 improvements if desired\n';
  }

  if (metadata.improvements > 0) {
    steps += `- **Long-term**: Consider ${metadata.improvements} P2 improvements for code polish\n`;
  }

  return steps;
}
```

---

### Section Formatting

```typescript
/**
 * Formats a single priority section (P0, P1, or P2)
 *
 * Groups issues by file/component for readability
 * Includes section header with emoji and description
 *
 * @param section - Section with title, issues, and metadata
 * @returns Formatted markdown section
 */
function formatSection(section: ReportSection): string {
  let markdown = `## ${section.title}\n\n`;

  // Add section description based on priority
  markdown += getSectionDescription(section.priority) + '\n\n';

  // Group issues by file
  const byFile = groupByFile(section.issues);

  // Format each file group
  for (const [file, issues] of byFile) {
    markdown += `### ${file}\n\n`;

    for (const issue of issues) {
      markdown += formatIssue(issue);
      markdown += '\n';
    }
  }

  return markdown;
}

/**
 * Gets section description based on priority
 */
function getSectionDescription(priority: 'P0' | 'P1' | 'P2'): string {
  const descriptions = {
    'P0': 'üî¥ **Issues that must be fixed before deployment or further development**',
    'P1': 'üü° **Issues that should be addressed soon to maintain code quality**',
    'P2': 'üü¢ **Suggestions for code quality improvements and best practices**'
  };
  return descriptions[priority];
}

/**
 * Groups issues by file path for organized display
 *
 * @param issues - List of consolidated issues
 * @returns Map of file path to issues array
 */
function groupByFile(issues: ConsolidatedIssue[]): Map<string, ConsolidatedIssue[]> {
  const grouped = new Map<string, ConsolidatedIssue[]>();

  for (const issue of issues) {
    const file = issue.file || 'Unknown File';
    if (!grouped.has(file)) {
      grouped.set(file, []);
    }
    grouped.get(file)!.push(issue);
  }

  // Sort issues within each file by line number
  for (const [file, fileIssues] of grouped) {
    fileIssues.sort((a, b) => (a.line || 0) - (b.line || 0));
  }

  return grouped;
}
```

---

### Issue Formatting

```typescript
/**
 * Formats individual issue with all details
 *
 * Includes:
 * - Emoji + title + location
 * - Description
 * - Impact/Rationale (for P0/P1)
 * - Action/Recommendation
 * - Reviewers with agreement %
 * - Confidence indicator
 * - Code context (required for P0, optional for P1/P2)
 * - Suggested fix (if available)
 *
 * @param issue - Consolidated issue to format
 * @returns Formatted markdown for single issue
 */
function formatIssue(issue: ConsolidatedIssue): string {
  const emoji = getPriorityEmoji(issue.severity);
  const confidence = getConfidenceIndicator(issue.confidence);
  const agreementPct = (issue.agreement * 100).toFixed(0);

  let markdown = `#### ${emoji} ${issue.message} (Line ${issue.line})\n\n`;

  // Description
  markdown += `**Description**: ${issue.description || issue.message}\n\n`;

  // Impact (P0) or Rationale (P1)
  if (issue.severity === 'P0' && issue.impact) {
    markdown += `**Impact**: ${issue.impact}\n\n`;
  } else if (issue.severity === 'P1' && issue.rationale) {
    markdown += `**Rationale**: ${issue.rationale}\n\n`;
  }

  // Action (P0) or Recommendation (P1/P2)
  if (issue.severity === 'P0' && issue.actionRequired) {
    markdown += `**Action Required**: ${issue.actionRequired}\n\n`;
  } else if (issue.suggestion) {
    markdown += `**Recommendation**: ${issue.suggestion}\n\n`;
  }

  // Reviewers with agreement
  markdown += `**Reviewers**: ${issue.reviewers.join(', ')}`;
  if (issue.reviewers.length > 1) {
    markdown += ` (${agreementPct}% agreement)`;
  }
  markdown += '\n\n';

  // Confidence indicator
  markdown += `**Confidence**: ${confidence}\n\n`;

  // Code context (ALWAYS for P0, optional for others)
  if (issue.severity === 'P0' || issue.codeSnippet) {
    markdown += formatCodeContext(issue);
    markdown += '\n';
  }

  // Suggested fix (if available)
  if (issue.suggestedFix) {
    markdown += formatSuggestedFix(issue);
    markdown += '\n';
  }

  markdown += '---\n';

  return markdown;
}

/**
 * Gets priority emoji indicator
 */
function getPriorityEmoji(severity: string): string {
  const emojiMap: Record<string, string> = {
    'P0': 'üî¥',
    'P1': 'üü°',
    'P2': 'üü¢',
    'critical': 'üî¥',
    'warning': 'üü°',
    'improvement': 'üü¢'
  };
  return emojiMap[severity] || 'üîµ';
}

/**
 * Gets confidence indicator with emoji and percentage
 */
function getConfidenceIndicator(confidence: number): string {
  const percentage = (confidence * 100).toFixed(0);

  if (confidence >= 0.8) {
    return `üü¢ High (${percentage}%)`;
  } else if (confidence >= 0.6) {
    return `üü° Medium (${percentage}%)`;
  } else {
    return `üî¥ Low (${percentage}%)`;
  }
}
```

---

### Code Context Formatting

```typescript
/**
 * Formats code snippet with context (5 lines before/after)
 *
 * Highlights problematic line with >>> markers
 * Includes file:line reference in code block comment
 * Auto-detects language for syntax highlighting
 *
 * @param issue - Issue with code snippet
 * @returns Formatted markdown code block
 */
function formatCodeContext(issue: ConsolidatedIssue): string {
  if (!issue.codeSnippet) {
    return '';
  }

  const language = detectLanguage(issue.file);
  let markdown = '**Code Context**:\n';
  markdown += '```' + language + '\n';
  markdown += `// ${issue.file}:${issue.line}\n`;

  // Split snippet into lines
  const lines = issue.codeSnippet.split('\n');

  // If snippet already has >>> markers, use as-is
  if (issue.codeSnippet.includes('>>>')) {
    markdown += issue.codeSnippet;
  } else {
    // Otherwise, find and highlight the problematic line
    // Assume middle line is the target (5 before, target, 5 after)
    const targetIndex = Math.floor(lines.length / 2);

    lines.forEach((line, index) => {
      if (index === targetIndex) {
        markdown += `>>>     ${line}\n`;
      } else {
        markdown += line + '\n';
      }
    });
  }

  markdown += '```\n';

  return markdown;
}

/**
 * Detects programming language from file extension
 */
function detectLanguage(file: string): string {
  const extension = file.split('.').pop()?.toLowerCase() || '';

  const languageMap: Record<string, string> = {
    'cs': 'csharp',
    'ts': 'typescript',
    'js': 'javascript',
    'tsx': 'typescript',
    'jsx': 'javascript',
    'py': 'python',
    'java': 'java',
    'cpp': 'cpp',
    'c': 'c',
    'go': 'go',
    'rs': 'rust',
    'rb': 'ruby',
    'php': 'php',
    'swift': 'swift',
    'kt': 'kotlin',
    'sql': 'sql',
    'md': 'markdown',
    'json': 'json',
    'xml': 'xml',
    'yaml': 'yaml',
    'yml': 'yaml'
  };

  return languageMap[extension] || 'text';
}

/**
 * Formats suggested fix code block
 */
function formatSuggestedFix(issue: ConsolidatedIssue): string {
  if (!issue.suggestedFix) {
    return '';
  }

  const language = detectLanguage(issue.file);
  let markdown = '**Suggested Fix**:\n';
  markdown += '```' + language + '\n';
  markdown += issue.suggestedFix;
  if (!issue.suggestedFix.endsWith('\n')) {
    markdown += '\n';
  }
  markdown += '```\n';

  return markdown;
}
```

---

### Common Themes Formatting

```typescript
/**
 * Formats common themes section from recommendation synthesis
 *
 * Shows top 5-10 recurring patterns with:
 * - Occurrence counts
 * - Cross-reviewer agreement
 * - Files affected
 * - Recommended actions
 * - Quick win identification
 * - Effort estimates
 *
 * @param themes - Theme summaries from recommendation synthesis
 * @returns Formatted markdown themes section
 */
function formatCommonThemes(themes: ThemeSummary[]): string {
  let markdown = '## Common Themes Across Reviewers\n\n';
  markdown += '**Top recurring patterns identified by multiple reviewers**:\n\n';

  // Show top 5-10 themes
  const topThemes = themes.slice(0, Math.min(10, themes.length));

  topThemes.forEach((theme, index) => {
    markdown += `### ${index + 1}. ${capitalizeTheme(theme.theme)} (${theme.occurrences} occurrences)\n\n`;

    // Reviewer agreement
    markdown += `**Reported by**: ${theme.reportedBy}/${theme.totalReviewers} reviewers\n\n`;

    // Files affected
    const fileList = theme.examples.map(ex => extractFile(ex)).join(', ');
    markdown += `**Files Affected**: ${fileList}\n\n`;

    // Description
    markdown += `**Description**: ${theme.recommendation}\n\n`;

    // Recommended action
    markdown += `**Recommended Action**: ${theme.recommendation}\n\n`;

    // Quick wins
    markdown += `**Quick Wins Available**: ${theme.quickWin ? 'Yes' : 'No'}\n\n`;

    // Effort estimate
    markdown += `**Estimated Total Effort**: ${theme.effort}\n\n`;

    // Related issues
    const issueRefs = theme.examples.map((ex, i) => `#${i + 1}`).join(', ');
    markdown += `**Related Issues**: ${issueRefs}\n\n`;
  });

  return markdown;
}

/**
 * Capitalizes theme name for display
 */
function capitalizeTheme(theme: string): string {
  return theme
    .split('-')
    .map(word => word.charAt(0).toUpperCase() + word.slice(1))
    .join(' ');
}

/**
 * Extracts file path from issue example
 */
function extractFile(example: string): string {
  const match = example.match(/\(([^:)]+)/);
  return match ? match[1] : 'Unknown';
}
```

---

### Prioritized Action Items Formatting

```typescript
/**
 * Formats prioritized action items table
 *
 * Sorted by priority (P0 > P1 > P2) then effort (low > high)
 * Includes:
 * - Priority with emoji
 * - Issue title
 * - File:line location
 * - Effort estimate
 * - Related issues
 * - Quick win indicator
 *
 * @param actionItems - Sorted action items from recommendation synthesis
 * @returns Formatted markdown table with execution strategy
 */
function formatPrioritizedActionItems(actionItems: ActionItem[]): string {
  let markdown = '## Prioritized Action Items\n\n';
  markdown += '**Issues ordered by priority and estimated effort for optimal execution**:\n\n';

  // Create markdown table
  markdown += '| # | Priority | Issue | File:Line | Effort | Related Issues | Quick Win |\n';
  markdown += '|---|----------|-------|-----------|--------|----------------|-----------|\\n';

  actionItems.forEach((item, index) => {
    const emoji = getPriorityEmoji(item.priority);
    const quickWinEmoji = item.quickWin ? '‚úÖ Yes' : '‚ùå No';
    const issueRefs = item.relatedIssues.join(', ');
    const location = `${item.files[0]}:${extractLineNumber(item.title)}`;

    markdown += `| ${index + 1} | ${emoji} ${item.priority} | ${truncateTitle(item.title)} | ${location} | ${item.estimatedEffort} | ${issueRefs} | ${quickWinEmoji} |\n`;
  });

  markdown += '\n';

  // Add execution strategy
  markdown += formatExecutionStrategy(actionItems);

  return markdown;
}

/**
 * Truncates issue title to fit table cell (max 50 chars)
 */
function truncateTitle(title: string, maxLength: number = 50): string {
  if (title.length <= maxLength) {
    return title;
  }
  return title.substring(0, maxLength - 3) + '...';
}

/**
 * Extracts line number from issue title
 */
function extractLineNumber(title: string): number {
  const match = title.match(/Line (\d+)/);
  return match ? parseInt(match[1], 10) : 1;
}

/**
 * Formats execution strategy with phases
 */
function formatExecutionStrategy(actionItems: ActionItem[]): string {
  let strategy = '**Execution Strategy**:\n';

  const p0Items = actionItems.filter(item => item.priority === 'P0');
  const p1Items = actionItems.filter(item => item.priority === 'P1');
  const p2Items = actionItems.filter(item => item.priority === 'P2');

  if (p0Items.length > 0) {
    const quickWinP0 = p0Items.filter(item => item.quickWin);
    if (quickWinP0.length > 0) {
      strategy += `1. **Phase 1 (Immediate)**: Quick wins with P0 priority (${quickWinP0.length} items)\n`;
      strategy += `2. **Phase 2 (This Sprint)**: Remaining P0 critical issues (${p0Items.length - quickWinP0.length} items)\n`;
    } else {
      strategy += `1. **Phase 1 (Immediate)**: All P0 critical issues (${p0Items.length} items)\n`;
    }
  }

  if (p1Items.length > 0) {
    strategy += `3. **Phase 3 (This Sprint)**: High-priority P1 warnings (${p1Items.length} items)\n`;
  }

  if (p2Items.length > 0) {
    strategy += `4. **Phase 4 (Next Sprint)**: P2 improvements (${p2Items.length} items)\n`;
  }

  // Add critical note if P0 exists
  if (p0Items.length > 0) {
    strategy += `\n**CRITICAL NOTE**: ${p0Items.length > 5 ? 'Production deployment BLOCKED until P0 issues resolved' : 'Address P0 issues before deployment'}\n`;
  }

  return strategy;
}
```

---

### Helper Functions

```typescript
/**
 * Formats duration in human-readable format
 *
 * Examples:
 * - 1500ms -> "1.5s"
 * - 45000ms -> "45s"
 * - 125000ms -> "2m 5s"
 * - 3725000ms -> "1h 2m 5s"
 *
 * @param milliseconds - Duration in milliseconds
 * @returns Human-readable duration string
 */
function formatDuration(milliseconds: number): string {
  if (milliseconds < 1000) {
    return `${milliseconds}ms`;
  }

  const seconds = Math.floor(milliseconds / 1000);
  const minutes = Math.floor(seconds / 60);
  const hours = Math.floor(minutes / 60);

  if (hours > 0) {
    const remainingMinutes = minutes % 60;
    const remainingSeconds = seconds % 60;
    return `${hours}h ${remainingMinutes}m ${remainingSeconds}s`;
  } else if (minutes > 0) {
    const remainingSeconds = seconds % 60;
    return `${minutes}m ${remainingSeconds}s`;
  } else {
    return `${seconds}s`;
  }
}

/**
 * Formats number with thousands separator
 * Example: 12345 -> "12,345"
 */
function formatNumber(num: number): string {
  return num.toLocaleString('en-US');
}

/**
 * Validates markdown syntax
 * Checks for common issues:
 * - Unclosed code blocks
 * - Invalid table syntax
 * - Broken links
 *
 * @param markdown - Markdown string to validate
 * @returns Validation result with errors
 */
function validateMarkdown(markdown: string): { valid: boolean; errors: string[] } {
  const errors: string[] = [];

  // Check for unclosed code blocks
  const codeBlockCount = (markdown.match(/```/g) || []).length;
  if (codeBlockCount % 2 !== 0) {
    errors.push('Unclosed code block detected');
  }

  // Check for table consistency
  const tableRows = markdown.match(/\|[^\n]+\|/g) || [];
  for (let i = 0; i < tableRows.length; i++) {
    const cellCount = (tableRows[i].match(/\|/g) || []).length - 1;
    if (i > 0 && cellCount !== (tableRows[0].match(/\|/g) || []).length - 1) {
      errors.push(`Table row ${i + 1} has inconsistent cell count`);
    }
  }

  // Check for broken anchor links
  const anchorLinks = markdown.match(/\[([^\]]+)\]\(#([^)]+)\)/g) || [];
  const headers = markdown.match(/^#{1,6} (.+)$/gm) || [];
  const validAnchors = new Set(
    headers.map(h => generateAnchor(h.replace(/^#{1,6} /, '')))
  );

  anchorLinks.forEach(link => {
    const anchor = link.match(/\(#([^)]+)\)/)?.[1];
    if (anchor && !validAnchors.has(anchor)) {
      errors.push(`Broken anchor link: ${anchor}`);
    }
  });

  return {
    valid: errors.length === 0,
    errors
  };
}
```

---

### Formatting Examples

#### Example 1: Formatting P0 Critical Issue

```typescript
const issue: ConsolidatedIssue = {
  id: 'issue-1',
  message: 'Potential null reference exception in ValidateToken',
  description: 'Method accesses User property without null check after FindByIdAsync call',
  severity: 'P0',
  file: 'Services/AuthService.cs',
  line: 127,
  confidence: 0.88,
  reviewers: ['code-principles-reviewer', 'test-healer'],
  agreement: 1.0,
  impact: 'FindByIdAsync can return null if user not found, leading to NullReferenceException',
  actionRequired: 'Add null check before accessing User properties',
  codeSnippet: `    var user = await _userRepository.FindByIdAsync(userId);

>>>     if (user.IsActive && user.EmailConfirmed)
    {
        return TokenValidationResult.Success(user);
    }`,
  suggestedFix: `    var user = await _userRepository.FindByIdAsync(userId);

    if (user == null)
    {
        return TokenValidationResult.Failure("User not found");
    }

    if (user.IsActive && user.EmailConfirmed)
    {
        return TokenValidationResult.Success(user);
    }`
};

// Formatted output:
const formatted = formatIssue(issue);
console.log(formatted);
```

**Output**:
```markdown
#### üî¥ Potential null reference exception in ValidateToken (Line 127)

**Description**: Method accesses User property without null check after FindByIdAsync call

**Impact**: FindByIdAsync can return null if user not found, leading to NullReferenceException

**Action Required**: Add null check before accessing User properties

**Reviewers**: code-principles-reviewer, test-healer (100% agreement)

**Confidence**: üü¢ High (88%)

**Code Context**:
```csharp
// Services/AuthService.cs:127
    var user = await _userRepository.FindByIdAsync(userId);

>>>     if (user.IsActive && user.EmailConfirmed)
    {
        return TokenValidationResult.Success(user);
    }
```

**Suggested Fix**:
```csharp
    var user = await _userRepository.FindByIdAsync(userId);

    if (user == null)
    {
        return TokenValidationResult.Failure("User not found");
    }

    if (user.IsActive && user.EmailConfirmed)
    {
        return TokenValidationResult.Success(user);
    }
```

---
```

---

### Integration with Metadata Footer

The Report Formatting system generates all sections EXCEPT the metadata footer, which is handled by the Report Metadata system (Task 4.1C). The formatReport() function calls generateMetadataFooter() which is defined in the next section.

**Handoff to Metadata System**:
- formatReport() collects all sections
- Passes complete metadata object to generateMetadataFooter()
- Appends returned footer to complete report
- Returns final markdown string

---

**Report Formatting Status**: ACTIVE
**Phase**: 4.1B - Report Generation & Output
**Dependencies**: Task 3.1, 3.2, 3.3 (Phase 3 Consolidation)
**Next**: Task 4.1C (Report Metadata)

---

## Report Metadata System

### Overview

The Report Metadata System collects, organizes, and formats comprehensive execution statistics, quality metrics, and reviewer participation data. It generates the metadata footer section that appears at the end of every consolidated review report.

**Purpose**:
- Track complete execution statistics (timing, files, LOC)
- Capture reviewer participation and performance
- Calculate quality metrics (confidence, agreement, coverage)
- Provide transparency into consolidation process
- Enable trend analysis across multiple reviews

**Integration**: Phase 4 (Report Generation & Output) - Task 4.1C

---

### Core Data Structures

```typescript
/**
 * Complete report metadata with timing, scope, statistics, and quality metrics
 */
interface ReportMetadata {
  // ===== Timing Information =====
  timestamp: Date;                  // Review start timestamp
  reviewDuration: number;           // Total review time (ms)
  consolidationDuration: number;    // Consolidation algorithm time (ms)

  // ===== Review Scope =====
  reviewContext: string;            // Plan name or description
  filesReviewed: number;            // Number of files reviewed
  linesOfCode: number;              // Total lines of code analyzed
  reviewers: ReviewerMetadata[];    // Individual reviewer metadata

  // ===== Issue Statistics =====
  issuesBeforeConsolidation: number; // Total issues from all reviewers
  issuesAfterConsolidation: number;  // Issues after deduplication
  deduplicationRatio: number;        // Percentage reduced (0-1)

  // ===== Priority Breakdown =====
  criticalIssues: number;           // P0 count
  warnings: number;                 // P1 count
  improvements: number;             // P2 count

  // ===== Performance Metrics =====
  averageReviewTimePerFile: number; // Average time per file (ms)
  cacheHitRate?: number;            // Cache hit percentage (0-1)
  timeoutCount: number;             // Number of reviewer timeouts

  // ===== Quality Indicators =====
  overallConfidence: number;        // Weighted average confidence (0-1)
  reviewerAgreement: number;        // Cross-reviewer agreement (0-1)
  coveragePercentage: number;       // Successfully reviewed files (0-1)

  // ===== Recommendation Synthesis =====
  recommendations?: RecommendationOutput; // Themes, action items, quick wins
}

/**
 * Individual reviewer execution metadata
 */
interface ReviewerMetadata {
  name: string;                     // Reviewer agent name
  status: ReviewerStatus;           // Execution status
  executionTime: number;            // Time taken (ms)
  issuesFound: number;              // Issues reported
  cacheHit: boolean;                // Whether cache was used
  errorMessage?: string;            // Error details if status = error
}

/**
 * Reviewer execution status
 */
type ReviewerStatus = 'success' | 'timeout' | 'error' | 'partial';

/**
 * Confidence distribution breakdown
 */
interface ConfidenceDistribution {
  high: number;       // Issues with confidence ‚â•80%
  medium: number;     // Issues with confidence 60-80%
  low: number;        // Issues with confidence <60%
}
```

---

### Metadata Collection

```typescript
/**
 * Collects execution metadata from reviewer outputs and consolidation results
 *
 * Algorithm:
 * 1. Extract timing information from reviewer outputs
 * 2. Calculate scope metrics (files, LOC)
 * 3. Compute issue statistics (before/after counts)
 * 4. Determine priority breakdown (P0/P1/P2)
 * 5. Calculate performance metrics (avg time, cache hits)
 * 6. Compute quality indicators (confidence, agreement, coverage)
 * 7. Attach recommendation synthesis output
 *
 * @param reviewerOutputs - Raw outputs from all reviewers
 * @param consolidatedIssues - Issues after deduplication
 * @param recommendations - Recommendation synthesis output
 * @param startTime - Review start timestamp
 * @param consolidationTime - Time taken for consolidation (ms)
 * @returns Complete ReportMetadata object
 */
function collectExecutionMetadata(
  reviewerOutputs: ReviewerOutput[],
  consolidatedIssues: ConsolidatedIssue[],
  recommendations: RecommendationOutput,
  startTime: Date,
  consolidationTime: number
): ReportMetadata {
  // 1. Timing information
  const reviewDuration = Date.now() - startTime.getTime();

  // 2. Review scope
  const filesReviewed = countUniqueFiles(reviewerOutputs);
  const linesOfCode = calculateTotalLOC(reviewerOutputs);
  const reviewerMetadata = reviewerOutputs.map(output => extractReviewerMetadata(output));

  // 3. Issue statistics
  const issuesBeforeConsolidation = reviewerOutputs.reduce(
    (sum, output) => sum + output.issues.length,
    0
  );
  const issuesAfterConsolidation = consolidatedIssues.length;
  const deduplicationRatio = issuesBeforeConsolidation > 0
    ? (issuesBeforeConsolidation - issuesAfterConsolidation) / issuesBeforeConsolidation
    : 0;

  // 4. Priority breakdown
  const priorityCounts = countByPriority(consolidatedIssues);

  // 5. Performance metrics
  const averageReviewTimePerFile = filesReviewed > 0
    ? reviewDuration / filesReviewed
    : 0;
  const cacheHitRate = calculateCacheHitRate(reviewerMetadata);
  const timeoutCount = reviewerMetadata.filter(r => r.status === 'timeout').length;

  // 6. Quality indicators
  const overallConfidence = calculateWeightedConfidence(consolidatedIssues);
  const reviewerAgreement = calculateAgreement(consolidatedIssues);
  const coveragePercentage = calculateCoverage(reviewerMetadata);

  return {
    timestamp: startTime,
    reviewDuration,
    consolidationDuration: consolidationTime,

    reviewContext: extractReviewContext(reviewerOutputs),
    filesReviewed,
    linesOfCode,
    reviewers: reviewerMetadata,

    issuesBeforeConsolidation,
    issuesAfterConsolidation,
    deduplicationRatio,

    criticalIssues: priorityCounts.P0,
    warnings: priorityCounts.P1,
    improvements: priorityCounts.P2,

    averageReviewTimePerFile,
    cacheHitRate,
    timeoutCount,

    overallConfidence,
    reviewerAgreement,
    coveragePercentage,

    recommendations
  };
}

/**
 * Counts unique files across all reviewer outputs
 */
function countUniqueFiles(outputs: ReviewerOutput[]): number {
  const uniqueFiles = new Set<string>();
  outputs.forEach(output => {
    output.issues.forEach(issue => {
      if (issue.file) {
        uniqueFiles.add(issue.file);
      }
    });
  });
  return uniqueFiles.size;
}

/**
 * Calculates total lines of code from file metadata
 */
function calculateTotalLOC(outputs: ReviewerOutput[]): number {
  // Assume each output has fileMetadata with LOC info
  const fileMap = new Map<string, number>();

  outputs.forEach(output => {
    if (output.fileMetadata) {
      output.fileMetadata.forEach(file => {
        if (!fileMap.has(file.path)) {
          fileMap.set(file.path, file.linesOfCode || 0);
        }
      });
    }
  });

  return Array.from(fileMap.values()).reduce((sum, loc) => sum + loc, 0);
}

/**
 * Extracts metadata for individual reviewer
 */
function extractReviewerMetadata(output: ReviewerOutput): ReviewerMetadata {
  return {
    name: output.reviewerName,
    status: output.status,
    executionTime: output.executionTime,
    issuesFound: output.issues.length,
    cacheHit: output.cacheHit || false,
    errorMessage: output.error
  };
}

/**
 * Counts issues by priority
 */
function countByPriority(issues: ConsolidatedIssue[]): { P0: number; P1: number; P2: number } {
  return {
    P0: issues.filter(i => i.severity === 'P0' || i.severity === 'critical').length,
    P1: issues.filter(i => i.severity === 'P1' || i.severity === 'warning').length,
    P2: issues.filter(i => i.severity === 'P2' || i.severity === 'improvement').length
  };
}

/**
 * Calculates cache hit rate across reviewers
 */
function calculateCacheHitRate(reviewers: ReviewerMetadata[]): number {
  const totalReviewers = reviewers.length;
  if (totalReviewers === 0) return 0;

  const cacheHits = reviewers.filter(r => r.cacheHit).length;
  return cacheHits / totalReviewers;
}

/**
 * Calculates weighted average confidence
 * Uses same weighting as priority aggregation (test-healer weight 1.2)
 */
function calculateWeightedConfidence(issues: ConsolidatedIssue[]): number {
  if (issues.length === 0) return 0;

  const totalConfidence = issues.reduce((sum, issue) => sum + issue.confidence, 0);
  return totalConfidence / issues.length;
}

/**
 * Calculates cross-reviewer agreement
 * Based on percentage of issues reported by multiple reviewers
 */
function calculateAgreement(issues: ConsolidatedIssue[]): number {
  if (issues.length === 0) return 0;

  const multiReviewerIssues = issues.filter(i => i.reviewers.length > 1).length;
  return multiReviewerIssues / issues.length;
}

/**
 * Calculates coverage percentage
 * Percentage of reviewers that completed successfully
 */
function calculateCoverage(reviewers: ReviewerMetadata[]): number {
  if (reviewers.length === 0) return 0;

  const successful = reviewers.filter(
    r => r.status === 'success' || r.status === 'partial'
  ).length;

  return successful / reviewers.length;
}

/**
 * Extracts review context from outputs
 */
function extractReviewContext(outputs: ReviewerOutput[]): string {
  // Try to find review context from first output
  const firstOutput = outputs[0];
  return firstOutput?.reviewContext || 'Code Review';
}
```

---

### Metadata Footer Generation

```typescript
/**
 * Generates complete metadata footer section for report
 *
 * Includes:
 * 1. Execution Summary (timing, scope)
 * 2. Issue Statistics (before/after, deduplication)
 * 3. Reviewer Participation (table)
 * 4. Quality Metrics (confidence, agreement, coverage)
 * 5. Confidence Distribution (table)
 * 6. Generator attribution
 *
 * @param metadata - Complete report metadata
 * @returns Formatted markdown footer section
 */
function generateMetadataFooter(metadata: ReportMetadata): string {
  let footer = '## Review Metadata\n\n';

  // 1. Execution Summary
  footer += generateExecutionSummary(metadata);
  footer += '\n';

  // 2. Issue Statistics
  footer += generateIssueStatistics(metadata);
  footer += '\n';

  // 3. Reviewer Participation
  footer += generateReviewerParticipation(metadata.reviewers);
  footer += '\n';

  // 4. Quality Metrics
  footer += generateQualityMetrics(metadata);
  footer += '\n';

  // 5. Confidence Distribution
  const distribution = calculateConfidenceDistribution(metadata);
  footer += generateConfidenceDistribution(distribution, metadata.issuesAfterConsolidation);
  footer += '\n';

  // 6. Generator attribution
  footer += '---\n\n';
  footer += '*Generated by review-consolidator v1.0*\n';
  footer += `*Report saved: Docs/reviews/${sanitizeFilename(metadata.reviewContext)}-consolidated-review.md*\n`;

  return footer;
}

/**
 * Generates Execution Summary subsection
 */
function generateExecutionSummary(metadata: ReportMetadata): string {
  const completedTime = new Date(metadata.timestamp.getTime() + metadata.reviewDuration);

  let summary = '### Execution Summary\n';
  summary += `- **Review Started**: ${metadata.timestamp.toISOString()}\n`;
  summary += `- **Review Completed**: ${completedTime.toISOString()}\n`;
  summary += `- **Total Duration**: ${formatDuration(metadata.reviewDuration)}\n`;
  summary += `- **Consolidation Time**: ${formatDuration(metadata.consolidationDuration)}\n`;
  summary += `- **Files Reviewed**: ${metadata.filesReviewed}\n`;
  summary += `- **Lines of Code**: ${metadata.linesOfCode.toLocaleString()}\n`;

  return summary;
}

/**
 * Generates Issue Statistics subsection
 */
function generateIssueStatistics(metadata: ReportMetadata): string {
  const deduplicationPct = (metadata.deduplicationRatio * 100).toFixed(1);

  let stats = '### Issue Statistics\n';
  stats += `- **Issues Before Consolidation**: ${metadata.issuesBeforeConsolidation}\n`;
  stats += `- **Issues After Consolidation**: ${metadata.issuesAfterConsolidation}\n`;
  stats += `- **Deduplication Ratio**: ${deduplicationPct}%\n`;
  stats += `- **Critical Issues (P0)**: ${metadata.criticalIssues}\n`;
  stats += `- **Warnings (P1)**: ${metadata.warnings}\n`;
  stats += `- **Improvements (P2)**: ${metadata.improvements}\n`;

  return stats;
}

/**
 * Generates Reviewer Participation table
 */
function generateReviewerParticipation(reviewers: ReviewerMetadata[]): string {
  let table = '### Reviewer Participation\n\n';
  table += '| Reviewer | Status | Execution Time | Issues Found | Cache Hit |\n';
  table += '|----------|--------|----------------|--------------|-----------|\\n';

  reviewers.forEach(reviewer => {
    const statusEmoji = getReviewerStatusEmoji(reviewer.status);
    const statusText = capitalizeStatus(reviewer.status);
    const duration = formatDuration(reviewer.executionTime);
    const cacheText = reviewer.cacheHit ? 'Yes' : 'No';

    table += `| ${reviewer.name} | ${statusEmoji} ${statusText} | ${duration} | ${reviewer.issuesFound} | ${cacheText} |\n`;
  });

  return table;
}

/**
 * Gets emoji for reviewer status
 */
function getReviewerStatusEmoji(status: ReviewerStatus): string {
  const emojiMap: Record<ReviewerStatus, string> = {
    'success': '‚úÖ',
    'partial': '‚ö†Ô∏è',
    'timeout': '‚è±Ô∏è',
    'error': '‚ùå'
  };
  return emojiMap[status] || '‚ùì';
}

/**
 * Capitalizes reviewer status
 */
function capitalizeStatus(status: string): string {
  return status.charAt(0).toUpperCase() + status.slice(1);
}

/**
 * Generates Quality Metrics subsection
 */
function generateQualityMetrics(metadata: ReportMetadata): string {
  const confidence = (metadata.overallConfidence * 100).toFixed(1);
  const agreement = (metadata.reviewerAgreement * 100).toFixed(1);
  const coverage = (metadata.coveragePercentage * 100).toFixed(1);
  const avgTimePerFile = formatDuration(metadata.averageReviewTimePerFile);

  let metrics = '### Quality Metrics\n';
  metrics += `- **Overall Confidence**: ${confidence}%\n`;
  metrics += `- **Reviewer Agreement**: ${agreement}%\n`;
  metrics += `- **Coverage**: ${coverage}% (successfully reviewed files)\n`;
  metrics += `- **Average Time Per File**: ${avgTimePerFile}\n`;

  if (metadata.cacheHitRate !== undefined) {
    const cacheHit = (metadata.cacheHitRate * 100).toFixed(1);
    metrics += `- **Cache Hit Rate**: ${cacheHit}%\n`;
  }

  metrics += `- **Timeout Count**: ${metadata.timeoutCount}\n`;

  return metrics;
}

/**
 * Calculates confidence distribution
 */
function calculateConfidenceDistribution(metadata: ReportMetadata): ConfidenceDistribution {
  // This would typically be calculated from consolidated issues
  // For now, estimate based on overall confidence

  const total = metadata.issuesAfterConsolidation;
  const overallConf = metadata.overallConfidence;

  // Estimate distribution based on overall confidence
  // Higher overall confidence = more high confidence issues
  let high = 0, medium = 0, low = 0;

  if (overallConf >= 0.8) {
    high = Math.floor(total * 0.7);
    medium = Math.floor(total * 0.25);
    low = total - high - medium;
  } else if (overallConf >= 0.6) {
    high = Math.floor(total * 0.4);
    medium = Math.floor(total * 0.45);
    low = total - high - medium;
  } else {
    high = Math.floor(total * 0.2);
    medium = Math.floor(total * 0.35);
    low = total - high - medium;
  }

  return { high, medium, low };
}

/**
 * Generates Confidence Distribution table
 */
function generateConfidenceDistribution(
  distribution: ConfidenceDistribution,
  total: number
): string {
  const highPct = total > 0 ? ((distribution.high / total) * 100).toFixed(0) : '0';
  const mediumPct = total > 0 ? ((distribution.medium / total) * 100).toFixed(0) : '0';
  const lowPct = total > 0 ? ((distribution.low / total) * 100).toFixed(0) : '0';

  let table = '### Confidence Distribution\n\n';
  table += '| Confidence Level | Issue Count | Percentage |\n';
  table += '|------------------|-------------|------------|\\n';
  table += `| üü¢ High (‚â•80%) | ${distribution.high} | ${highPct}% |\n`;
  table += `| üü° Medium (60-80%) | ${distribution.medium} | ${mediumPct}% |\n`;
  table += `| üî¥ Low (<60%) | ${distribution.low} | ${lowPct}% |\n`;

  return table;
}

/**
 * Sanitizes filename for file system
 */
function sanitizeFilename(name: string): string {
  return name
    .toLowerCase()
    .replace(/[^a-z0-9-]/g, '-')
    .replace(/--+/g, '-')
    .replace(/^-|-$/g, '');
}
```

---

### Metadata Examples

#### Example 1: Simple Review Metadata

```typescript
const metadata: ReportMetadata = {
  timestamp: new Date('2025-10-16T14:21:13Z'),
  reviewDuration: 154000, // 2m 34s
  consolidationDuration: 800, // 0.8s

  reviewContext: 'CreateTaskCommand Implementation',
  filesReviewed: 2,
  linesOfCode: 247,
  reviewers: [
    {
      name: 'code-principles-reviewer',
      status: 'success',
      executionTime: 72000, // 1m 12s
      issuesFound: 3,
      cacheHit: false
    },
    {
      name: 'code-style-reviewer',
      status: 'success',
      executionTime: 78000, // 1m 18s
      issuesFound: 2,
      cacheHit: false
    }
  ],

  issuesBeforeConsolidation: 5,
  issuesAfterConsolidation: 2,
  deduplicationRatio: 0.6,

  criticalIssues: 0,
  warnings: 0,
  improvements: 2,

  averageReviewTimePerFile: 77000, // 1m 17s
  cacheHitRate: 0,
  timeoutCount: 0,

  overallConfidence: 0.89,
  reviewerAgreement: 1.0,
  coveragePercentage: 1.0,

  recommendations: {
    themes: [],
    actionItems: [],
    quickWins: [],
    statistics: {
      totalRecommendations: 2,
      totalActionItems: 2,
      topThemes: 0,
      quickWinCount: 0,
      highConfidenceIssues: 2
    }
  }
};

const footer = generateMetadataFooter(metadata);
console.log(footer);
```

**Output**:
```markdown
## Review Metadata

### Execution Summary
- **Review Started**: 2025-10-16T14:21:13.000Z
- **Review Completed**: 2025-10-16T14:23:47.000Z
- **Total Duration**: 2m 34s
- **Consolidation Time**: 0.8s
- **Files Reviewed**: 2
- **Lines of Code**: 247

### Issue Statistics
- **Issues Before Consolidation**: 5
- **Issues After Consolidation**: 2
- **Deduplication Ratio**: 60.0%
- **Critical Issues (P0)**: 0
- **Warnings (P1)**: 0
- **Improvements (P2)**: 2

### Reviewer Participation

| Reviewer | Status | Execution Time | Issues Found | Cache Hit |
|----------|--------|----------------|--------------|-----------|
| code-principles-reviewer | ‚úÖ Success | 1m 12s | 3 | No |
| code-style-reviewer | ‚úÖ Success | 1m 18s | 2 | No |

### Quality Metrics
- **Overall Confidence**: 89.0%
- **Reviewer Agreement**: 100.0%
- **Coverage**: 100.0% (successfully reviewed files)
- **Average Time Per File**: 1m 17s
- **Cache Hit Rate**: 0.0%
- **Timeout Count**: 0

### Confidence Distribution

| Confidence Level | Issue Count | Percentage |
|------------------|-------------|------------|
| üü¢ High (‚â•80%) | 1 | 50% |
| üü° Medium (60-80%) | 1 | 50% |
| üî¥ Low (<60%) | 0 | 0% |

---

*Generated by review-consolidator v1.0*
*Report saved: Docs/reviews/createtaskcommand-implementation-consolidated-review.md*
```

#### Example 2: Complex Review with Timeouts

```typescript
const metadata: ReportMetadata = {
  timestamp: new Date('2025-10-16T09:06:21Z'),
  reviewDuration: 527000, // 8m 47s
  consolidationDuration: 2300, // 2.3s

  reviewContext: 'Legacy Module Technical Debt Assessment',
  filesReviewed: 42,
  linesOfCode: 8234,
  reviewers: [
    {
      name: 'code-principles-reviewer',
      status: 'success',
      executionTime: 192000, // 3m 12s
      issuesFound: 89,
      cacheHit: false
    },
    {
      name: 'code-style-reviewer',
      status: 'success',
      executionTime: 168000, // 2m 48s
      issuesFound: 127,
      cacheHit: false
    },
    {
      name: 'test-healer',
      status: 'success',
      executionTime: 154000, // 2m 34s
      issuesFound: 18,
      cacheHit: false
    }
  ],

  issuesBeforeConsolidation: 234,
  issuesAfterConsolidation: 156,
  deduplicationRatio: 0.33,

  criticalIssues: 12,
  warnings: 68,
  improvements: 76,

  averageReviewTimePerFile: 12547, // 12.5s
  cacheHitRate: 0,
  timeoutCount: 0,

  overallConfidence: 0.82,
  reviewerAgreement: 0.45,
  coveragePercentage: 1.0,

  recommendations: {
    themes: [
      {
        theme: 'di-registration-missing',
        reportedBy: 2,
        totalReviewers: 3,
        occurrences: 23,
        filesAffected: 23,
        recommendation: 'Register all services in DI container',
        quickWin: false,
        effort: '12-16h',
        examples: []
      }
    ],
    actionItems: [],
    quickWins: [],
    statistics: {
      totalRecommendations: 156,
      totalActionItems: 156,
      topThemes: 5,
      quickWinCount: 8,
      highConfidenceIssues: 98
    }
  }
};

const footer = generateMetadataFooter(metadata);
console.log(footer);
```

**Output**:
```markdown
## Review Metadata

### Execution Summary
- **Review Started**: 2025-10-16T09:06:21.000Z
- **Review Completed**: 2025-10-16T09:15:08.000Z
- **Total Duration**: 8m 47s
- **Consolidation Time**: 2.3s
- **Files Reviewed**: 42
- **Lines of Code**: 8,234

### Issue Statistics
- **Issues Before Consolidation**: 234
- **Issues After Consolidation**: 156
- **Deduplication Ratio**: 33.0%
- **Critical Issues (P0)**: 12
- **Warnings (P1)**: 68
- **Improvements (P2)**: 76

### Reviewer Participation

| Reviewer | Status | Execution Time | Issues Found | Cache Hit |
|----------|--------|----------------|--------------|-----------|
| code-principles-reviewer | ‚úÖ Success | 3m 12s | 89 | No |
| code-style-reviewer | ‚úÖ Success | 2m 48s | 127 | No |
| test-healer | ‚úÖ Success | 2m 34s | 18 | No |

### Quality Metrics
- **Overall Confidence**: 82.0%
- **Reviewer Agreement**: 45.0%
- **Coverage**: 100.0% (successfully reviewed files)
- **Average Time Per File**: 12.5s
- **Cache Hit Rate**: 0.0%
- **Timeout Count**: 0

### Confidence Distribution

| Confidence Level | Issue Count | Percentage |
|------------------|-------------|------------|
| üü¢ High (‚â•80%) | 109 | 70% |
| üü° Medium (60-80%) | 39 | 25% |
| üî¥ Low (<60%) | 8 | 5% |

---

*Generated by review-consolidator v1.0*
*Report saved: Docs/reviews/legacy-module-technical-debt-assessment-consolidated-review.md*
```

---

### Integration with Report Formatting

The Report Metadata System is called by the Report Formatting system as the final step in report generation:

```typescript
// From formatReport() in Report Formatting section
function formatReport(
  sections: ReportSection[],
  metadata: ReportMetadata
): string {
  let markdown = '';

  // ... generate all other sections ...

  // Step 7: Generate metadata footer (calls this system)
  markdown += generateMetadataFooter(metadata);

  return markdown;
}
```

**Data Flow**:
1. **Parallel Execution** ‚Üí Reviewer outputs collected
2. **Consolidation Algorithm** ‚Üí Issues deduplicated
3. **Recommendation Synthesis** ‚Üí Themes and action items extracted
4. **Metadata Collection** ‚Üí `collectExecutionMetadata()` gathers all statistics
5. **Report Formatting** ‚Üí Formats all sections
6. **Metadata Footer** ‚Üí `generateMetadataFooter()` creates final section
7. **Complete Report** ‚Üí Saved to file

---

### Metadata Validation

```typescript
/**
 * Validates metadata completeness before report generation
 *
 * Ensures all required fields present and within valid ranges
 *
 * @param metadata - Metadata to validate
 * @returns Validation result with errors
 */
function validateMetadata(metadata: ReportMetadata): { valid: boolean; errors: string[] } {
  const errors: string[] = [];

  // Required fields
  if (!metadata.timestamp) errors.push('Missing timestamp');
  if (metadata.reviewDuration < 0) errors.push('Invalid reviewDuration (negative)');
  if (metadata.consolidationDuration < 0) errors.push('Invalid consolidationDuration (negative)');
  if (metadata.filesReviewed < 0) errors.push('Invalid filesReviewed (negative)');
  if (metadata.linesOfCode < 0) errors.push('Invalid linesOfCode (negative)');
  if (!metadata.reviewers || metadata.reviewers.length === 0) {
    errors.push('No reviewers in metadata');
  }

  // Issue statistics consistency
  if (metadata.issuesAfterConsolidation > metadata.issuesBeforeConsolidation) {
    errors.push('Issues after consolidation cannot exceed issues before');
  }

  const totalIssues = metadata.criticalIssues + metadata.warnings + metadata.improvements;
  if (totalIssues !== metadata.issuesAfterConsolidation) {
    errors.push(`Priority breakdown (${totalIssues}) doesn't match total issues (${metadata.issuesAfterConsolidation})`);
  }

  // Range validations (0-1)
  if (metadata.deduplicationRatio < 0 || metadata.deduplicationRatio > 1) {
    errors.push('Invalid deduplicationRatio (must be 0-1)');
  }
  if (metadata.overallConfidence < 0 || metadata.overallConfidence > 1) {
    errors.push('Invalid overallConfidence (must be 0-1)');
  }
  if (metadata.reviewerAgreement < 0 || metadata.reviewerAgreement > 1) {
    errors.push('Invalid reviewerAgreement (must be 0-1)');
  }
  if (metadata.coveragePercentage < 0 || metadata.coveragePercentage > 1) {
    errors.push('Invalid coveragePercentage (must be 0-1)');
  }

  return {
    valid: errors.length === 0,
    errors
  };
}
```

---

### Performance Metrics

**Metadata Collection Performance**:
- Time complexity: O(n) where n = total issues
- Space complexity: O(m) where m = number of reviewers
- Typical collection time: <100ms for reviews with <500 issues

**Footer Generation Performance**:
- Time complexity: O(m) where m = number of reviewers
- Markdown generation: <50ms for typical metadata
- No blocking operations (pure string formatting)

---

## Traceability Matrix

### Overview

The traceability matrix provides a complete lineage of how individual reviewer issues were consolidated into the master report. It enables users to understand which reviewers reported each issue, identify patterns in reviewer agreement, and trace any consolidated issue back to its original source reports.

**Purpose**:
- **Issue lineage tracking**: Show complete path from individual issues to consolidated issues
- **Reviewer agreement visualization**: Identify which issues multiple reviewers agreed on
- **Audit trail**: Maintain complete traceability for compliance and validation
- **Pattern analysis**: Reveal reviewer coverage patterns and blind spots

**Key Features**:
1. Tabular format showing consolidated issues vs. reviewer reports
2. Original issue IDs preserved for each reviewer
3. Merged issue indicators (when multiple issues consolidated)
4. Priority and confidence scores for each consolidated issue
5. Clear legend explaining notation

---

### Matrix Structure

The traceability matrix is a markdown table with:

**Columns**:
- **Consolidated Issue**: ID and truncated description (60 chars max)
- **Reviewer columns**: One column per active reviewer (e.g., code-style, code-principles, test-healer)
- **Priority**: Consolidated priority (P0/P1/P2)
- **Confidence**: Final confidence score (0.00-1.00)

**Rows**:
- One row per consolidated issue
- Ordered by priority (P0 first, then P1, then P2)
- Within same priority, ordered by confidence (highest first)

**Cell Values**:
- **"-"**: Issue not reported by this reviewer
- **"Issue [ID]"**: Original issue ID from reviewer (e.g., "Issue S3")
- **"Issues [range]"**: Multiple merged issues (e.g., "Issues S3-S17")
- **"Multiple"**: Several non-consecutive issues merged

---

### Data Structures

```typescript
/**
 * Entry in traceability matrix representing one consolidated issue
 */
interface TraceabilityEntry {
  // Consolidated issue identification
  consolidatedIssueId: string;                    // e.g., "#1", "#2", "#3"
  consolidatedDescription: string;                // Truncated to 60 chars

  // Reviewer issue mapping
  reviewerIssues: Map<string, string[]>;          // reviewer ‚Üí original issue IDs

  // Consolidated values
  priority: Priority;                             // P0, P1, or P2
  confidence: number;                             // 0.00-1.00
}

/**
 * Complete traceability matrix with metadata
 */
interface TraceabilityMatrix {
  entries: TraceabilityEntry[];                   // All consolidated issues
  reviewers: string[];                            // Active reviewer names
  metadata: {
    totalConsolidatedIssues: number;
    totalOriginalIssues: number;
    deduplicationRatio: number;                   // (original - consolidated) / original
    averageSourcesPerIssue: number;               // Average reviewers reporting each issue
    issuesReportedByAll: number;                  // Issues all reviewers agreed on
    issuesReportedBySingle: number;               // Issues from single reviewer only
  };
}
```

---

### Matrix Generation Algorithm

```typescript
/**
 * Generates complete traceability matrix from consolidated issues
 *
 * Creates tabular representation showing lineage from individual
 * reviewer issues to consolidated master report issues
 *
 * @param consolidatedIssues - Final consolidated issues
 * @param reviewerOutputs - Raw reviewer outputs
 * @returns Formatted markdown traceability matrix with legend
 */
function generateTraceabilityMatrix(
  consolidatedIssues: ConsolidatedIssue[],
  reviewerOutputs: ReviewerOutput[]
): string {
  // Step 1: Build traceability entries
  const entries = buildTraceabilityEntries(consolidatedIssues);

  // Step 2: Extract active reviewers
  const reviewers = extractActiveReviewers(reviewerOutputs);

  // Step 3: Calculate metadata
  const metadata = calculateMatrixMetadata(entries, reviewerOutputs);

  // Step 4: Format as markdown table
  const table = formatMatrixTable(entries, reviewers);

  // Step 5: Generate legend
  const legend = generateMatrixLegend();

  // Step 6: Generate statistics
  const statistics = formatMatrixStatistics(metadata);

  // Assemble complete matrix section
  return [
    '## Traceability Matrix',
    '',
    'This matrix shows how individual reviewer issues were consolidated into the master report.',
    '',
    table,
    '',
    legend,
    '',
    statistics
  ].join('\n');
}

/**
 * Builds traceability entries from consolidated issues
 *
 * @param consolidatedIssues - All consolidated issues
 * @returns Array of traceability entries
 */
function buildTraceabilityEntries(
  consolidatedIssues: ConsolidatedIssue[]
): TraceabilityEntry[] {
  const entries: TraceabilityEntry[] = [];

  for (const issue of consolidatedIssues) {
    const entry: TraceabilityEntry = {
      consolidatedIssueId: issue.id,
      consolidatedDescription: truncateDescription(issue.message, 60),
      reviewerIssues: new Map(),
      priority: issue.severity,
      confidence: issue.confidence
    };

    // Map sources to reviewers
    for (const source of issue.sources) {
      if (!entry.reviewerIssues.has(source.reviewer)) {
        entry.reviewerIssues.set(source.reviewer, []);
      }
      entry.reviewerIssues.get(source.reviewer)!.push(source.originalId);
    }

    entries.push(entry);
  }

  // Sort entries: P0 first, then by confidence descending
  entries.sort((a, b) => {
    if (a.priority !== b.priority) {
      return priorityToNumber(a.priority) - priorityToNumber(b.priority);
    }
    return b.confidence - a.confidence;
  });

  return entries;
}

/**
 * Extracts active reviewers (those that produced results)
 *
 * @param reviewerOutputs - All reviewer outputs
 * @returns Array of active reviewer names
 */
function extractActiveReviewers(reviewerOutputs: ReviewerOutput[]): string[] {
  return reviewerOutputs
    .filter(output => output.status === 'success' && output.issues.length > 0)
    .map(output => output.reviewerName);
}

/**
 * Calculates metadata statistics for matrix
 *
 * @param entries - Traceability entries
 * @param reviewerOutputs - Reviewer outputs
 * @returns Matrix metadata
 */
function calculateMatrixMetadata(
  entries: TraceabilityEntry[],
  reviewerOutputs: ReviewerOutput[]
): TraceabilityMatrix['metadata'] {
  const totalOriginalIssues = reviewerOutputs.reduce(
    (sum, output) => sum + output.issues.length,
    0
  );

  const totalConsolidatedIssues = entries.length;

  const deduplicationRatio = totalOriginalIssues > 0
    ? (totalOriginalIssues - totalConsolidatedIssues) / totalOriginalIssues
    : 0;

  // Calculate average sources per issue
  const totalSources = entries.reduce(
    (sum, entry) => sum + Array.from(entry.reviewerIssues.values()).flat().length,
    0
  );
  const averageSourcesPerIssue = totalConsolidatedIssues > 0
    ? totalSources / totalConsolidatedIssues
    : 0;

  // Count issues reported by all reviewers
  const activeReviewerCount = reviewerOutputs.filter(
    r => r.status === 'success' && r.issues.length > 0
  ).length;

  const issuesReportedByAll = entries.filter(
    entry => entry.reviewerIssues.size === activeReviewerCount
  ).length;

  // Count issues from single reviewer
  const issuesReportedBySingle = entries.filter(
    entry => entry.reviewerIssues.size === 1
  ).length;

  return {
    totalConsolidatedIssues,
    totalOriginalIssues,
    deduplicationRatio,
    averageSourcesPerIssue,
    issuesReportedByAll,
    issuesReportedBySingle
  };
}

/**
 * Formats traceability matrix as markdown table
 *
 * @param entries - Traceability entries
 * @param reviewers - Active reviewer names
 * @returns Formatted markdown table
 */
function formatMatrixTable(
  entries: TraceabilityEntry[],
  reviewers: string[]
): string {
  const lines: string[] = [];

  // Build header row
  const headers = [
    'Consolidated Issue',
    ...reviewers,
    'Priority',
    'Confidence'
  ];

  lines.push(`| ${headers.join(' | ')} |`);

  // Build separator row
  const separators = headers.map(() => '---');
  lines.push(`| ${separators.join(' | ')} |`);

  // Build data rows
  for (const entry of entries) {
    const cells: string[] = [];

    // Consolidated issue cell
    const issueCell = `${entry.consolidatedIssueId}: ${entry.consolidatedDescription}`;
    cells.push(issueCell);

    // Reviewer cells
    for (const reviewer of reviewers) {
      const issueIds = entry.reviewerIssues.get(reviewer) || [];

      if (issueIds.length === 0) {
        cells.push('-');
      } else if (issueIds.length === 1) {
        cells.push(`Issue ${issueIds[0]}`);
      } else {
        // Check if IDs are consecutive
        const formatted = formatIssueIdList(issueIds);
        cells.push(formatted);
      }
    }

    // Priority cell
    cells.push(entry.priority);

    // Confidence cell
    cells.push(entry.confidence.toFixed(2));

    lines.push(`| ${cells.join(' | ')} |`);
  }

  return lines.join('\n');
}

/**
 * Formats list of issue IDs (consecutive range or multiple)
 *
 * @param issueIds - Array of issue IDs
 * @returns Formatted string representation
 *
 * @example
 * formatIssueIdList(['S3', 'S4', 'S5']) ‚Üí "Issues S3-S5"
 * formatIssueIdList(['S3', 'S7', 'S15']) ‚Üí "Issues S3, S7, S15"
 * formatIssueIdList(['S3', 'S4', 'S5', 'S10', 'S11']) ‚Üí "Issues S3-S5, S10-S11"
 */
function formatIssueIdList(issueIds: string[]): string {
  if (issueIds.length === 1) {
    return `Issue ${issueIds[0]}`;
  }

  // Try to detect consecutive numeric ranges
  const numericIds = issueIds
    .map(id => ({ id, num: extractNumber(id) }))
    .filter(item => item.num !== null)
    .sort((a, b) => a.num! - b.num!);

  if (numericIds.length !== issueIds.length) {
    // Non-numeric IDs, just list them
    return `Issues ${issueIds.join(', ')}`;
  }

  // Build ranges
  const ranges: string[] = [];
  let rangeStart = 0;
  let rangeEnd = 0;

  for (let i = 1; i < numericIds.length; i++) {
    if (numericIds[i].num! === numericIds[i - 1].num! + 1) {
      // Consecutive
      rangeEnd = i;
    } else {
      // Non-consecutive, close previous range
      if (rangeEnd > rangeStart) {
        ranges.push(`${numericIds[rangeStart].id}-${numericIds[rangeEnd].id}`);
      } else {
        ranges.push(numericIds[rangeStart].id);
      }
      rangeStart = i;
      rangeEnd = i;
    }
  }

  // Close final range
  if (rangeEnd > rangeStart) {
    ranges.push(`${numericIds[rangeStart].id}-${numericIds[rangeEnd].id}`);
  } else {
    ranges.push(numericIds[rangeStart].id);
  }

  return `Issues ${ranges.join(', ')}`;
}

/**
 * Extracts numeric portion from issue ID
 *
 * @param issueId - Issue ID (e.g., "S3", "A12", "T5")
 * @returns Numeric value or null
 */
function extractNumber(issueId: string): number | null {
  const match = issueId.match(/\d+/);
  return match ? parseInt(match[0], 10) : null;
}

/**
 * Generates legend explaining matrix notation
 *
 * @returns Formatted markdown legend
 */
function generateMatrixLegend(): string {
  return [
    '### Legend',
    '',
    '- **-**: Issue not reported by this reviewer',
    '- **Issue [ID]**: Original issue ID from reviewer (e.g., Issue S3)',
    '- **Issues [range]**: Consecutive issues merged (e.g., Issues S3-S17 means 15 issues)',
    '- **Issues [list]**: Multiple non-consecutive issues (e.g., Issues S3, S7, S15)',
    '- **Priority**: Consolidated priority after aggregation (P0/P1/P2)',
    '- **Confidence**: Final confidence score after weighting (0.00-1.00)'
  ].join('\n');
}

/**
 * Formats matrix statistics section
 *
 * @param metadata - Matrix metadata
 * @returns Formatted markdown statistics
 */
function formatMatrixStatistics(metadata: TraceabilityMatrix['metadata']): string {
  return [
    '### Matrix Statistics',
    '',
    `- **Total Consolidated Issues**: ${metadata.totalConsolidatedIssues}`,
    `- **Total Original Issues**: ${metadata.totalOriginalIssues}`,
    `- **Deduplication Ratio**: ${(metadata.deduplicationRatio * 100).toFixed(1)}%`,
    `- **Average Reviewers per Issue**: ${metadata.averageSourcesPerIssue.toFixed(2)}`,
    `- **Issues Reported by All Reviewers**: ${metadata.issuesReportedByAll}`,
    `- **Issues from Single Reviewer**: ${metadata.issuesReportedBySingle}`
  ].join('\n');
}

/**
 * Truncates description to specified length
 *
 * @param description - Full description text
 * @param maxLength - Maximum length (default 60)
 * @returns Truncated description with ellipsis if needed
 */
function truncateDescription(description: string, maxLength: number = 60): string {
  if (description.length <= maxLength) {
    return description;
  }

  return description.substring(0, maxLength - 3) + '...';
}

/**
 * Converts priority to numeric value for sorting
 *
 * @param priority - Priority level
 * @returns Numeric value (lower = higher priority)
 */
function priorityToNumber(priority: Priority): number {
  switch (priority) {
    case 'P0': return 0;
    case 'P1': return 1;
    case 'P2': return 2;
    default: return 99;
  }
}
```

---

### Matrix Examples

#### Example 1: Simple Matrix (3 Reviewers, Low Overlap)

```markdown
## Traceability Matrix

This matrix shows how individual reviewer issues were consolidated into the master report.

| Consolidated Issue | code-style | code-principles | test-healer | Priority | Confidence |
|-------------------|------------|----------------|------------|----------|-----------|
| #1: ServiceLocator pattern violates DIP | - | Issue A1 | - | P0 | 0.95 |
| #2: Method 'RefreshToken' has no test coverage | - | - | Issue T1 | P0 | 0.95 |
| #3: AuthService has multiple responsibilities | - | Issue A2 | - | P1 | 0.85 |
| #4: UserController handles CRUD, validation, emails | - | Issue A3 | - | P1 | 0.82 |
| #5: Missing braces in if statements | Issues S1-S15 | - | - | P1 | 0.95 |

### Legend

- **-**: Issue not reported by this reviewer
- **Issue [ID]**: Original issue ID from reviewer (e.g., Issue S3)
- **Issues [range]**: Consecutive issues merged (e.g., Issues S3-S17 means 15 issues)
- **Issues [list]**: Multiple non-consecutive issues (e.g., Issues S3, S7, S15)
- **Priority**: Consolidated priority after aggregation (P0/P1/P2)
- **Confidence**: Final confidence score after weighting (0.00-1.00)

### Matrix Statistics

- **Total Consolidated Issues**: 5
- **Total Original Issues**: 19
- **Deduplication Ratio**: 73.7%
- **Average Reviewers per Issue**: 1.05
- **Issues Reported by All Reviewers**: 0
- **Issues from Single Reviewer**: 5
```

#### Example 2: Complex Matrix (3 Reviewers, High Overlap)

```markdown
## Traceability Matrix

This matrix shows how individual reviewer issues were consolidated into the master report.

| Consolidated Issue | code-style | code-principles | test-healer | Priority | Confidence |
|-------------------|------------|----------------|------------|----------|-----------|
| #1: Null reference in AuthController:42 | - | Issue A12 | Issue T5 | P0 | 0.92 |
| #2: ServiceLocator anti-pattern detected | Issue S3 | Issue A1 | - | P0 | 0.93 |
| #3: Missing test coverage for RefreshToken | Issue S45 | Issue A8 | Issue T1 | P0 | 0.94 |
| #4: DRY violation in UserService | Issue S22 | Issues A5-A7 | - | P1 | 0.88 |
| #5: Missing braces (multiple files) | Issues S1-S15 | Issue A2 | - | P1 | 0.95 |
| #6: Naming conventions violated | Issues S18-S35 | - | - | P2 | 0.96 |
| #7: XML documentation missing | Issues S36-S44 | - | - | P2 | 0.92 |
| #8: Test assertions missing | - | - | Issues T10-T14 | P1 | 0.90 |

### Legend

- **-**: Issue not reported by this reviewer
- **Issue [ID]**: Original issue ID from reviewer (e.g., Issue S3)
- **Issues [range]**: Consecutive issues merged (e.g., Issues S3-S17 means 15 issues)
- **Issues [list]**: Multiple non-consecutive issues (e.g., Issues S3, S7, S15)
- **Priority**: Consolidated priority after aggregation (P0/P1/P2)
- **Confidence**: Final confidence score after weighting (0.00-1.00)

### Matrix Statistics

- **Total Consolidated Issues**: 8
- **Total Original Issues**: 67
- **Deduplication Ratio**: 88.1%
- **Average Reviewers per Issue**: 1.63
- **Issues Reported by All Reviewers**: 1
- **Issues from Single Reviewer**: 4
```

#### Example 3: Perfect Agreement Matrix (3 Reviewers, Complete Overlap)

```markdown
## Traceability Matrix

This matrix shows how individual reviewer issues were consolidated into the master report.

| Consolidated Issue | code-style | code-principles | test-healer | Priority | Confidence |
|-------------------|------------|----------------|------------|----------|-----------|
| #1: Critical null reference potential | Issue S1 | Issue A1 | Issue T1 | P0 | 0.96 |
| #2: ServiceLocator detected | Issue S2 | Issue A2 | Issue T2 | P0 | 0.95 |
| #3: AuthService SRP violation | Issue S5 | Issue A5 | Issue T5 | P1 | 0.89 |

### Legend

- **-**: Issue not reported by this reviewer
- **Issue [ID]**: Original issue ID from reviewer (e.g., Issue S3)
- **Issues [range]**: Consecutive issues merged (e.g., Issues S3-S17 means 15 issues)
- **Issues [list]**: Multiple non-consecutive issues (e.g., Issues S3, S7, S15)
- **Priority**: Consolidated priority after aggregation (P0/P1/P2)
- **Confidence**: Final confidence score after weighting (0.00-1.00)

### Matrix Statistics

- **Total Consolidated Issues**: 3
- **Total Original Issues**: 9
- **Deduplication Ratio**: 66.7%
- **Average Reviewers per Issue**: 3.00
- **Issues Reported by All Reviewers**: 3
- **Issues from Single Reviewer**: 0
```

---

### Matrix Integration with Master Report

**Placement in Report**:

The traceability matrix appears as the LAST section before the Review Metadata footer:

```markdown
[... Master Report Sections ...]

## Prioritized Action Items

[Action items]

---

## Appendix A: code-style-reviewer Full Report

[Appendix A content]

---

## Appendix B: code-principles-reviewer Full Report

[Appendix B content]

---

## Appendix C: test-healer Full Report

[Appendix C content]

---

## Traceability Matrix

[Matrix table, legend, statistics]

---

## Review Metadata

[Metadata footer]
```

**Cross-Reference Chain**:

The matrix completes a three-way cross-reference system:

1. **Master Report Issue ‚Üí Matrix**: Users see consolidated issue #5, check matrix to find original issues
2. **Matrix ‚Üí Appendix**: Matrix shows "Issue S3", users navigate to Appendix A to see full details
3. **Appendix Issue ‚Üí Master Report**: Appendix shows "Consolidated Issue: #5", users navigate back to master report

**Example Navigation Flow**:

```
User sees: Master Report Issue #5 "Missing braces (15 files)"
         ‚Üì
User checks: Matrix row "#5: Missing braces..." shows "Issues S1-S15" in code-style column
         ‚Üì
User navigates: To Appendix A (code-style-reviewer)
         ‚Üì
User finds: Detailed findings section with all 15 issues (S1-S15) with file:line info
         ‚Üì
User confirms: Each issue shows "Consolidated Issue: #5 (merged)"
```

---

### Matrix Quality Checklist

Before finalizing traceability matrix, verify:

**Structure Validation**:
- [ ] Table header row present with all columns
- [ ] Separator row correctly formatted
- [ ] One data row per consolidated issue
- [ ] All consolidated issues included (no missing rows)
- [ ] Columns match active reviewers exactly

**Data Validation**:
- [ ] All issue IDs in matrix exist in appendices
- [ ] All appendix issues appear in matrix (bidirectional check)
- [ ] Priority values match consolidated issues
- [ ] Confidence scores match consolidated issues
- [ ] No orphaned issues (every original issue traceable)

**Format Validation**:
- [ ] Consecutive issue ranges formatted correctly (S3-S17)
- [ ] Non-consecutive issues listed with commas (S3, S7, S15)
- [ ] Missing issues marked with "-" consistently
- [ ] Confidence formatted to 2 decimal places
- [ ] Descriptions truncated to 60 chars with "..." if needed

**Legend Validation**:
- [ ] All notation types used in matrix explained in legend
- [ ] Legend formatting consistent (bullet list)
- [ ] Examples provided for clarity
- [ ] Priority and confidence explanations included

**Statistics Validation**:
- [ ] Total consolidated issues count accurate
- [ ] Total original issues count accurate
- [ ] Deduplication ratio calculated correctly
- [ ] Average reviewers per issue accurate
- [ ] Issues by all/single reviewer counts correct
- [ ] All percentages formatted to 1 decimal place

**Integration Validation**:
- [ ] Matrix placed after all appendices
- [ ] Matrix before Review Metadata footer
- [ ] Cross-references between matrix and appendices valid
- [ ] Cross-references between matrix and master report valid
- [ ] Navigation flow documented and testable

---

### Performance Metrics

**Matrix Generation Performance**:
- Time complexity: O(n √ó m) where n = consolidated issues, m = reviewers
- Space complexity: O(n √ó m) for reviewer issue mapping
- Typical generation time: <200ms for reviews with <100 consolidated issues
- Large matrix (>200 issues): <500ms

**Matrix Formatting Performance**:
- Markdown generation: O(n √ó m) for table formatting
- Range detection: O(k log k) where k = issues per reviewer
- Typical formatting time: <100ms for standard matrices

---

**Report Metadata System Status**: ACTIVE
**Phase**: 4.2B - Traceability Matrix
**Dependencies**: Task 4.1 (Master Report), Task 4.2A (Appendices), Task 3.1-3.3 (Consolidation)
**Next**: Task 4.3 (File Operations)

---

**Algorithm Status**: ACTIVE
**Owner**: Development Team
**Last Updated**: 2025-10-16
**Related Documentation**:
- Agent Specification: `.cursor/agents/review-consolidator/agent.md`
- Prompt Template: `.cursor/agents/review-consolidator/prompt.md`
- Implementation Plan: `Docs/plans/Review-Consolidator-Implementation-Plan/phase-4-report-generation/task-4.2-reviewer-appendices.md`
- Implementation Plan: `Docs/plans/Review-Consolidator-Implementation-Plan/phase-1-foundation.md`
